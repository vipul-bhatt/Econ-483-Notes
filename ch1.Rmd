# Introduction to Forecasting {#intro}

## Time Series
A time series is a specific kind of data where observations of a variable are recorded over time. For example, the data for the U.S. GDP for the last 30 years is a time series data. 

Such data shows how a variable is changing over time. Depending on the variable of interest we can have data measured at different frequencies. Some commonly used frequencies are intra-day, daily, weekly, monthly, quarterly, semi-annual and annual. Figure \@ref(fig:ch1-figure1) below plots data for quarterly and monthly frequency. 

```{r ch1-figure1, echo = FALSE, fig.cap = 'Time Series at quarterly and monthly frequency', out.width='80%', fig.asp=.75, fig.align='center'}

suppressMessages(library(xts, warn.conflicts = FALSE, quietly=T))
suppressMessages(library(quantmod, warn.conflicts = FALSE, quietly = T))

# get data from fred stat
options("getSymbols.warning4.0"=FALSE)
invisible(getSymbols('GDPC1',src='FRED'))
invisible(getSymbols('RSXFSN',src='FRED'))

y1=GDPC1
y2=RSXFSN

# plot data over time

par(mfrow=c(2,1))
plot.xts(as.xts(y1),yaxis.right=F, main="Real GDP (Billions of 2009 Dollars)")
plot.xts(as.xts(y2),yaxis.right=F, main="Advance Retail Sales (Millions of Dollars)")


```
The first panel shows data for the real gross domestic product (GDP) for the US in billions of 2012 dollars, measured at a quarterly frequency. The second panel shows data for the advance retail sales (millions of dollars), measured at monthly frequency.

Formally, we denote a time series variable by $y_t$, where $t=0,1,2,..,T$ is the observation index. For example, at $t=10$ we get the tenth observation of this time series, $y_{10}$.




## Serial Correlation

Serial correlation (or auto correlation) refers to the tendency of observations of a time series being correlated over time. It is a measure of the temporal dynamics of a time series and addresses the following question: what is the effect of past realizations of a time series on the current period value? Formally, 

\begin{equation}
\rho(s)=Cor(y_t, y_{t-s}) =\frac{	Cov(y_t,y_{t-s})}{\sqrt{\sigma^2_{y_t} \times \sigma^2_{y_{t-s}}}}
(\#eq:sercor)
\end{equation}

 where $Cov(y_t,y_{t-s})= E(y_t-\mu_{y_t})(y_{t-s}-\mu_{y_{t-s}})$ and $\sigma^2_{y_t}=E(y_t-\mu_{y_t})^2$

Here, $\rho(s)$ is the serial correlation of order $s$. For example, $s=1$ implies *first order* serial correlation between $y_t$ and $y_{t-1}$, $s=2$ implies *second order* serial correlation between $y_t$ and $y_{t-2}$, and so on.


Note that often we use historical data to forecast. If there is no serial correlation, then past can offer no guidance for the present and future. In that sense, presence of serial correlation of some order is the first condition for being able to forecast a time series using its historical realizations.

Now, we can either have positive or negative serial correlation in data. Figure \@ref(fig:ch1-figure2) plots two time series with positive and negative serial correlation, respectively.

```{r ch1-figure2, echo = FALSE, fig.cap = 'Serial Correlation', out.width='80%', fig.asp=.75, fig.align='center'}

# simulate AR(1) process: phi = 0.5
ar1.model = list(ar=0.9)
mu = 1
set.seed(123)
y = mu + arima.sim(model=ar1.model,n=100)
p1=plot(y[1:100],y[2:101], ylab=expression('y'[t+1]), xlab=expression('y'[t]), main="Positive Serial Correlation")

# simulate AR(1) process: phi = -0.6
ar1.model = list(ar=-0.9)
mu = 1
set.seed(123)
x= mu + arima.sim(model=ar1.model,n=100)
p2=plot(x[1:100],x[2:101], ylab=expression('y'[t+1]), xlab=expression('y'[t]), main="Negative Serial Correlation")

par(mfrow=c(2,1))
invisible(p1)
invisible(p2)

```
## Testing for Serial Correlion

We can use a Lagrange-Multiplier (LM) test for detecting serial correlation. This test is also known as *Breuch-Godfrey* test. I will use the linear regression model to explain this test. Consider the following regression model:
\begin{equation}
y_t=\beta_0 + \beta_1 X_{1t}+\epsilon_t
\end{equation}

Consider the following model for serial correlation of order *p* for the error term:
\begin{equation}
\epsilon_t=\rho_1 \epsilon_{t-1}+\rho_2 \epsilon_{t-2}+...+ \rho_p \epsilon_{t-p}+\nu_t
(\#eq:bg)
\end{equation}

Then we are interested in the following test:

\[H_0=\rho_1=\rho_2=...=\rho_p=0 \]
\[H_A = Not \ H_0 \]

To implement this test, we estimate the BG regression model given by:
\begin{equation}
e_t=\alpha_0 + \alpha_1 X_{1t}+ \rho_1 e_{t-1}+\rho_2 e_{t-2}+...+ \rho_p e_{t-p}+\nu_t
(\#eq:bg1)
\end{equation}


where we replacr the error term with the OLS residuals (denoted by $e$). The LM test statistic is given by:

\[ LM  = N\times R^2_{BG}  \sim \chi^2_p  \]

If the test statistic value is greater than the critical value then we reject the null hypothesis.


## White Noise Process

A time series is a *white noise* process is it has zero mean, constant and finite variance, and is serially uncorrelated. Formally, $y_t$ is a white noise process if:

1. $E(y_t)=0$
2. $Var(y_t)=\sigma^2_y$
3. $Cov(y_t,y_{t-s})= 0 \forall s\neq t$

We can compress the above definition as: $y_t\sim WN(0,\sigma^2_y)$. 
Often we assume that the unexplained part of a time series follows a white noise process. Formally,

\begin{equation}
Time \ Series \ = \  Explained  \ + \ White \ Noise
\end{equation}

By definition we cannot forecast a white noise process. An important diagnostics of model adequacy is to test whether the estimated residuals are white noise (more on this later).

## Important Elements of Forecasting

```{definition, d1, name='Forecast'}
```
A *forecast*  is an *informed* guess about the unknown future value of a time series of interest. For example, what is the stock price of Facebook next Monday? 

There are three possible types of forecasts:

1. *Density Forecast*: we forecast the entire probability distribution of the possible future value of the time series of interest. Hence,

\begin{equation}
F(a)=P[y_{t+1}\leq a]
\end{equation}

give us the probability that the 1-period ahead future value of $y_{t+1}$ will be less than or equal to $a$.  For example, the future real GDP growth could be normally distributed with a mean of 1.3% and a standard deviation of 1.83%. Figure \@ref(fig:ch1-figure3) below plots the density forecast for real GDP growth.



```{r ch1-figure3, echo = FALSE, fig.cap = 'Density Forecast for Future Real GDP Growth', out.width='80%', fig.asp=.75, fig.align='center'}

population_mean <- 1.3
population_sd <- 1.83
sd_to_fill <- 2
lower_bound <- population_mean - population_sd * sd_to_fill
upper_bound <- population_mean + population_sd * sd_to_fill
 
# Generates equally spaced values within 4 standard deviations of the mean
# This is used to connect the points on the curve so the more points the better
x <- seq(-4, 4, length = 1000) * population_sd + population_mean
 
# Returns the height of the probably distribution at each of those points
y <- dnorm(x, population_mean, population_sd)
 
# Generate the plot, where:
# - type: the type of plot to be drawn where "n" means do not plot the points
# - xlab: the title of the x axis
# - ylab: the title of the y axis
# - main: the overall title for the plot
# - axes: when false it suppresses the axis automatically generated by the high level plotting function so that we can create custom axis
plot(x, y, type="n", xlab = "Future Real GDP Growth (%)", ylab = "", main = "Distribution of Future Real GDP Growth", axes = FALSE)
 
# Connect all of the points with each other to form the bell curve
lines(x, y)
 
# Returns a vector of boolean values representing whether the x value is between the two bounds then
# filters the values so that only the ones within the bounds are returned
bounds_filter <- x >= lower_bound & x <= upper_bound
x_within_bounds <- x[bounds_filter]
y_within_bounds <- y[bounds_filter]
 
# We want the filled in area to extend all the way down to the y axis which is why these two lines are necessary
# It makes the first point in the polygon (lower_bound, 0) and the last point (upper_bound, 0)
x_polygon <- c(lower_bound, x_within_bounds, upper_bound)
y_polygon <- c(0, y_within_bounds, 0)
 
polygon(x_polygon, y_polygon, col = "blue")
 
# Now determine the probability that someone falls between the two bounds so we can display it above the curve
# Remember that pnorm returns the probability that a normally distributed random number will be less than the given number
probability_within_bounds <- pnorm(upper_bound, population_mean, population_sd) - pnorm(lower_bound, population_mean, population_sd)
 
# Concatenate the various values so we can display it on the curve
text <- paste("P(", lower_bound, "< y[t+1] <", upper_bound, ") =", signif(probability_within_bounds, digits = 2))
 
# Display the text on the plot. The default "side" parameter is 3, representing the top of the plot.
mtext(text)
 
# Add an axis to the current plot, where:
# - side: which side of the plot the axis should be drawn on where 1 represents the bottom
# - at: the points at which the tick-marks are to be drawn
# - pos: the coordinate at which the axis line is to be drawn
sd_axis_bounds = 5
axis_bounds <- seq(-sd_axis_bounds * population_sd + population_mean, sd_axis_bounds * population_sd + population_mean, by = population_sd)
axis(side = 1, at = axis_bounds, pos = 0)

```


2. *Point Forecast*: our forecast at each horizon is a single number. Often we use the expected value or mean as the point forecast. For example, the point forecast for the 1-period ahead real GDP growth can be the mean of the probability distribution of the future real GDP growth:
\begin{equation}
f_{t,1}=1.3%
\end{equation}


3. *Interval Forecast*: our forecast at each horizon is a range which is obtained by adding *margin of errors* to the point forecast. With some probability we expect our future value to fall withing this range. For example, the  95% interval forecast for the next period real GDP growth is (-2.36%,4.96%). Hence, with 95% confidence we expect next period GDP to fall between -2.36% and 4.96%.


```{definition, d2, name='Forecast Horizon'}
```
*Forecast Horizon* is the number of periods into the future for which we forecast a time series. We will denote it by $h$. Hence, for $h=1$, we are looking at 1-period ahead forecast,  for $h=2$ we are looking at 2-period ahead forecast and so on.

Formally, for a given time series $y_t$, the h-period ahead unknown value is denoted by $y_{t+h}$. The forecast of this value is denoted $f_{t,h}$.  

```{r ch1-figure4, echo = FALSE, fig.cap = 'Forecast Horizon', out.width='80%', fig.asp=.8, fig.align='center'}

plot(1:28,axes = FALSE, type="n",xlab="",ylab="", main="")
axis(1, at=seq(1,28,by=2),
     labels=c("0","", "", "t-1","t","t+1", "" , "" ,"T","T+1","T+2","","", "T+h"))
abline(v=17, col="blue", lty="dotted")
text(8, 16,"Sample")
text(22, 16,"Forecast Horizon \n (Out-of-Sample)")

```

```{definition, d3, name='Forecast Error'}
```

A *forecast error* is the difference between the realization of the future value and the previously made forecast. Formally, the $h$-period ahead forecast error is given by:

\begin{equation}
e_{t,h}=y_{t+h}-f_{t,h}
\end{equation}

Hence, for every horizon, we will have a forecast and a corresponding forecast error. These errors can be negative (indicating over prediction) or positive (indicating under prediction). 

```{definition, d4, name='Information Set'}
```

Forecasts are based on *information* available at the time of making the forecast. *Information Set* contains all the relevant information about the time series we would like to forecast. We denote the set of information available at time $T$ by $\Omega_T$. There are two types of information sets:

1. Univariate Information set: Only includes historical data on the time series of interest:
\begin{equation}
\Omega_T=\{y_T, y_{T-1}, y_{T-2}, ...., y_1\}
\end{equation}

2. Multivariate Information set: Includes historical data on the time series of interest as well as any other variable(s) of interest. For example, suppose we have one more variable $x$ that is relevant for forecasting $y$. Then:
\begin{equation}
\Omega_T=\{y_T, x_T, y_{T-1}, x_{T-1}, y_{T-2},x_{T-2}. ...., y_1, x_1\}
\end{equation}





## Loss Function and Optimal Forecast

Think of a forecast as a solution to an *optimization* problem. When forecasts are wrong, the person making the forecast will suffer some *loss*. This loss will be a function of the magnitude as well as the sign of the *forecast error*. Hence, we can think of an *optimal forecast* as a solution to a minimization problem where the forecaster is minimizing the loss from the forecast error.

```{definition, d5, name='Loss Function'}
```

A *loss* function is a mapping between forecast errors and their associated losses. Formally, we denote the h-period ahead loss function by $L(e_{t,h})$. For a function to be used as a loss function, three properties must be satisfied:

1. $L(0)=0$
2. $\frac{dL}{de}>0$
3. $L(e)$ is a continuous function.


Two types of loss functions are:

- Symmetric Loss Function: both positive and negative forecast errors lead to same loss. See Figure \@ref(fig:ch1-figure5). A commonly used loss function is *quadratic loss function* given by:

\begin{equation}
L(e_{t,h})=e_{t,h}^2 = (y_{t+h}-f{t,h})^2
\end{equation}


```{r ch1-figure5, echo = FALSE, fig.cap = 'Quadratic Loss Functions', out.width='80%', fig.asp=.8, fig.align='center'}

curve(x^2, from=-10, to=10, xlab="e", ylab="Loss")
```

- Asymmetric Loss Function: loss depends on the sign of the forecast error. For example, it could be that positive errors produce greater loss when compared to negative errors. See the function below and Figure \@ref(fig:ch1-figure6) that attaches a higher loss to  positive errors:

\begin{equation}
L(e_{t,h})=e_{t,h}^2+4 \times e_{t,h}
\end{equation}

```{r ch1-figure6, echo = FALSE, fig.cap = 'Asymmetric Loss Function', out.width='80%', fig.asp=.8, fig.align='center'}


curve(x^2+4*x, from=-10, to=10, xlab="e", ylab="Loss")
```

Once we have chosen our loss function, the optimal forecast can be obtained by minimizing the expected loss function. 

```{definition, d6, name='Optimal Forecast'}
```

An *optimal forecast*  minimizes the expected loss from the forecast, given the information available at the time. Mathematically, we denote it by $f^*_{t,h}$ and it solves the following minimization problem:
\begin{equation}
min_{f_{t,h}} E(L(e_{t,h})|\Omega_t)
\end{equation}

In theory we can assume any functional form for the loss function and that will lead to a different *optimal forecast*. An important result that follows from a specific functional form is stated as Theorem 1.1.

```{theorem}
If the loss function is quadtratic then the optimal forecast is the conditional mean of the time series of interest. Formally, if $L(e_{t,h})=e_{t,h}^2$ then,
\begin{equation}
f^*_{t,h}=E(y_{t+h}|\Omega_t)
\end{equation}
```

Note that $E(e_{t,h}^2)$ is known as *mean squared errors (MSE)*. Hence, the expected loss from a quadratic loss function is the same as the MSE. In this course, we assume that the forecaster faces a quadratic loss function and hence based on Theorem 1.1, we will learn different models for estimating the conditional mean of the future value of the time series of interest, i.e., $E(y_{t+h}|\Omega_t)$.

