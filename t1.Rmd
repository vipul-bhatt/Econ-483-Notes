# Regression-based Forecasting

One way to compute the conditional expectation is the linear regression model. Here, our information set contains data on all relevant explantory variacbles available at the time of forecast, i.e,

\begin{equation}
\Omega_t={X_{1t}, X_{2t},...X_{Kt}}
\end{equation}

Hence, we get the following equality:

\begin{equation}
E(y_t|\Omega_t)=E(y_{t+h}|X_{1t}, X_{2t}, X_{3t},...,X_{Kt})
\end{equation}

The right hand side of the above equation is the multiple regression model of the form:
 \begin{equation}
 y_{t}=\beta_0+\beta_1 X_{1t}+\beta_2 X_{2t}+..+\beta_K X_{Kt}+\epsilon_t
 \end{equation}
 
We can easily estimate the above model using Ordinary Least Squares (OLS) and compute the *predicted value* of $y$:
  \begin{equation}
  	\widehat{y}_t = \widehat{\beta_0} +\widehat{\beta_1} X_{1t} +\widehat{\beta_2} X_{2t}+...+ \widehat{\beta_k} X_{Kt}
  \end{equation}
  
 
 
The above equation can be used to compute the optimal forecast. Suppose, we are interested in computed the $h$ period ahead forecast for $y$. Then, using the above equation we get:
  \begin{equation}
  	  	\widehat{y}_{t+h} =  \widehat{\beta_0} +\widehat{\beta_1} X_{1t+h} +\widehat{\beta_2} X_{2t+h}+...+ \widehat{\beta_k} X_{Kt+h}
  	\end{equation}
  	
## Scenario Analysis and Conditional Forecasts

One way to use a regression model to produce forecasts is called
 *scenario analysis* where we produce a different forecast for the dependent under each possible scenario about the future values of the independent variables. For example, what will be the forecast for inflation if the Federal Reserve Bank raises the interest rate? Would our forecast differ depending on the size of the increase in the interest rate?

## Unconditional Forecasts

An alternative is to separately forecast each independent variable and then compute the forecast for the dependent variable.  Yet another alternative is to use lagged variables as independent variables. Depending on the number of lags, we can forecast that much ahead into future (see Distributed Lag Section for details).

## Some practical issues

1. To forecast the dependent variable we first need to compute a forecast for the independent variable. Errors in this step induce errors later.
  
2.  *Spurious regression*: It is quite possible to find a strong linear relationship between two completely unrelated variables over time if they share a common time trend.

3. *Model Uncertainty*: We do not know the true functional form for the regression model and hence our estimated model is only a proxy for the true model.

4. *Parameter Uncertainty*: This kind of forecast uses regression coefficients that are computed using a fixed sample. Over time with new data, there will be changes in these coefficients.

## Distributed Lag Regression Models

Consider the following simple regression model:

\begin{equation}
y_t= \beta_0 +\beta_1 x_t + \epsilon_t
\end{equation}

Here, if want to forecast $y_{t+1}$ then we must either consider different scenarios for $x_{t+1}$ or indpendently forecast $x_{t+1}$ first, and then use it to compute forecast for $y_{t+1}$. An alternative is to estimate the following lagged regression model:

\begin{equation}
y_t= \beta_0 +\beta_1 x_{t-1} + \epsilon_t
\end{equation}

Note that by estimating the above model we get the following predicted value equation for $t+1$:

\begin{equation}
\widehat{y_{t+1}}=\widehat{\beta_0}+\widehat{\beta_1}x_{t}
\end{equation}

Hence, we can easily produce 1-period ahead forecast from this model. In order to produce forecast farther into future we would need to add more lags of the independent variable to the model. A generalized model of this kind is called *distributed lag model* and is given by:
\begin{equation}
y_t= \beta_0 +\sum_{s=1}^p\beta_s x_{t-s} + \epsilon_t
\end{equation}

The number of lags to include can be determined using some kind of goodness of fit measure. 

## Model Selection Criterion

Most often we compare models that have different number of independent variables. Here, we must account for the tradeoff between goodness of fit and degrees of freedom. Increasing the number of independent variables will:

1. lower the MSE and hence leads to better fit.

2. lowers the degrees of freedom

Two commonly used measures based on MSE incorporate this tradeoff:

1. Akaike Information Criterion (AIC):
\[ AIC= MSE \times e^{\frac{2k}{T}} \]

where $k$ is the number of estimated parameteris, $T$ is the sample size. Then, $K/T$ is the number of parameters estimated per observation and $e^{\frac{2k}{T}}$ is the *penalty factor* imposed on adding more variables to the model. As we increase $k$, this penalty factor will increase exponentially for a giveb value of $T$.

2. Bayesian Information Criterion (BIC):

\[ BIC= MSE \times T^{\frac{k}{T}} \]


Lower values of either AIC or BIC indicates greater accuracy. So we select a model with lower value of either of these two criteria. Note that the penalty imposed by BIC is harsher and hence it will typically select a more parsiomonius model.

