[
["index.html", "Applied Time Series Analysis Preface", " Applied Time Series Analysis Vipul Bhatt 2018-10-29 Preface These lecture notes are prepared for an upper level undergraduate course in time series econometrics. Every fall I teach a course on applied time series analysis at James Madison University. These notes borrow heavily from the teaching material that I have developed over several years of instruction of this course. One of my main objective is to develop a primer on time series analysis that is more accessible to undergraduate students than standard textbooks available in the market. Most of these textbooks in my opinion are densely written and assume advanced mathematical skills on the part of our students. Further, I have also struggled with their topic selection and organization. Often I end up not following the chapters in order and modify content (by adding or subtracting) to meet my students needs. Such changes causes confusion for some students and more importantly discourages optimal use of the textbook. Hence, this is an undertaking to develop a primer on time series that is accessible, follows a more logical sequencing of topics, and covers content that is most useful for undergraduate students in business and economics. Note: These notes have been prepared by me using various sources, published and unpublished. All errors that remain are mine. "],
["intro.html", "Chapter 1 Introduction to Forecasting 1.1 Time Series 1.2 Serial Correlation 1.3 Testing for Serial Correlion 1.4 White Noise Process 1.5 Important Elements of Forecasting 1.6 Loss Function and Optimal Forecast", " Chapter 1 Introduction to Forecasting 1.1 Time Series A time series is a specific kind of data where observations of a variable are recorded over time. For example, the data for the U.S. GDP for the last 30 years is a time series data. Such data shows how a variable is changing over time. Depending on the variable of interest we can have data measured at different frequencies. Some commonly used frequencies are intra-day, daily, weekly, monthly, quarterly, semi-annual and annual. Figure 1.1 below plots data for quarterly and monthly frequency. Figure 1.1: Time Series at quarterly and monthly frequency The first panel shows data for the real gross domestic product (GDP) for the US in billions of 2012 dollars, measured at a quarterly frequency. The second panel shows data for the advance retail sales (millions of dollars), measured at monthly frequency. Formally, we denote a time series variable by \\(y_t\\), where \\(t=0,1,2,..,T\\) is the observation index. For example, at \\(t=10\\) we get the tenth observation of this time series, \\(y_{10}\\). 1.2 Serial Correlation Serial correlation (or auto correlation) refers to the tendency of observations of a time series being correlated over time. It is a measure of the temporal dynamics of a time series and addresses the following question: what is the effect of past realizations of a time series on the current period value? Formally, \\[\\begin{equation} \\rho(s)=Cor(y_t, y_{t-s}) =\\frac{ Cov(y_t,y_{t-s})}{\\sqrt{\\sigma^2_{y_t} \\times \\sigma^2_{y_{t-s}}}} \\tag{1.1} \\end{equation}\\] where \\(Cov(y_t,y_{t-s})= E(y_t-\\mu_{y_t})(y_{t-s}-\\mu_{y_{t-s}})\\) and \\(\\sigma^2_{y_t}=E(y_t-\\mu_{y_t})^2\\) Here, \\(\\rho(s)\\) is the serial correlation of order \\(s\\). For example, \\(s=1\\) implies first order serial correlation between \\(y_t\\) and \\(y_{t-1}\\), \\(s=2\\) implies second order serial correlation between \\(y_t\\) and \\(y_{t-2}\\), and so on. Note that often we use historical data to forecast. If there is no serial correlation, then past can offer no guidance for the present and future. In that sense, presence of serial correlation of some order is the first condition for being able to forecast a time series using its historical realizations. Now, we can either have positive or negative serial correlation in data. Figure 1.2 plots two time series with positive and negative serial correlation, respectively. Figure 1.2: Serial Correlation Figure 1.2: Serial Correlation 1.3 Testing for Serial Correlion We can use a Lagrange-Multiplier (LM) test for detecting serial correlation. This test is also known as Breuch-Godfrey test. I will use the linear regression model to explain this test. Consider the following regression model: \\[\\begin{equation} y_t=\\beta_0 + \\beta_1 X_{1t}+\\epsilon_t \\end{equation}\\] Consider the following model for serial correlation of order p for the error term: \\[\\begin{equation} \\epsilon_t=\\rho_1 \\epsilon_{t-1}+\\rho_2 \\epsilon_{t-2}+...+ \\rho_p \\epsilon_{t-p}+\\nu_t \\tag{1.2} \\end{equation}\\] Then we are interested in the following test: \\[H_0=\\rho_1=\\rho_2=...=\\rho_p=0 \\] \\[H_A = Not \\ H_0 \\] To implement this test, we estimate the BG regression model given by: \\[\\begin{equation} e_t=\\alpha_0 + \\alpha_1 X_{1t}+ \\rho_1 e_{t-1}+\\rho_2 e_{t-2}+...+ \\rho_p e_{t-p}+\\nu_t \\tag{1.3} \\end{equation}\\] where we replacr the error term with the OLS residuals (denoted by \\(e\\)). The LM test statistic is given by: \\[ LM = N\\times R^2_{BG} \\sim \\chi^2_p \\] If the test statistic value is greater than the critical value then we reject the null hypothesis. 1.4 White Noise Process A time series is a white noise process is it has zero mean, constant and finite variance, and is serially uncorrelated. Formally, \\(y_t\\) is a white noise process if: \\(E(y_t)=0\\) \\(Var(y_t)=\\sigma^2_y\\) \\(Cov(y_t,y_{t-s})= 0 \\forall s\\neq t\\) We can compress the above definition as: \\(y_t\\sim WN(0,\\sigma^2_y)\\). Often we assume that the unexplained part of a time series follows a white noise process. Formally, \\[\\begin{equation} Time \\ Series \\ = \\ Explained \\ + \\ White \\ Noise \\end{equation}\\] By definition we cannot forecast a white noise process. An important diagnostics of model adequacy is to test whether the estimated residuals are white noise (more on this later). 1.5 Important Elements of Forecasting Definition 1.1 (Forecast) A forecast is an informed guess about the unknown future value of a time series of interest. For example, what is the stock price of Facebook next Monday? There are three possible types of forecasts: Density Forecast: we forecast the entire probability distribution of the possible future value of the time series of interest. Hence, \\[\\begin{equation} F(a)=P[y_{t+1}\\leq a] \\end{equation}\\] give us the probability that the 1-period ahead future value of \\(y_{t+1}\\) will be less than or equal to \\(a\\). For example, the future real GDP growth could be normally distributed with a mean of 1.3% and a standard deviation of 1.83%. Figure 1.3 below plots the density forecast for real GDP growth. Figure 1.3: Density Forecast for Future Real GDP Growth Point Forecast: our forecast at each horizon is a single number. Often we use the expected value or mean as the point forecast. For example, the point forecast for the 1-period ahead real GDP growth can be the mean of the probability distribution of the future real GDP growth: \\[\\begin{equation} f_{t,1}=1.3% \\end{equation}\\] Interval Forecast: our forecast at each horizon is a range which is obtained by adding margin of errors to the point forecast. With some probability we expect our future value to fall withing this range. For example, the 95% interval forecast for the next period real GDP growth is (-2.36%,4.96%). Hence, with 95% confidence we expect next period GDP to fall between -2.36% and 4.96%. Definition 1.2 (Forecast Horizon) Forecast Horizon is the number of periods into the future for which we forecast a time series. We will denote it by \\(h\\). Hence, for \\(h=1\\), we are looking at 1-period ahead forecast, for \\(h=2\\) we are looking at 2-period ahead forecast and so on. Formally, for a given time series \\(y_t\\), the h-period ahead unknown value is denoted by \\(y_{t+h}\\). The forecast of this value is denoted \\(f_{t,h}\\). Figure 1.4: Forecast Horizon Definition 1.3 (Forecast Error) A forecast error is the difference between the realization of the future value and the previously made forecast. Formally, the \\(h\\)-period ahead forecast error is given by: \\[\\begin{equation} e_{t,h}=y_{t+h}-f_{t,h} \\end{equation}\\] Hence, for every horizon, we will have a forecast and a corresponding forecast error. These errors can be negative (indicating over prediction) or positive (indicating under prediction). Definition 1.4 (Information Set) Forecasts are based on information available at the time of making the forecast. Information Set contains all the relevant information about the time series we would like to forecast. We denote the set of information available at time \\(T\\) by \\(\\Omega_T\\). There are two types of information sets: Univariate Information set: Only includes historical data on the time series of interest: \\[\\begin{equation} \\Omega_T=\\{y_T, y_{T-1}, y_{T-2}, ...., y_1\\} \\end{equation}\\] Multivariate Information set: Includes historical data on the time series of interest as well as any other variable(s) of interest. For example, suppose we have one more variable \\(x\\) that is relevant for forecasting \\(y\\). Then: \\[\\begin{equation} \\Omega_T=\\{y_T, x_T, y_{T-1}, x_{T-1}, y_{T-2},x_{T-2}. ...., y_1, x_1\\} \\end{equation}\\] 1.6 Loss Function and Optimal Forecast Think of a forecast as a solution to an optimization problem. When forecasts are wrong, the person making the forecast will suffer some loss. This loss will be a function of the magnitude as well as the sign of the forecast error. Hence, we can think of an optimal forecast as a solution to a minimization problem where the forecaster is minimizing the loss from the forecast error. Definition 1.5 (Loss Function) A loss function is a mapping between forecast errors and their associated losses. Formally, we denote the h-period ahead loss function by \\(L(e_{t,h})\\). For a function to be used as a loss function, three properties must be satisfied: \\(L(0)=0\\) \\(\\frac{dL}{de}&gt;0\\) \\(L(e)\\) is a continuous function. Two types of loss functions are: Symmetric Loss Function: both positive and negative forecast errors lead to same loss. See Figure 1.5. A commonly used loss function is quadratic loss function given by: \\[\\begin{equation} L(e_{t,h})=e_{t,h}^2 = (y_{t+h}-f{t,h})^2 \\end{equation}\\] Figure 1.5: Quadratic Loss Functions Asymmetric Loss Function: loss depends on the sign of the forecast error. For example, it could be that positive errors produce greater loss when compared to negative errors. See the function below and Figure 1.6 that attaches a higher loss to positive errors: \\[\\begin{equation} L(e_{t,h})=e_{t,h}^2+4 \\times e_{t,h} \\end{equation}\\] Figure 1.6: Asymmetric Loss Function Once we have chosen our loss function, the optimal forecast can be obtained by minimizing the expected loss function. Definition 1.6 (Optimal Forecast) An optimal forecast minimizes the expected loss from the forecast, given the information available at the time. Mathematically, we denote it by \\(f^*_{t,h}\\) and it solves the following minimization problem: \\[\\begin{equation} min_{f_{t,h}} E(L(e_{t,h})|\\Omega_t) \\end{equation}\\] In theory we can assume any functional form for the loss function and that will lead to a different optimal forecast. An important result that follows from a specific functional form is stated as Theorem 1.1. Theorem 1.1 If the loss function is quadtratic then the optimal forecast is the conditional mean of the time series of interest. Formally, if \\(L(e_{t,h})=e_{t,h}^2\\) then, \\[\\begin{equation} f^*_{t,h}=E(y_{t+h}|\\Omega_t) \\end{equation}\\] Note that \\(E(e_{t,h}^2)\\) is known as mean squared errors (MSE). Hence, the expected loss from a quadratic loss function is the same as the MSE. In this course, we assume that the forecaster faces a quadratic loss function and hence based on Theorem 1.1, we will learn different models for estimating the conditional mean of the future value of the time series of interest, i.e., \\(E(y_{t+h}|\\Omega_t)\\). "],
["regression-based-forecasting.html", "Chapter 2 Regression-based Forecasting 2.1 Scenario Analysis and Conditional Forecasts 2.2 Unconditional Forecasts 2.3 Some practical issues 2.4 Distributed Lag Regression Models 2.5 Application: A Model of Investment Expenditure", " Chapter 2 Regression-based Forecasting One way to compute the conditional expectation is the linear regression model. Here, our information set contains data on all relevant explanatory variables available at the time of forecast, i.e, \\[\\begin{equation} \\Omega_t={X_{1t}, X_{2t},...X_{Kt}} \\end{equation}\\] Hence, we get the following equality: \\[\\begin{equation} E(y_t|\\Omega_t)=E(y_{t}|X_{1t}, X_{2t}, X_{3t},...,X_{Kt}) \\end{equation}\\] The right hand side of the above equation is the multiple regression model of the form: \\[\\begin{equation} y_{t}=\\beta_0+\\beta_1 X_{1t}+\\beta_2 X_{2t}+..+\\beta_K X_{Kt}+\\epsilon_t \\end{equation}\\] We can easily estimate the above model using Ordinary Least Squares (OLS) and compute the predicted value of \\(y\\): \\[\\begin{equation} \\widehat{y}_t = \\widehat{\\beta_0} +\\widehat{\\beta_1} X_{1t} +\\widehat{\\beta_2} X_{2t}+...+ \\widehat{\\beta_k} X_{Kt} \\end{equation}\\] The above equation can be used to compute the optimal forecast. Suppose, we are interested in computed the \\(h\\) period ahead forecast for \\(y\\). Then, using the above equation we get: \\[\\begin{equation} \\widehat{y}_{t+h} = \\widehat{\\beta_0} +\\widehat{\\beta_1} X_{1t+h} +\\widehat{\\beta_2} X_{2t+h}+...+ \\widehat{\\beta_k} X_{Kt+h} \\end{equation}\\] 2.1 Scenario Analysis and Conditional Forecasts One way to use a regression model to produce forecasts is called scenario analysis where we produce a different forecast for the dependent under each possible scenario about the future values of the independent variables. For example, what will be the forecast for inflation if the Federal Reserve Bank raises the interest rate? Would our forecast differ depending on the size of the increase in the interest rate? 2.2 Unconditional Forecasts An alternative is to separately forecast each independent variable and then compute the forecast for the dependent variable. Yet another alternative is to use lagged variables as independent variables. Depending on the number of lags, we can forecast that much ahead into future (see Distributed Lag Section for details). 2.3 Some practical issues To forecast the dependent variable we first need to compute a forecast for the independent variable. Errors in this step induce errors later. Spurious regression: It is quite possible to find a strong linear relationship between two completely unrelated variables over time if they share a common time trend. Model Uncertainty: We do not know the true functional form for the regression model and hence our estimated model is only a proxy for the true model. Parameter Uncertainty: This kind of forecast uses regression coefficients that are computed using a fixed sample. Over time with new data, there will be changes in these coefficients. 2.4 Distributed Lag Regression Models Consider the following simple regression model: \\[\\begin{equation} y_t= \\beta_0 +\\beta_1 x_t + \\epsilon_t \\end{equation}\\] Here, if want to forecast \\(y_{t+1}\\) then we must either consider different scenarios for \\(x_{t+1}\\) or independently forecast \\(x_{t+1}\\) first, and then use it to compute forecast for \\(y_{t+1}\\). An alternative is to estimate the following lagged regression model: \\[\\begin{equation} y_t= \\beta_0 +\\beta_1 x_{t-1} + \\epsilon_t \\end{equation}\\] Note that by estimating the above model we get the following predicted value equation for \\(t+1\\): \\[\\begin{equation} \\widehat{y_{t+1}}=\\widehat{\\beta_0}+\\widehat{\\beta_1}x_{t} \\end{equation}\\] Hence, we can easily produce 1-period ahead forecast from this model. In order to produce forecast farther into future we would need to add more lags of the independent variable to the model. A generalized model of this kind is called distributed lag model and is given by: \\[\\begin{equation} y_t= \\beta_0 +\\sum_{s=1}^p\\beta_s x_{t-s} + \\epsilon_t \\end{equation}\\] The number of lags to include can be determined using some kind of goodness of fit measure. 2.4.1 Dynamic Effect of X on Y A very useful benefit of estimating a distributed lag model is that it allows us to measure how changes in \\(x\\) in the current period can impact the dependent variable over time. Consider a simple distributed lag model with two lags: \\[\\begin{equation} y_t=\\beta_0 + \\beta_1 x_{t-1} + \\beta_2 x_{t-2} +\\epsilon_t \\end{equation}\\] In this model the lag structure implies that any change in \\(x\\) will persist for two periods in terms of its effect on \\(y\\). In fact we now have to consider the dynamic effect of \\(x\\) on \\(y\\). Formally, there are two types of effects: dynamic effect of \\(x\\) on \\(y\\) given by: \\[ \\frac{\\partial y_{t+s}}{\\partial x_t} \\quad s=0,1,2,... \\] In our example, the sequence of dynamic effects are: \\[\\begin{equation} \\frac{\\partial y_{t}}{\\partial x_t} =0; \\ \\frac{\\partial y_{t+1}}{\\partial x_t}=\\beta_1; \\ \\frac{\\partial y_{t+2}}{\\partial x_t}=\\beta_2; \\ \\frac{\\partial y_{t+s}}{\\partial x_t}=0 \\ \\forall \\ s&gt;2 \\end{equation}\\] long run effect of \\(x\\) on \\(y\\) given by: \\[\\begin{equation} \\sum_{s=0}^p\\frac{\\partial y_{t+s}}{\\partial x_t} \\end{equation}\\] In our example, the long run effect is: \\[\\beta_1+\\beta_2\\] 2.4.2 Model Selection Criterion Most often we compare models that have different number of independent variables. For example, in our application, in order to select the number of lags for output and capital stock, we will essentially compare models with different number of independent variables. In such cases we must account for the trade-off between goodness of fit and degrees of freedom. Increasing the number of independent variables will: lower the MSE and hence leads to better fit. lowers the degrees of freedom Two commonly used measures based on MSE incorporate this trade-off: Akaike Information Criterion (AIC): \\[ AIC= MSE \\times e^{\\frac{2k}{T}} \\] where \\(k\\) is the number of estimated parameters, \\(T\\) is the sample size. Then, \\(K/T\\) is the number of parameters estimated per observation and \\(e^{\\frac{2k}{T}}\\) is the penalty factor imposed on adding more variables to the model. As we increase \\(k\\), this penalty factor will increase exponentially for a given value of \\(T\\). Bayesian Information Criterion (BIC): \\[ BIC= MSE \\times T^{\\frac{k}{T}} \\] Lower values of either AIC or BIC indicates greater accuracy. So we select a model with lower value of either of these two criteria. Note that the penalty imposed by BIC is harsher and hence it will typically select a more parsimonious model (Figure 2.1). Figure 2.1: Penalty Factor of AIC and BIC 2.5 Application: A Model of Investment Expenditure 2.5.1 A Multiple Regression Model of Invesment Expenditure Suppose have annual data on private investment, private sector output, and capital stock. Our model specification is given by: \\[\\begin{equation} y_t= \\beta_0 + \\beta_1 x_{1t}+ \\beta_2 x_{2t}+\\epsilon_t \\end{equation}\\] We can estimate the above model using OLS and then conduct scenario-based forecasting. For ease of interpretation, we will convert all variables in natural logarithms. Table 2.1 below presents the estimated coefficients of our regression model. Higher output and capital stock leads to greater investment expenditure. Table 2.1: A Multiple Regression Model of Investment Expenditure Estimated Coefficients Std. Error t-ratio p-value (Intercept) -4.8421855 0.9623332 -5.031714 0.0000044 x1 0.9987751 0.2418282 4.130102 0.0001104 x2 0.4204833 0.3643054 1.154205 0.2528456 Next, we forecast of investment expenditure under three different scenarios: For next 3 years, both output and capital stock remain at the average of last 3 years. For next 3 years, both output and capital stock remain at 1% above the average of last 3 years. For next 3 years, both output and capital stock remain at 1% below the average of last 3 years. Figure 2.2 below present our investment expenditure outlook under these 3 scenarios. Figure 2.2: Investment outlook for next 3 years 2.5.2 A Distributed Lag Model of Investment Expenditure In this application we will estimate a distributed lag model for investment expenditure. The idea here is that it takes time for investment to respond to output and capital stock changes. The model specification we want to estimate is: \\[\\begin{equation} y_t= \\beta_0 + \\sum_{i=1}^p\\beta_i x_{1t-i}+\\sum_{i=1}^p\\alpha_i x_{2t-i}+\\epsilon_t \\end{equation}\\] where \\(y\\) denotes real investment expenditure of the private sector, \\(x_1\\) denotes output of the private sector, and \\(x_2\\) denotes capital stock of the private sector. We estimate our model by first selecting the optimal lag order for each independent variable, and selecting the one with lowest value for AIC/BIC. From @(tab:ch2-table2) we find that the lowest BIC occurs at lag=2. Hence, we estimate a model with two lags for each independent variable in our model. Table 2.2: Optimal Order of the lags Lag AIC BIC 1 -98.42469 -89.72714 2 -127.36043 -114.31411 3 -129.95355 -112.55845 4 -127.49885 -105.75498 Hence, our final model is given by: \\[\\begin{equation} y_t= \\beta_0 + \\sum_{i=1}^2\\beta_i x_{1t-i}+\\sum_{i=1}^2\\alpha_i x_{2t-i} \\end{equation}\\] The results of our estimation are presented below in Table 2.3 Table 2.3: Distributed Lag Model of Investment Expenditure Estimated Coefficients Std. Error t-ratio p-value (Intercept) -6.541336 0.8300877 -7.880295 0.0000000 L(x1, 1:2)1 10.932987 1.9354346 5.648854 0.0000005 L(x1, 1:2)2 -9.769632 1.9286226 -5.065601 0.0000044 L(x2, 1:2)1 2.545657 0.7735580 3.290842 0.0017028 L(x2, 1:2)2 -2.189088 0.7904689 -2.769354 0.0075309 Using our estimated model we can easily compute the dynamic effect as well as the long run effect of each independent variable on the dependent variable. Given the lag structure of our estimated model, we can also produce forecasts for \\(y_{t+1}\\) by computing the following equation: \\[\\begin{equation} f_{t,1}=\\widehat{y_{t+1}}=\\hat{\\beta_0}+\\hat{\\beta_1}x_{1t} + \\hat{\\beta_2}x_{1t-1}+ \\hat{\\alpha_1}x_{2t}+\\hat{\\alpha_2}x_{2t-1} \\end{equation}\\] "],
["components-of-a-time-series.html", "Chapter 3 Components of a Time Series 3.1 Decomposing a time series 3.2 Uses of Decomposition of a time series", " Chapter 3 Components of a Time Series A given time series can have four possible components: Trend: denoted by \\(B_t\\) captures the long run behavior of the time series of interest. Season: denoted by \\(S_t\\) are periodic fluctuations over seasons. The period of the season is fixed and known. For example, rise in non-durable sales during Christmas. Cycle: denoted by \\(C_t\\) are non-periodic are fluctuations in that they occur regularly but over periods that are not fixed in duration. Irregular: denoted by \\(\\epsilon_t\\) are random fluctuations, typically modeled as a white noise process. 3.1 Decomposing a time series We can decompose any given time series into its components. There are two ways to accomplish this: Additive Decomposition: Here it is assumed that all four components are added to obtain the underlying timer series: \\[\\begin{equation} y_t= B_t+S_t+C_t +\\epsilon_t \\end{equation}\\] Multiplicative Decomposition: Here it is assumed that all four components are multiplied to obtain the underlying timer series: \\[\\begin{equation} y_t= B_t \\times S_t \\times C_t \\times \\epsilon_t \\end{equation}\\] Note that using properties of logarithms, multiplicative decomposition is the same as additive decomposition in log terms: \\[\\begin{equation} log(y_t)= log(B_t) + log(S_t) + log(C_t) + log(\\epsilon_t) \\end{equation}\\] Most statistical software can implement these decomposition using data on a time series variable as input. Typically they combine cyclical component with irregular component and provide a three-way decomposition. In Figure 3.1 I use R to decompose real GDP for the US into its components. Figure 3.1: Additive Decomposition of Retail Sales 3.2 Uses of Decomposition of a time series The usefulness of decomposing a time series depends on our objective. It may be of interest to study each component separately or to simply improve our understanding of the temporal dynamics of a time series of interest. Decomposing it into different components is the first step towards achieving that goal. We can also use the decomposition to filter out components that we are not interested in studying. If for example we are only interested in modeling the cyclical component of the time series, then we can assume some kind decomposition, additive or multiplicative, and filter out the trend and seasonal component. For example, assuming additive decomposition, the filtered time series is given by: \\[\\begin{equation} Filtered \\ y_t= y_t-B_t-S_t \\end{equation}\\] We can then proceed to model the cyclical component using the filtered data. "],
["smoothing-methods.html", "Chapter 4 Smoothing Methods 4.1 Moving Average Method 4.2 Simple Exponential Smoothing 4.3 Holt-Winters Smoothing 4.4 Holt-Winters Smoothing with Seasonality 4.5 Application", " Chapter 4 Smoothing Methods One way to approach forecasting is to average out the fluctuations in the underlying time series to produce a smoothed data which can be extrapolated to produce forecasts. These smoothing methods are essentially model-free and may not even produce optimal forecasts. Depending on the method used one can accommodate seasonal as well as trend components of the underlying time series. 4.1 Moving Average Method We compute an average of most recent data values for the time series and use it as a forecast for the next period. An important parameter is the window over which we take the average. Let us denote this window by \\(m\\), then: \\[\\begin{equation} y^s_{t+1}=\\frac{\\sum \\limits_{i=t-m+1}^{t}{y_i}}{m} \\end{equation}\\] A larger value of \\(m\\) produces greater smoothing and most software have a default value of this parameter which can be changed if needed. 4.2 Simple Exponential Smoothing In the moving average method, all observations received same weight. However, it is reasonable to argue that more recent observations may have a greater influence than those in the remote past. In this method, the weight attached to past observations exponentially decay over time. Here is the algorithm for computing the smoothed data and its forecast: Initialize at t=1: \\[y_1^s=y_1\\] Update: \\[y_{t}^{s}= \\alpha y_t + (1-\\alpha)y_{t-1}^{s} \\quad for \\ t=2,3,...T\\] 3: h-period ahead forecast: \\[f_{T,h}= y_T^s\\] Here the h-period ahead forecast is: Exercise: Can you show that \\(y_{t}^{s}\\) is a is the weighted moving average of all past observations? Use backward substitution method. Here \\(\\alpha \\in (0,1)\\) is the smoothing parameter, with smaller value indicating greater smoothing. 4.3 Holt-Winters Smoothing We add trend component to the simple exponential smoothing. In step 2 the equation we use to update the smoothed data is given by: \\[\\begin{align} y_{t}^{s}= \\alpha y_t + (1-\\alpha)(y_{t-1}^{s}+B_{t-1}) \\\\ \\nonumber B_t = \\beta (y_t^s -y_{t-1}^s) + (1-\\beta) B_{t-1} \\end{align}\\] We now have an additional parameter \\(\\beta\\) that is the trend parameter. Here the h-period ahead forecast is: \\[\\begin{align} f_{T,h} = y_T^s + h\\times B_T \\end{align}\\] 4.4 Holt-Winters Smoothing with Seasonality We now add seasonal component along with trend. Assuming multiplicative seasonality with period \\(n\\): \\[\\begin{align} y_{t}^{s}= \\alpha \\frac{y_t}{S_{t-n}} + (1-\\alpha)(y_{t-1}^{s}+B_{t-1})\\\\ B_t = \\beta (y_t^s -y_{t-1}^s) + (1-\\beta) B_{t-1}\\\\ S_t = \\gamma\\frac{y_t}{y_t^s}+(1-\\gamma)S_{t-n} \\end{align}\\] The h-period ahead forecast is given by: \\[\\begin{equation} f_{T,h}= (y_T^s + h\\times B_T) \\times S_{T+h-n} \\end{equation}\\] 4.5 Application We use R to implement a 12-period ahead forecast for new housing starts for the U.S. The data is at monthly frequency from June 2000 through June 2018. The resulting forecasts are plotted in Figure 4.1. Figure 4.1: Forecast of Housing Starts: Three Smoothing Methods "],
["modeling-trend-and-seasonal-components.html", "Chapter 5 Modeling Trend and Seasonal Components 5.1 Trend Estimation 5.2 Seasonal Model", " Chapter 5 Modeling Trend and Seasonal Components 5.1 Trend Estimation An important component of a time series is trend that captures the long run evolution of the variable of interest. There are two types of trends: Deterministic Trend: the underlying trend component is a known function of time with unknown parameters. Stochastic Trend: the trend component is random. In this note we will focus on estimating and forecasting deterministic trend models. We will come back to stochastic trend later when we talk about stationarity property of a time series. 5.1.1 Parametrizing a deterministic trend Whether or not there is deterministic trend in the data can be typically gleaned by simply plotting the time series over time. For example, Figure @ref(fig: ch5-figure1) below plots real GDP for the US at quarterly frequency. We can observe a positive time trend with real GDP increasing with time. In this section we will learn to fit a function that captures this relationship accurately. Figure 5.1: Real GDP (2012 Chained Billions of Dollars) Note: The variable time is denoted by \\(t\\) and it is artificially created to take value of 1 for the first period, 2 for the second period and so on. There are two commonly used functional forms for capturing a deterministic trend: Polynomial Trend: We fit a polynomial of appropriate order to capture the time trend. For example, A. Linear trend: \\[\\begin{equation} y_t=\\beta_0 +\\beta_1 t +\\epsilon_t \\end{equation}\\] B. Quadratic trend: \\[\\begin{equation} y_t=\\beta_0 +\\beta_1 t + \\beta_2 t^2 +\\epsilon_t \\end{equation}\\] In general, we can fit a polynomial of order \\(q\\): \\[\\begin{equation} y_t=\\beta_0 + \\sum_{i=1}^q \\beta_i t^i +\\epsilon_t \\end{equation}\\] We can estimate this model using the OLS. One of the key component here is to determine the right order of the polynomial. We can begin with a large enough number for \\(q\\) and then select the appropriate order using AIC or BIC criterion. Exponential or log-linear trend: In some cases we may want to use an exponential trend or equivalently a log-linear trend. \\[\\begin{align} y_t=e^{(\\beta_0 +\\beta_1 t +\\epsilon_t)}\\\\ equivalently\\\\ log(y_t)=\\beta_0 +\\beta_1 t +\\epsilon_t \\end{align}\\] Again we can estimate the above model using OLS. 5.1.2 Uses of the Deterministic Trend Model Once we have finalized our deterministic trend model i.e., either a polynomial of a specific order or log-liner trend, we can use the estimated model for the following two purposes: Detrending our data: Suppose we would like to eliminate trend from our data. The residual from our final trend model is the detrended time series. Forecasting: We can also forecast our time series based on the estimated trend. For example, suppose our final model is a quadratic trend. The predicted value is given by: \\[\\begin{equation} \\widehat{y_t}=\\widehat{\\beta_0}+\\widehat{\\beta_1} t + \\widehat{\\beta_2} t^2 \\end{equation}\\] Then, the 1-period ahead forecast for \\(y_{t+1}\\) can be obtained by solving: \\[\\begin{equation} \\widehat{y_{t+1}}=\\widehat{\\beta_0}+\\widehat{\\beta_1} (t+1) + \\widehat{\\beta_2} (t+1)^2 \\end{equation}\\] 5.1.3 Application: Estimating a polynomial trend for U.S. Real GDP We will now fit a polynomial trend to the US real GDP data that was presented in Figure ??. We first estimate polynomials of different orders and select the optimal order determined by the lowest possible AIC/BIC. Table 5.1. shows these statistics for up to 4th order polynomial. We find that the lowest value occur at \\(q=4\\). Table 5.1: Optimal Order of the Polynomial order AIC BIC 1 4716.836 4727.794 2 4133.469 4148.079 3 4105.087 4123.350 4 4002.212 4024.127 Hence, our final trend model is: \\[\\begin{equation} y_t=\\beta_0 +\\beta_1 t + \\beta_2 t^2 + \\beta_3 t^3 + \\beta_4 t^4 +\\epsilon_t \\end{equation}\\] The estimated trend model is presented in Table 5.2. Table 5.2: Regression Results Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1714.277 81.019 21.159 0 trend 43.398 3.911 11.097 0 I(trend^2) -0.344 0.055 -6.194 0 I(trend^3) 0.003 0.000 10.233 0 I(trend^4) 0.000 0.000 -11.160 0 Using the estimated model, we can compute the detrended data as the residual and also forecast \\(y_t\\). Figure 5.2 below plots the detrended real GDP obtained as a residual from our trend model. Figure 5.2: Detrended Real GDP Figure 5.3 shows the forecast of real GDP for next 8 quarters along with the 95% confidence bands. Figure 5.3: Forecast of Real GDP 5.2 Seasonal Model We now focus on the seasonal component of a time series, i.e., that is periodic fluctuations that repeat themselves every season. For example, increase in ice cream sales during summer season. Just like trend component, such seasonal pattern could be deterministic or stochastic. In this chapter we will focus on estimating deterministic seasonal component. In Figure 5.4 we plot housing starts in the U.S. The data is at monthly frequency and we can see a clear seasonal pattern. Housing starts seem to increase in spring and summer months. This is followed by a decline in fall and winter months. Figure 5.4: Housing Starts in U.S. One option to deal with seasonality is to either obtain seasonally adjusted data from the source itself. Alternatively, we can use decomposition method and appropriately filter out the seasonal component. However, if our objective is to explicitly model the seasonal component of a time series then we must work with non-seasonally adjusted data. 5.2.1 Regression Model with Seasonal Dummy Variables One way to account for seasonal patterns in data is to add dummy variables for season. To avoid perfect multicollinearity, is there are \\(s\\) seasons, we can include \\(s-1\\) dummy variables. For example, for quarterly data, \\(s=4\\) and hence we need \\(s-1=3\\) dummy variables in our regression model. Formally, for quarterly data, the seasonal regression model is given by: \\[\\begin{equation} y_t= \\beta_0 + \\beta_1 D_{1t}+ \\beta_2 D_{2t} + \\beta_3 D_{3t} + \\epsilon_t \\end{equation}\\] In the above regression model, \\(D_1,D_2,\\) and \\(D_3\\) are dummy variables that capture first three quarters of the year. For example, \\(D_1=1\\) for the first quarter and \\(D_1=0\\) otherwise. Similarly, \\(D_2=1\\) for the second quarter and \\(D_2=0\\) otherwise. In this example, we use the fourth quarter as the base group. The above model can be estimated using OLS. Again, we can use the residual from our estimated model as a measure of deseasonlized data. We can also forecast the dependent variable based on the seasonal component only. 5.2.2 Application: Seasonal Model of Housing Starts We now estimate a seasonal regression model for the housing starts data presented in Figure 5.4. The data is at monthly frequency which implies we can have 12 possible seasons and hence would need 11 dummy variables in our regression model. Formally, we use January as the base group and include dummy variables for the last 11 months of the year: \\[\\begin{equation} y_t=\\beta_0 + \\sum_{i=2}^{12}\\beta_i D_{it} + \\epsilon_t \\end{equation}\\] Table 5.3 presents the estimation results for this exercise. In Figure 5.5 we plot the forecast of housing starts for next 12 months using our estimated model, along with 95% confidence bands. Table 5.3: Regression Results Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 84.422 9.852 8.569 0.000 season2 2.283 13.933 0.164 0.870 season3 19.133 13.933 1.373 0.171 season4 28.350 13.933 2.035 0.043 season5 34.061 13.933 2.445 0.015 season6 35.562 13.749 2.587 0.010 season7 33.422 13.933 2.399 0.017 season8 27.794 13.933 1.995 0.047 season9 26.139 13.933 1.876 0.062 season10 25.622 13.933 1.839 0.067 season11 10.489 13.933 0.753 0.452 season12 0.639 13.933 0.046 0.963 Figure 5.5: Forecast of Housing Starts "],
["modeling-cycle.html", "Chapter 6 Modeling Cycle 6.1 Stationarity and Autocorrelation 6.2 Autoregressive (AR) Model 6.3 Estimating an AR model 6.4 Moving Average (MA) Model 6.5 ARMA(p, q) 6.6 Testing for Unit root", " Chapter 6 Modeling Cycle In this chapter we will focus on the cyclical component of a time series and hence focus on data that either has no trend and seasonal components, or data that is filtered to eliminate any trend and seasonality. One of the most commonly used method to model cyclicality is the Autogressive Moving Average (ARMA). This model has two distinct components: Autoregressive (AR) component: the current period value of a time series variable depends on its past (lagged) observations. We use \\(p\\) to denote the order of the AR component and is the number of lags of a variable that directly affect the current period value. For example, a firm’s production in the current period maybe impacted by past levels of production. If last year’s production exceeded demand, the stock of unsold goods may be used to meet this period demand first, hence lowering the current period production. Moving average (MA) component: the current period value of a time series variable depends on current period shock as well as past shocks to this variable. We use \\(q\\) to denote the order of the MA component and is the number of past period shocks that affect the current period value of the variable of interest. For example, if the Federal Reserve Bank raises the interest in 2016, the effects of that policy shock may impact investment and consumption spending in 2017. Before we consider these time series model in details it is useful to discuss certain properties of time series that allow us a better understanding of these models. 6.1 Stationarity and Autocorrelation 6.1.1 Covariance Stationary Time Series Definition 6.1 (Covariance Stationary Time Series) A time series \\(\\{y_t\\}\\) is said to be a covariance stationary process if: \\(E(y_t)=\\mu_y \\quad \\forall \\quad t\\) \\(Var(y_t)=\\sigma_y^2 \\quad \\forall \\quad t\\) \\(Cov(y_t,y_{t-s})=\\gamma(s) \\quad \\forall \\quad s\\neq t\\) One way to think about stationarity is mean-reversion, i.e, the tendency of a time series to return to its long-run unconditional mean following a shock (or a series of shock). Figure @(fig:ch6-figure1) below shows this property graphically. Figure 6.1: Reversion to mean Figure 6.2: Reversion to mean in practice In practice however, you will not be able to visualize a mean-reverting stationary process this clearly. For example, in Figure 6.2 we plot real GDP growth for the U.S. which is a stationary process with a mean of 0.7%. In this chapter we will only consider stationary time series data. Later on we will learn how to work with non-stationary data. 6.1.2 Correlation vs Autocorrelation In statistics, correlation is a measure of relationship between two variables. In the time series setting, we can think of the current period value and the past period value of a variable as two separate variables, and compute correlation between them. Such a correlation, between current and lagged observation of a time series is called serial correlation or autocorrelation. In general, for a time series, \\(\\{y_t\\}\\), the autocorrelation is given by: \\[\\begin{align} Cor(y_t,y_{t-s})=\\frac{ Cov(y_t,y_{t-s})}{\\sqrt{\\sigma^2_{y_t} \\times \\sigma^2_{y_{t-s}}}} \\end{align}\\] where \\(Cov(y_t,y_{t-s})= E(y_t-\\mu_{y_t})(y_{t-s}-\\mu_{y_{t-s}})\\) and \\(\\sigma^2_{y_t}=E(y_t-\\mu_{y_t})^2\\) For a stationary time series, using the three conditions the Autocorrelation Function (ACF) denoted by \\(\\rho(s)\\) is given by: \\[\\begin{align} ACF(s) \\ or \\ \\rho(s)=\\frac{\\gamma(s)}{\\gamma(0)} \\end{align}\\] Non-zero values of the ACF indicates presences of serial correlation in the data. Figure 6.3 shows the ACF for a stationary time series with positive serial correlation. If your data is stationary then the ACF should eventually converge to 0. For a non-stationary data, the ACF function will not decay over time. Figure 6.3: ACF for a Stationary Time Series 6.1.3 Partial Autocorrelation Definition 6.2 (Partial Auto Correlation Function (PACF)) The ACF captures the relationship between the current period value of a time series and all of its past observations. It includes both direct as well as indirect effects of the past observations on the current period value. Often times it is of interest to measure the direct relationship between the current and past observations, partialing out all indirect effects. The partial autocorrelation function (PACF) for a stationary time series \\(y_t\\) at lag \\(s\\) is the direct correlation between \\(y_t\\) and \\(y_{t-s}\\), after filtering out the linear influence of \\(y_{t-1},\\ldots,y_{t-s-1}\\) on \\(y_t\\). Figure 6.4 below shows the PACF for a stationary time series where only one lag directly affects the time series in the current period. Figure 6.4: PACF for a Stationary Time Series 6.1.4 Lag operator A lag operator denoted by \\(L\\) allows us to write ARMA models in a more concise way. Applying lag operator once moves the time index by one period; applying it twice moves the time index back by two period; applying it \\(s\\) times moves the index back by \\(s\\) periods. \\[ Ly_t=y_{t-1} \\] \\[ L^2y_t=y_{t-2} \\] \\[ L^3y_t=y_{t-3} \\] \\[\\vdots\\] \\[ L^sy_t=y_{t-s} \\] 6.2 Autoregressive (AR) Model A stationarytime series \\(\\{x_t\\}\\) can be modeled as an AR process. In general, an AR(p) model is given by: \\[\\begin{equation} y_t = \\phi_0 +\\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ...... + \\phi_p y_{t-p}+\\epsilon_t \\end{equation}\\] Here \\(\\phi_i\\) captures the effect of \\(y_{t-i}\\) on \\(y_t\\). The order of the AR process is not known apriori. It is common to use either AIC or BIC to determine the optimal lag length for an AR process. Using the Lag operator, we can rewrite the above AR(p) model as follows: \\[ \\Phi(L)y_t=\\phi_0+\\epsilon_t \\] where \\(\\displaystyle \\Phi(L)\\) is a polynomial of degree \\(p\\) in L: \\[ \\Phi(L) = 1-\\phi_1 L - \\phi_2 L^2- \\ldots\\ldots\\ldots\\ldots -\\phi_p L^p\\] For example, an AR(1) model can be written as: \\[y_t=\\phi_0+\\phi_1 y_{t-1} + \\epsilon_t \\Rightarrow \\Phi(L)y_t=\\phi_0+\\epsilon_t\\] where, \\[ \\Phi(L) = 1-\\phi_1 L \\] Characteristic equation: A characteristic equation is given by: \\[\\Phi(L)=0\\] The roots of this equation play an important role in determining the dynamic behavior of a time series. 6.2.1 Unit root and Stationarity For a time series to be stationary there should be no unit root in its characteristic equation. In other words, all roots of the characteristic equation must fall outside the unit circle. Consider the following AR(1) model: \\[\\Phi(L)y_t = \\phi_0 + \\epsilon_t\\] The characteristic equation is given by: \\[\\Phi(L)=1-\\phi_1L=0 \\] The root that satisfies the above equation is: \\[ L^*=\\frac{1}{\\phi_1}\\] For no unit root to be present, \\(L^*&gt;|1|\\) which implies that \\(|\\phi_1|&lt;1\\). Typically, for any AR process to be stationary, some restrictions will be imposed on the values of \\(\\phi_i&#39;s\\), the coefficients of the lagged variables in the model. 6.2.2 Properties of an AR(1) model A stationary AR(1) model is given by: \\[ y_t=\\phi_0 +\\phi_1 y_{t-1}+ \\epsilon_t \\quad ; \\ \\epsilon_t\\sim WN(0, \\sigma_\\epsilon^2) \\ and \\ |\\phi_1|&lt;1\\] \\(\\displaystyle \\phi_1\\) measures the persistence in data. A larger value indicates shocks to \\(y_t\\) dissipate slowly over time. Stationarity of \\(y_t\\) implies certain restrictions on the AR(1) model. Constant long run mean: is the unconditional expectation of \\(y_t\\): \\[ E(y_t) = \\mu_y= \\frac{\\phi_0}{1-\\phi_1} \\] Constant long run variance: is the unconditional variance of \\(y_t\\): \\[ Var(y_t)=\\sigma^2_y= \\frac{\\sigma^2_\\epsilon}{1-\\phi_1^2}\\] ACF function: \\[ \\rho(s) = \\phi_1^s\\] PACF function: \\[\\begin{equation*} PACF(s) = \\begin{cases} \\phi_1 &amp; \\text{if s=1}\\\\ 0 &amp; \\text{if s&gt;1} \\end{cases} \\end{equation*}\\] 6.3 Estimating an AR model When estimating the AR model we have two alternatives: OLS: biased (but consistent) estimates. Also, later on when we add MA components we cannot use OLS. Maximum Likelihood Estimation (MLE): can be used to estimate AR as well as MA components 6.3.1 Maximum Likelihood Estimation (MLE) MLE approach is based on the following idea: what set of values of our parameters maximize the likelihood of observing our data if the model we have was used to generate this data. Likelihood function: is a function that gives us the probability of observing our data given a model with some parameters. 6.3.1.1 Likelihood vs Probability Consider a simple example of tossing a coin. Let \\(X\\) denotes the random variable that is the outcome of this experiment being either heads or tails. Let \\(\\theta\\) denote the probability of heads which implies \\(1-\\theta\\) is the probability of obtaining tails. Here, \\(\\theta\\) is our parameter of interest. Suppose we toss the coin 10 times and obtain the following data on \\(X\\): \\[X=\\{H,H,H,H,H,H,T,T,T,T\\}\\] Then, the probability of obtaining this sequence of X is given by: \\[Prob (X|\\theta)=\\theta^6 (1-\\theta)^4\\] This is the probability distribution function the variable \\(X\\). As we change \\(X\\), we get a different probability for a given value of \\(\\theta\\). Now let us ask a different question. Once we have observed the sequence of heads and tails, lets call it our data which is fixed. Then, what is probability of observing this data, if our probability distribution function is given by the equation above? That gives us the likelihood function: \\[ L(\\theta)=Prob(X|\\theta)=\\theta^6(1-\\theta)^4\\] Note that with fixed \\(X\\), as we change \\(\\theta\\) the likelihood of observing this data will change. This is an important point that distinguishes likelihood function from the probability distribution function. Although both have the same equation, the probability function is a function of the data with the value of the parameter fixed, while the likelihood function is a function of the parameter with the data fixed. 6.3.1.2 Maximum Likelhood Estimation Now we are in a position to formally define the likelihood function. Definition 6.3 Let \\(X\\) denotes a random variable with a given probability distribution function denoted by \\(f(x_i|\\theta)\\). Let \\(D=\\{x_1, x_2,\\dots,x_n\\}\\) denote a sample realization of \\(X\\). Then, the likelhood function, denoted by \\(L(\\theta)\\) is given by: \\[L(\\theta)=f(x_1,x_2,\\dots,x_n|\\theta)\\] If we further assume that each realization of \\(X\\) is independent of the others, we get: \\[L(\\theta)=f(x_1,x_2,\\dots,x_n|\\theta)=f(x_1|\\theta)\\times f(x_2|\\theta) \\times \\dots \\times f(x_n|\\theta)\\] A mathematical simplification is to work with natural logs of the likelihood function, which assuming independently distributed random sample, gives us: \\[ lnL(\\theta)=ln(f(x_1|\\theta)\\times f(x_2|\\theta) \\times \\dots \\times f(x_n|\\theta))=\\sum_{i=1}^{N}ln(f(x_i|\\theta))\\] Definition 6.4 The maximum likelihood estimator, denoted by \\(\\hat{\\theta}_{MLE}\\), maximizes the log likelihood function: \\[ \\hat{\\theta}_{MLE} \\equiv arg \\max_{\\theta} lnL(\\theta) \\] Example 6.1 Compute maximum likelihood estimator of \\(\\mu\\) of an indpendently distributed random variable that is normally distributed with a mean of \\(\\mu\\) and a variance of \\(1\\): \\[ f(y_t|\\mu)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2} (y_t-\\mu)^2}\\] Solution: The log likelihood function is given by: \\[lnL= -Tln2\\pi-\\frac{1}{2}\\sum_{t=1}^T(y_t-\\mu)^2 \\] From the first order condition, we get \\[ \\frac{\\partial LnL}{\\partial \\mu}=\\sum_{t=1}^T(y_t-\\mu)=0\\Rightarrow \\hat{\\mu}_{MLE}=\\frac{\\sum_{t=1}^T y_t}{T}\\] 6.3.2 MLE of an AR(p) model One complication we face in estimating an AR(p) model is that by definition the realizations of the variable are not independent of each other. As a result we cannot simplify the likelihood function by multiplying individual probability density functions to obtain the joint probability density function, i.e., \\[ f(y_1,y_2,\\dots,y_T|\\theta) \\neq f(y_1|\\theta)\\times f(y_2|\\theta)\\times \\dots \\times f(y_T|\\theta)\\] Furthermore, as the order of AR increases, the joint density function we need to estimate becomes even more complicated. In this class we will focus on the method that divides the joint density into the product of conditional densities and density of a set of initial values. The idea comes from the conditional probability formula for two related events \\(A\\) and \\(B\\): \\[ P(A|B) =\\frac{P(\\text{A and B})}{P(B)} \\Rightarrow P(\\text{A and B}) = P(A|B)\\times P(B) \\] In the time series context, I will explain this for a stationary AR(1) model. We know that in this model only last period observation directly affects the current period value. Hence, consider the first two observations of a stationary time series: \\(y_1\\) and \\(y_2\\). Then the joint density of these adjacent observations is given by, \\[ f(y_1,y_2;\\theta)= f(y_2|y_1; \\theta)\\times f(y_1;\\theta)\\] Similarly, for the first three observations we get: \\[ f(y_1,y_2,y_3;\\theta)= f(y_3|y_2; \\theta)\\times f(y_2|y_1; \\theta) \\times f(y_1; \\theta)\\] Hence, for \\(T\\) observations we get: \\[ f(y_1,y_2,y_3, ...,y_T; \\theta)= f(y_T|y_{T-1};\\theta)\\times f(y_{T-1}|y_{T-2}; \\theta)\\times.... \\times f(y_1; \\theta)\\] The log-likelihood function is given by: \\[ ln \\ L(\\theta) = ln \\ f(y_1;\\theta) + \\sum_{t=2}^{T} ln \\ f(y_t|y_{t-1}; \\theta) \\] We can then maximize the above likelihood function to obtain an MLE estimator for the AR(1) model. 6.3.3 Selection of optimal order of the AR model Note that apriori we do not know the order of the AR model for any given time series. We can determine the optimal lag order by using either AIC or BIC. The process is as follows: Set \\(p=p_{max}\\) where \\(p_{max}\\) is an integer. A rule of thumb is to set \\[p_{max}=integer\\left[12\\times \\left(\\frac{T}{100}\\right)^{0.25}\\right]\\] Estimate all AR models from \\(p=1\\) to \\(p=p_{max}\\). Select the final model as the one with lowest AIC or lowest BIC. 6.3.4 Forecasting using AR(p) model Having estimated our AR(p) model with the optimal lag length, we can use the conditional mean to compute the forecast and conditional variance to compute the forecast errors. Consider an AR(1) model: \\[y_t=\\phi_0+\\phi_1 y_{t-1} +\\epsilon_t\\] Then, the 1-period ahead forecast is given by: \\[f_{t,1}=E(y_{t+1}|\\Omega_t)=\\phi_0+\\phi_1 y_t\\] Similarly, the 2-period ahead forecast is given by: \\[f_{t,2}=E(y_{t+2}|\\Omega_t)=\\phi_0+\\phi_1 E(y_{t+1}|\\Omega_t) =\\phi_0+\\phi_1f_{t,1}\\] In general, we can get the following recursive forecast equation for h-period’s ahead: \\[f_{t,h}=\\phi_0+\\phi_1 f_{t,h-1}\\] Correspondingly, the 1-period ahead forecast error is given by: \\[e_{t,1}=y_{t+1}- f_{t,1}=\\epsilon_{t+1}\\] The 2-period ahead forecast error is given by: \\[e_{t,2}=y_{t+2}-f_{t,2}=\\epsilon_{t+2}+\\phi_1 \\epsilon_{t+1} \\] Hence, the h-period ahead forecast is given by: \\[e_{t,h}=\\epsilon_{t+h} + \\phi_1 \\epsilon_{t+h-1}\\] Theorem 6.1 The h-period ahead forecast converges to the unconditional mean of \\(y_t\\), i.e., \\[\\lim_{h\\to\\infty} f_{t,h}=\\mu_y=\\frac{\\phi_0}{1-\\phi_1}\\] Theorem 6.2 The variance of the h-period ahead forecast error converges to the unconditional variance of \\(y_t\\), i.e., \\[\\lim_{h\\to\\infty} Var(e_{t,h})=\\sigma^2_y=\\frac{\\sigma^2_\\epsilon}{1-\\phi_1^2}\\] 6.4 Moving Average (MA) Model Another commonly used method for capturing the cyclical component of the time series is the moving average (MA) model where the current value of a time series linearly depends on current and past shocks. Formally, a stationary time series \\(\\{y_t\\}\\) can be modeled as an MA(q) process: \\[\\begin{equation} y_t = \\theta_0 + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ...... + \\theta_q \\epsilon_{t-q} \\end{equation}\\] Using lag operator, we can write this in more compact form as: \\[y_t = \\theta_0 +\\Theta(L) \\epsilon_t\\] where \\(\\Theta(L)=1+\\theta_1 L+ \\theta_2 L^2+...+\\theta_q L^q\\) is lag polynomial of order \\(q\\). Note that because each one of the current and past shocks are white noise processes, an MA(q) model is always stationary. 6.4.1 Invertibility of an MA process Consider the following MA(1) process with\\(\\theta_0=0\\) for simplicity: \\[y_t=\\epsilon_t +\\theta_1 \\epsilon_{t-1}\\] Using the lag operator we can rewrite this equation as follows: \\[y_t= (1+\\theta_1L)\\epsilon_t \\Rightarrow y_t(1+\\theta_1 L) ^{-1}=\\epsilon_t\\] Note that if \\(|\\theta_1|&lt;1\\), then we can use the Taylor series expansion centered at 0 and get: \\[(1+\\theta_1 L)^{-1}=1-\\theta_1 L+(\\theta_1L)^2-(\\theta_1L)^3+ (\\theta_1L)^4-...... \\] Hence, an MA(1) can be rewritten as follows: \\[y_t (1-\\theta_1 L+(\\theta_1L)^2-(\\theta_1L)^3+ (\\theta_1L)^4-......)=\\epsilon_t\\] \\[\\Rightarrow y_t -\\theta_1 y_{t-1} +\\theta_1^2y_{t-2}-\\theta_1^3 y_{t-3}....=\\epsilon_t\\] Rearranging terms, we get the \\(AR(\\infty)\\) representation for an invertible MA(1) model: \\[y_t=-\\sum_{i=1}^{\\infty}(-\\theta_1)^i \\ y_{t-i}+\\epsilon_t\\] Definition 6.5 An MA process is invertible if it can be represented as a stationary \\(AR(\\infty)\\). 6.4.2 Properties of an invetible MA(1) An invertible MA(1) model is given by: \\[ y_t = \\theta_0 + \\epsilon_t + \\theta_1 \\epsilon_{t-1} \\quad ; \\ \\epsilon_t\\sim WN(0, \\sigma_\\epsilon^2) \\ and \\ |\\theta_1|&lt;1\\] Constant unconditional mean of \\(y_t\\): \\[E(y_t)=\\mu_y =\\theta_0 \\] Constant unconditional variance of \\(y_t\\): \\[Var(y_t)=\\sigma^2_y=\\sigma^2_\\epsilon(1+\\theta_1^2)\\]) ACF function: \\[\\begin{equation*} ACF(s) = \\begin{cases} \\frac{\\theta_1}{1+\\theta_1^2} &amp; \\text{if s=1}\\\\ 0 &amp; \\text{if s&gt;1} \\end{cases} \\end{equation*}\\] PACF function: using the invertibility it is evident that PACF of an MA(1) decays with \\(s\\). 6.4.3 Forecast based on MA(q) Like before, the h-period ahead forecast is the conditional expected value of the time series. Consider an MA(1) model: \\[y_t=\\theta_0 +\\epsilon_t + \\theta_1 \\epsilon_{t-1}\\] Then, the 1-period ahead forecast is given by: The h-period ahead forecast for \\(h&gt;1\\) is given by: \\[f_{t,h}=E(y_{t+h}|\\Omega_t)=\\theta_0\\] In general, for an MA(q) model, the forecast for \\(h&gt;q\\) is the long run mean \\(\\theta_0\\). This is why we say that an MA(q) process has a memory of q periods. 6.5 ARMA(p, q) An ARMA model simply combines both AR and MA components to model the dynamics of a time series. Formula, \\[\\begin{equation} y_t = \\phi_0 +\\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ...... + \\phi_p y_{t-p}+\\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ...... + \\theta_q \\epsilon_{t-q} \\end{equation}\\] Note that: Estimation is done by maximum likelihood method. Optimal order for AR and MA components is selected using AIC and/or BIC. The forecast of \\(y_t\\) from an ARMA(p,q) model will be dominated by the AR component for \\(h&gt;q\\). To see this consider the following ARMA(1,1) model: \\[y_t = \\phi_0 +\\phi_1 y_{t-1}+ \\epsilon_t + \\theta_1 \\epsilon_{t-1}\\] Then, the 1-period ahead forecast is: \\[f_{t,1} = E(y_{t+1}|\\Omega_t) = \\phi_0 + \\phi_1 y_t + \\theta_1 \\epsilon_{t-1}\\] Here both MA and AR component affect the forecast. But now consider the 2-period ahead forecast: \\[f_{t,2} = E(y_{t+2}|\\Omega_t) = \\phi_0 + \\phi_1 f_{t,1}\\] Here there is no role played by the MA component. For any \\(h&gt;1\\) only the AR component affects the forecast from this model. 6.6 Testing for Unit root "]
]
