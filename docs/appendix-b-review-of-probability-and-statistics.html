<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>B Appendix B: Review of Probability and Statistics | Applied Time Series Analysis</title>
  <meta name="description" content="Lecture notes for Applied Time Series Analysis">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="B Appendix B: Review of Probability and Statistics | Applied Time Series Analysis />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes for Applied Time Series Analysis" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="B Appendix B: Review of Probability and Statistics | Applied Time Series Analysis />
  
  <meta name="twitter:description" content="Lecture notes for Applied Time Series Analysis" />
  

<meta name="author" content="Vipul Bhatt">


<meta name="date" content="2019-08-20">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="appendix-a-review-of-differential-calculus-and-optimization.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; min-height: 1.25em; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; }
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
@media screen {
a.sourceLine::before { text-decoration: underline; color: initial; }
}
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.bn { color: #40a070; } /* BaseN */
code span.fl { color: #40a070; } /* Float */
code span.ch { color: #4070a0; } /* Char */
code span.st { color: #4070a0; } /* String */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.ot { color: #007020; } /* Other */
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.fu { color: #06287e; } /* Function */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code span.cn { color: #880000; } /* Constant */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.ss { color: #bb6688; } /* SpecialString */
code span.im { } /* Import */
code span.va { color: #19177c; } /* Variable */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.op { color: #666666; } /* Operator */
code span.bu { } /* BuiltIn */
code span.ex { } /* Extension */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.at { color: #7d9029; } /* Attribute */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Time Series Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Forecasting</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#time-series"><i class="fa fa-check"></i><b>1.1</b> Time Series</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#serial-correlation"><i class="fa fa-check"></i><b>1.2</b> Serial Correlation</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#testing-for-serial-correlion"><i class="fa fa-check"></i><b>1.3</b> Testing for Serial Correlion</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#white-noise-process"><i class="fa fa-check"></i><b>1.4</b> White Noise Process</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#important-elements-of-forecasting"><i class="fa fa-check"></i><b>1.5</b> Important Elements of Forecasting</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#loss-function-and-optimal-forecast"><i class="fa fa-check"></i><b>1.6</b> Loss Function and Optimal Forecast</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html"><i class="fa fa-check"></i><b>2</b> Regression-based Forecasting</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#scenario-analysis-and-conditional-forecasts"><i class="fa fa-check"></i><b>2.1</b> Scenario Analysis and Conditional Forecasts</a></li>
<li class="chapter" data-level="2.2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#unconditional-forecasts"><i class="fa fa-check"></i><b>2.2</b> Unconditional Forecasts</a></li>
<li class="chapter" data-level="2.3" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#some-practical-issues"><i class="fa fa-check"></i><b>2.3</b> Some practical issues</a></li>
<li class="chapter" data-level="2.4" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#distributed-lag-regression-models"><i class="fa fa-check"></i><b>2.4</b> Distributed Lag Regression Models</a><ul>
<li class="chapter" data-level="2.4.1" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#dynamic-effect-of-x-on-y"><i class="fa fa-check"></i><b>2.4.1</b> Dynamic Effect of X on Y</a></li>
<li class="chapter" data-level="2.4.2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#model-selection-criterion"><i class="fa fa-check"></i><b>2.4.2</b> Model Selection Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#application-a-model-of-investment-expenditure"><i class="fa fa-check"></i><b>2.5</b> Application: A Model of Investment Expenditure</a><ul>
<li class="chapter" data-level="2.5.1" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#a-multiple-regression-model-of-invesment-expenditure"><i class="fa fa-check"></i><b>2.5.1</b> A Multiple Regression Model of Invesment Expenditure</a></li>
<li class="chapter" data-level="2.5.2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#a-distributed-lag-model-of-investment-expenditure"><i class="fa fa-check"></i><b>2.5.2</b> A Distributed Lag Model of Investment Expenditure</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="components-of-a-time-series.html"><a href="components-of-a-time-series.html"><i class="fa fa-check"></i><b>3</b> Components of a Time Series</a><ul>
<li class="chapter" data-level="3.1" data-path="components-of-a-time-series.html"><a href="components-of-a-time-series.html#decomposing-a-time-series"><i class="fa fa-check"></i><b>3.1</b> Decomposing a time series</a></li>
<li class="chapter" data-level="3.2" data-path="components-of-a-time-series.html"><a href="components-of-a-time-series.html#uses-of-decomposition-of-a-time-series"><i class="fa fa-check"></i><b>3.2</b> Uses of Decomposition of a time series</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="smoothing-methods.html"><a href="smoothing-methods.html"><i class="fa fa-check"></i><b>4</b> Smoothing Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="smoothing-methods.html"><a href="smoothing-methods.html#moving-average-method"><i class="fa fa-check"></i><b>4.1</b> Moving Average Method</a></li>
<li class="chapter" data-level="4.2" data-path="smoothing-methods.html"><a href="smoothing-methods.html#simple-exponential-smoothing"><i class="fa fa-check"></i><b>4.2</b> Simple Exponential Smoothing</a></li>
<li class="chapter" data-level="4.3" data-path="smoothing-methods.html"><a href="smoothing-methods.html#holt-winters-smoothing"><i class="fa fa-check"></i><b>4.3</b> Holt-Winters Smoothing</a></li>
<li class="chapter" data-level="4.4" data-path="smoothing-methods.html"><a href="smoothing-methods.html#holt-winters-smoothing-with-seasonality"><i class="fa fa-check"></i><b>4.4</b> Holt-Winters Smoothing with Seasonality</a></li>
<li class="chapter" data-level="4.5" data-path="smoothing-methods.html"><a href="smoothing-methods.html#application"><i class="fa fa-check"></i><b>4.5</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html"><i class="fa fa-check"></i><b>5</b> Modeling Trend and Seasonal Components</a><ul>
<li class="chapter" data-level="5.1" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#trend-estimation"><i class="fa fa-check"></i><b>5.1</b> Trend Estimation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#parametrizing-a-deterministic-trend"><i class="fa fa-check"></i><b>5.1.1</b> Parametrizing a deterministic trend</a></li>
<li class="chapter" data-level="5.1.2" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#uses-of-the-deterministic-trend-model"><i class="fa fa-check"></i><b>5.1.2</b> Uses of the Deterministic Trend Model</a></li>
<li class="chapter" data-level="5.1.3" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#application-estimating-a-polynomial-trend-for-u.s.-real-gdp"><i class="fa fa-check"></i><b>5.1.3</b> Application: Estimating a polynomial trend for U.S. Real GDP</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#seasonal-model"><i class="fa fa-check"></i><b>5.2</b> Seasonal Model</a><ul>
<li class="chapter" data-level="5.2.1" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#regression-model-with-seasonal-dummy-variables"><i class="fa fa-check"></i><b>5.2.1</b> Regression Model with Seasonal Dummy Variables</a></li>
<li class="chapter" data-level="5.2.2" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#application-seasonal-model-of-housing-starts"><i class="fa fa-check"></i><b>5.2.2</b> Application: Seasonal Model of Housing Starts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modeling-cycle.html"><a href="modeling-cycle.html"><i class="fa fa-check"></i><b>6</b> Modeling Cycle</a><ul>
<li class="chapter" data-level="6.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#stationarity-and-autocorrelation"><i class="fa fa-check"></i><b>6.1</b> Stationarity and Autocorrelation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#covariance-stationary-time-series"><i class="fa fa-check"></i><b>6.1.1</b> Covariance Stationary Time Series</a></li>
<li class="chapter" data-level="6.1.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#correlation-vs-autocorrelation"><i class="fa fa-check"></i><b>6.1.2</b> Correlation vs Autocorrelation</a></li>
<li class="chapter" data-level="6.1.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#partial-autocorrelation"><i class="fa fa-check"></i><b>6.1.3</b> Partial Autocorrelation</a></li>
<li class="chapter" data-level="6.1.4" data-path="modeling-cycle.html"><a href="modeling-cycle.html#lag-operator"><i class="fa fa-check"></i><b>6.1.4</b> Lag operator</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#autoregressive-ar-model"><i class="fa fa-check"></i><b>6.2</b> Autoregressive (AR) Model</a><ul>
<li class="chapter" data-level="6.2.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#unit-root-and-stationarity"><i class="fa fa-check"></i><b>6.2.1</b> Unit root and Stationarity</a></li>
<li class="chapter" data-level="6.2.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#properties-of-an-ar1-model"><i class="fa fa-check"></i><b>6.2.2</b> Properties of an AR(1) model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#estimating-an-ar-model"><i class="fa fa-check"></i><b>6.3</b> Estimating an AR model</a><ul>
<li class="chapter" data-level="6.3.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>6.3.1</b> Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="6.3.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#mle-of-an-arp-model"><i class="fa fa-check"></i><b>6.3.2</b> MLE of an AR(p) model</a></li>
<li class="chapter" data-level="6.3.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#selection-of-optimal-order-of-the-ar-model"><i class="fa fa-check"></i><b>6.3.3</b> Selection of optimal order of the AR model</a></li>
<li class="chapter" data-level="6.3.4" data-path="modeling-cycle.html"><a href="modeling-cycle.html#forecasting-using-arp-model"><i class="fa fa-check"></i><b>6.3.4</b> Forecasting using AR(p) model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="modeling-cycle.html"><a href="modeling-cycle.html#moving-average-ma-model"><i class="fa fa-check"></i><b>6.4</b> Moving Average (MA) Model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#invertibility-of-an-ma-process"><i class="fa fa-check"></i><b>6.4.1</b> Invertibility of an MA process</a></li>
<li class="chapter" data-level="6.4.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#properties-of-an-invertible-ma1"><i class="fa fa-check"></i><b>6.4.2</b> Properties of an invertible MA(1)</a></li>
<li class="chapter" data-level="6.4.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#forecast-based-on-maq"><i class="fa fa-check"></i><b>6.4.3</b> Forecast based on MA(q)</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="modeling-cycle.html"><a href="modeling-cycle.html#armap-q"><i class="fa fa-check"></i><b>6.5</b> ARMA(p, q)</a></li>
<li class="chapter" data-level="6.6" data-path="modeling-cycle.html"><a href="modeling-cycle.html#integrated-arma-or-arimapdq"><i class="fa fa-check"></i><b>6.6</b> Integrated ARMA or ARIMA(p,d,q)</a></li>
<li class="chapter" data-level="6.7" data-path="modeling-cycle.html"><a href="modeling-cycle.html#trend-stationary-vs-difference-stationary-time-series"><i class="fa fa-check"></i><b>6.7</b> Trend Stationary vs Difference Stationary Time Series</a></li>
<li class="chapter" data-level="6.8" data-path="modeling-cycle.html"><a href="modeling-cycle.html#testing-for-a-unit-root"><i class="fa fa-check"></i><b>6.8</b> Testing for a unit root</a><ul>
<li class="chapter" data-level="6.8.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#testing-for-unit-root-in-usdcad-exchange-rate"><i class="fa fa-check"></i><b>6.8.1</b> Testing for unit root in USD/CAD exchange rate</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="modeling-cycle.html"><a href="modeling-cycle.html#box-jenkins-method-for-estimating-arimapdq"><i class="fa fa-check"></i><b>6.9</b> Box-Jenkins Method for estimating ARIMA(p,d,q)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modeling-volatility.html"><a href="modeling-volatility.html"><i class="fa fa-check"></i><b>7</b> Modeling Volatility</a><ul>
<li class="chapter" data-level="7.1" data-path="modeling-volatility.html"><a href="modeling-volatility.html#some-stylized-facts-about-stock-market-volatility"><i class="fa fa-check"></i><b>7.1</b> Some stylized facts about stock market volatility</a></li>
<li class="chapter" data-level="7.2" data-path="modeling-volatility.html"><a href="modeling-volatility.html#archq-autoregressive-conditional-heteroscedasticiy-of-order-q"><i class="fa fa-check"></i><b>7.2</b> ARCH(q): Autoregressive Conditional Heteroscedasticiy of order <span class="math inline">\(q\)</span></a></li>
<li class="chapter" data-level="7.3" data-path="modeling-volatility.html"><a href="modeling-volatility.html#garchpq-generalized-autoregressive-conditional-heteroscedasicity-of-order-p-and-q"><i class="fa fa-check"></i><b>7.3</b> GARCH(p,q): Generalized Autoregressive Conditional Heteroscedasicity of order <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span></a></li>
<li class="chapter" data-level="7.4" data-path="modeling-volatility.html"><a href="modeling-volatility.html#extensions-of-standard-garch-model"><i class="fa fa-check"></i><b>7.4</b> Extensions of standard GARCH model</a><ul>
<li class="chapter" data-level="7.4.1" data-path="modeling-volatility.html"><a href="modeling-volatility.html#gjr-garch11"><i class="fa fa-check"></i><b>7.4.1</b> GJR-GARCH(1,1)</a></li>
<li class="chapter" data-level="7.4.2" data-path="modeling-volatility.html"><a href="modeling-volatility.html#exponential-garch-or-egarch11"><i class="fa fa-check"></i><b>7.4.2</b> Exponential GARCH or EGARCH(1,1)</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="modeling-volatility.html"><a href="modeling-volatility.html#application-of-garch-model-estimating-volatility-of-sp500-return"><i class="fa fa-check"></i><b>7.5</b> Application of GARCH model: Estimating volatility of SP500 return</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-a-review-of-differential-calculus-and-optimization.html"><a href="appendix-a-review-of-differential-calculus-and-optimization.html"><i class="fa fa-check"></i><b>A</b> Appendix A: Review of Differential Calculus and Optimization</a><ul>
<li class="chapter" data-level="A.1" data-path="appendix-a-review-of-differential-calculus-and-optimization.html"><a href="appendix-a-review-of-differential-calculus-and-optimization.html#derivative-of-a-single-variable-function"><i class="fa fa-check"></i><b>A.1</b> Derivative of a single variable function</a><ul>
<li class="chapter" data-level="A.1.1" data-path="appendix-a-review-of-differential-calculus-and-optimization.html"><a href="appendix-a-review-of-differential-calculus-and-optimization.html#rules-of-differentiation"><i class="fa fa-check"></i><b>A.1.1</b> Rules of Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appendix-a-review-of-differential-calculus-and-optimization.html"><a href="appendix-a-review-of-differential-calculus-and-optimization.html#second-derivative-and-non-linearity"><i class="fa fa-check"></i><b>A.2</b> Second derivative and non-linearity</a></li>
<li class="chapter" data-level="A.3" data-path="appendix-a-review-of-differential-calculus-and-optimization.html"><a href="appendix-a-review-of-differential-calculus-and-optimization.html#partial-derivatives-multi-variable-functions"><i class="fa fa-check"></i><b>A.3</b> Partial derivatives: Multi-variable functions</a></li>
<li class="chapter" data-level="A.4" data-path="appendix-a-review-of-differential-calculus-and-optimization.html"><a href="appendix-a-review-of-differential-calculus-and-optimization.html#optimization"><i class="fa fa-check"></i><b>A.4</b> Optimization</a></li>
<li class="chapter" data-level="" data-path="appendix-a-review-of-differential-calculus-and-optimization.html"><a href="appendix-a-review-of-differential-calculus-and-optimization.html#problems"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html"><i class="fa fa-check"></i><b>B</b> Appendix B: Review of Probability and Statistics</a><ul>
<li class="chapter" data-level="B.1" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#probability"><i class="fa fa-check"></i><b>B.1</b> Probability</a></li>
<li class="chapter" data-level="B.2" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#random-variable"><i class="fa fa-check"></i><b>B.2</b> Random Variable</a></li>
<li class="chapter" data-level="B.3" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#probability-distribution"><i class="fa fa-check"></i><b>B.3</b> Probability distribution</a><ul>
<li class="chapter" data-level="B.3.1" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#probability-distribution-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>B.3.1</b> Probability distribution of a discrete random variable</a></li>
<li class="chapter" data-level="B.3.2" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#probability-distribution-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>B.3.2</b> Probability distribution of a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#moments-of-a-probability-distribution-function"><i class="fa fa-check"></i><b>B.4</b> Moments of a probability distribution function</a><ul>
<li class="chapter" data-level="B.4.1" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#first-moment-of-a-probability-distribution-expected-value"><i class="fa fa-check"></i><b>B.4.1</b> First moment of a probability distribution: Expected value</a></li>
<li class="chapter" data-level="B.4.2" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#second-moment-of-the-distribution."><i class="fa fa-check"></i><b>B.4.2</b> Second moment of the distribution.</a></li>
<li class="chapter" data-level="B.4.3" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#third-and-fourth-moments-skewness-and-kurtosis"><i class="fa fa-check"></i><b>B.4.3</b> Third and Fourth Moments: Skewness and Kurtosis</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#useful-probability-distributions"><i class="fa fa-check"></i><b>B.5</b> Useful probability distributions</a></li>
<li class="chapter" data-level="B.6" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#joint-probability-distribution"><i class="fa fa-check"></i><b>B.6</b> Joint Probability Distribution</a></li>
<li class="chapter" data-level="B.7" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#measures-of-statistical-association"><i class="fa fa-check"></i><b>B.7</b> Measures of statistical association</a><ul>
<li class="chapter" data-level="B.7.1" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#rules-of-expectation-and-variances"><i class="fa fa-check"></i><b>B.7.1</b> Rules of expectation and variances</a></li>
</ul></li>
<li class="chapter" data-level="B.8" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#sampling-and-estimation"><i class="fa fa-check"></i><b>B.8</b> Sampling and Estimation</a></li>
<li class="chapter" data-level="B.9" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#hypothesis-testing"><i class="fa fa-check"></i><b>B.9</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="B.9.1" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#testing-a-restriction-on-a-single-population-parameter"><i class="fa fa-check"></i><b>B.9.1</b> Testing a restriction on a single population parameter</a></li>
<li class="chapter" data-level="B.9.2" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#testing-a-restriction-on-multiple-population-parameter"><i class="fa fa-check"></i><b>B.9.2</b> Testing a restriction on multiple population parameter</a></li>
<li class="chapter" data-level="B.9.3" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#confidence-interval-and-hypothesis-testing"><i class="fa fa-check"></i><b>B.9.3</b> Confidence interval and Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b-review-of-probability-and-statistics.html"><a href="appendix-b-review-of-probability-and-statistics.html#problems-1"><i class="fa fa-check"></i>Problems</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://vipul-bhatt.github.io/Econ-483-Notes/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Time Series Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="appendix-b-review-of-probability-and-statistics" class="section level1">
<h1><span class="header-section-number">B</span> Appendix B: Review of Probability and Statistics</h1>
<p>Given that all students must have taken a course in statistics before enrolling for this class, it is assumed that everyone in the class is comfortable with concepts such probability, expected value, measures of central tendency, hypothesis testing etc. In this chapter, I will provide a brief review of some concepts that are most pertinent for Econometrics. I strongly encourage that you read your lecture notes for Statistics if you find it difficult to follow the material presented in this chapter.</p>
<div id="probability" class="section level2">
<h2><span class="header-section-number">B.1</span> Probability</h2>
<p>We begin with a brief review of probability thoery. To define probability we first need to develop an understanding of what we mean by <em>experiment</em>, <em>sample space</em>, and <em>event</em> in statistics.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-32" class="definition"><strong>Definition B.1  (Experiment)  </strong></span>An experiment is a process with an uncertain observable outcome. e.g. Toss of a coin can have two possible outcomes, heads or tails.</p>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-33" class="definition"><strong>Definition B.2  (Sample Space)  </strong></span>The sample space is the set of all possible outcomes of an experiment. I will denote it by <span class="math inline">\(S\)</span>. If we toss a coin then <span class="math inline">\(S=\{Heads,Tails\}\)</span>.</p>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-34" class="definition"><strong>Definition B.3  (Event)  </strong></span>An event is a subset of the sample space. I will denote it by <span class="math inline">\(E\)</span>. If we toss a coin and Heads shows up then <span class="math inline">\(E={Heads}\)</span>.</p>
</div>

<p>Now, we can define probability, which is a function that assigns a numerical value to the chance of an event occuring among all possible events in the sample space.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-35" class="definition"><strong>Definition B.4  (Probability)  </strong></span>
A function <span class="math inline">\(P\)</span> is called a probability function if:</p>
<ol style="list-style-type: decimal">
<li>For any given event, <span class="math inline">\(E\)</span>, <span class="math inline">\(0 \leq P(E)&lt; \leq 1\)</span>.</li>
<li>Suppose there are N possible events in S, i.e., <span class="math inline">\(S=\{E_1, E_2, E_3,..,E_N\}\)</span>. Then,
<span class="math display">\[P(E_1)+P(E_2)+P(E_3)+...+P(E_N)=1\]</span></li>
<li>Consider an event E. Then,</li>
</ol>
<p><span class="math display">\[P(\lnot E) = 1 -P(E) \]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p>If we have two disjoint events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, then:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(P(A \cup B)= P(A) + P(B)\)</span></li>
<li><span class="math inline">\(P(A \cap B)=0\)</span></li>
</ol></li>
<li><p>If we have two non-disjoint events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, then:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(P(A \cup B)= P(A) + P(B)-P(A \cap B)\)</span></li>
<li><span class="math inline">\(P(A \cap B)= P(A) \times P(B|A)\)</span></li>
</ol></li>
<li><p>If we have two independent events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, then:
<span class="math display">\[P(A \cap B) = P(A) \times P(B)\]</span></p></li>
<li><p>Bayes rule:</p></li>
</ol>
<p><span class="math display">\[P(A|B)=\frac{P(A) \times P(B|A)}{P(B)}\]</span></p>
<p>where <span class="math inline">\(\displaystyle{P(B)= P(B|A)\times P(A) + P(B|\lnot A) \times P(\lnot A)}\)</span></p>
</div>

<p>One such probability function is:
<span class="math display">\[\begin{align}
P(E) = \frac{\text{Number of  outcomes  in  E}}{\text{Number  of  outcomes  in  S}}
\end{align}\]</span></p>

<div class="example">
<p><span id="exm:unnamed-chunk-36" class="example"><strong>Example B.1  </strong></span>Consider a fair six-sided dice. The probability of obtaining an odd number if this dice is rolled once is given by 0.5. To see this, note that the event here is obtaining an odd number when a dice is rolled. Hence, <span class="math inline">\(E=\{1,3,5\}\)</span>. Also, <span class="math inline">\(S=\{1,2,3,4,5,6\}\)</span>. Using this, we get:
<span class="math display">\[\begin{equation}
P(E)=\frac{3}{6}=0.5
\end{equation}\]</span></p>
</div>

</div>
<div id="random-variable" class="section level2">
<h2><span class="header-section-number">B.2</span> Random Variable</h2>
<p>One of the most important applications of statistics is to resolve the randomness that is inherent in most economic choices. For example, the outcome of your college major is a random variable with many possible values. Most economic variables can be thought of as <strong>random variables</strong> that have many possible values which are unknown until they are realized. We will begin by formally defining a random variable.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-37" class="definition"><strong>Definition B.5  (Random Variable)  </strong></span>A random variable is a numerical representation of outcomes of an experiment. For example, in the example of a toss of a coin, suppose you win $10 if heads shows and you lose $5 if tails shows. In this case, tossing the coin was the experiment, and winnings from this game is the random variable with two possible values: $10 and -$5.</p>
</div>

<p>There are two types of random variables.</p>
<ol style="list-style-type: decimal">
<li><p>Discrete random variable: takes finite number of values. e.g. GPA points earned in Econ 385.</p></li>
<li><p>Continuous random variable: can take any value on the number line. e.g. GDP in the last quarter of 2019.</p></li>
</ol>
</div>
<div id="probability-distribution" class="section level2">
<h2><span class="header-section-number">B.3</span> Probability distribution</h2>
<p>By definition a random variable can take many <em>possible values</em>. In statistics a function that provides the probabilties of different realizations of a random varuable is called its <strong>probability distribution</strong>.</p>
<div id="probability-distribution-of-a-discrete-random-variable" class="section level3">
<h3><span class="header-section-number">B.3.1</span> Probability distribution of a discrete random variable</h3>
<p>For a discrete random variable the probability distribution is simply the list of all possible values this variable can and their corresponding probabilities. Let <span class="math inline">\(X\)</span> be a discrete random variable with <span class="math inline">\(n\)</span> possible values give by <span class="math inline">\(\{x_1,x_2,x_3,..,x_n\}\)</span>. Let <span class="math inline">\(p_i\)</span> denotes that probability that <span class="math inline">\(X=x_i\)</span>. Then, the probability distribution function of this random variable is given by:</p>
<table>
<thead>
<tr class="header">
<th align="center">X</th>
<th align="center">p(X)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_1\)</span></td>
<td align="center"><span class="math inline">\(p_1\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_2\)</span></td>
<td align="center"><span class="math inline">\(p_2\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_3\)</span></td>
<td align="center"><span class="math inline">\(p_3\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_n\)</span></td>
<td align="center"><span class="math inline">\(p_n\)</span></td>
</tr>
</tbody>
</table>

<div class="example">
<p><span id="exm:unnamed-chunk-38" class="example"><strong>Example B.2  (Grade Distribution)  </strong></span>A typical grade distribution is an example of a discrete random variable. Consider the following grade distribution:</p>
<table>
<thead>
<tr class="header">
<th align="center">GPA</th>
<th align="center">Percent of Students</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(10\%\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(20\%\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(2\)</span></td>
<td align="center"><span class="math inline">\(40\%\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(3\)</span></td>
<td align="center"><span class="math inline">\(20\%\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(4\)</span></td>
<td align="center"><span class="math inline">\(10\%\)</span></td>
</tr>
</tbody>
</table>
</div>

<p>Note that every GPA point corresponds to a letter grade. From the perspective of the student, <span class="math inline">\(X\)</span> is the random variable that is his letter grade, and the above distribution gives the probability of obtaining a particular letter grade. We can plot this simple probability distribution as follows:</p>
<div class="figure"><span id="fig:unnamed-chunk-39"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-39-1.png" alt="Probability Distribution of Letter Grades" width="672" />
<p class="caption">
Figure B.1: Probability Distribution of Letter Grades
</p>
</div>
<p>We can use the probability distribution of a discrete random variable in two different ways.</p>
<ol style="list-style-type: decimal">
<li>We can compute the probability of the random variable taking an exact value. This is known as the <strong>probability mass function (p.m.f)</strong> and is denoted by <span class="math inline">\(f(x)\)</span>:</li>
</ol>
<p><span class="math display">\[f(x)=P(X=x)\]</span></p>
<p>For example, the probability of obtaining a letter grade of C or <span class="math inline">\(P(X=2)\)</span> is 0.4 or 40%.</p>
<ol start="2" style="list-style-type: decimal">
<li>We can also infer the probability that a discrete random variable will be less than or equal to a certain value by cumulatively adding the probabilities. Formally, we can compute the <strong>cumulative probability distribution (c.d.f)</strong> which is denoted by <span class="math inline">\(F(x)\)</span>:</li>
</ol>
<p><span class="math display">\[F(x)=P(X\leq X)\]</span></p>
<p>Going back to our grade distribution example, we can add the column of cumulative probabilities to obtain the <span class="math inline">\(c.d.f\)</span>:</p>
<table>
<thead>
<tr class="header">
<th align="center">Grade</th>
<th align="center">Percent of Students</th>
<th align="center"><span class="math inline">\(F(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(10\%\)</span></td>
<td align="center"><span class="math inline">\(10\%\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(20\%\)</span></td>
<td align="center"><span class="math inline">\(30\%\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(2\)</span></td>
<td align="center"><span class="math inline">\(40\%\)</span></td>
<td align="center"><span class="math inline">\(70\%\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(3\)</span></td>
<td align="center"><span class="math inline">\(20\%\)</span></td>
<td align="center"><span class="math inline">\(90\%\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(4\)</span></td>
<td align="center"><span class="math inline">\(10\%\)</span></td>
<td align="center"><span class="math inline">\(100\%\)</span></td>
</tr>
</tbody>
</table>
<p>So for example, we can infer that the probability of obtaining the letter grade of C or lower i.e, <span class="math inline">\(P(X\leq 2)\)</span> is 0.7 or 70% which is obtained by adding the probabilities of obtaining letter grades of C, D, and F, respectively.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-40" class="example"><strong>Example B.3  (Bernoulli Random Variable)  </strong></span>When a random variable is binary then we call it a <strong>Bernoulli</strong> random variable and its probability distrubition is called <strong>Bernoulli</strong> distribution. Consider a random variable that can only take two values, say, <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. It is common to think of these two values as coding a set criterion with <span class="math inline">\(1\)</span> typically assigned if the criterion is met and <span class="math inline">\(0\)</span> is assigned for failing to meet the criterion. For example, <span class="math inline">\(X\)</span> could be whether you will get a job right after graduation. If you do then <span class="math inline">\(X=1\)</span> and if you do not then <span class="math inline">\(X=0\)</span>. Let <span class="math inline">\(p\)</span> denotes the probability that you will get a job. Then, the <span class="math inline">\(p.m.f.\)</span> of the Bernoulli distribution is given by:</p>
<p><span class="math display">\[f(x)=\begin{cases}
    p &amp; if \ X=1\\
    1-p &amp; if \ X=0
     \end{cases}\]</span></p>
<p>The <span class="math inline">\(c.d.f\)</span> of the Bernoulli distribution is given by:</p>
<p><span class="math display">\[F(x)=\begin{cases}
    0 &amp; if \quad X &lt; 0\\
    1-p &amp; if \quad 0\leq X&lt;1\\
    p &amp; if \quad  X\geq 1
     \end{cases}\]</span></p>
</div>

</div>
<div id="probability-distribution-of-a-continuous-random-variable" class="section level3">
<h3><span class="header-section-number">B.3.2</span> Probability distribution of a continuous random variable</h3>
<p>In economics a large majority of variables of interest in theory are continous random variables. For example, the change in the price of Apple stock between two time periods is the return on Apple stock. If you are a trader in the NYSE then the stock return on Apple is a continuous random variable that can take any value on an interval. In such a case we cannot obtain the probability of the random variable taking an exact value. But we can only compute the probability that this random variable will fall in a given interval. So at best we can determine the probability that GDP growth for the US next quarter will be between say 1/% and 2%. This probability is obtained by computing the area under the <strong>probability density function (p.d.f)</strong>. Let <span class="math inline">\(X\)</span> denote a continuous random variable and <span class="math inline">\(f(x)\)</span> denotes the p.d.f. Then,</p>
<ol style="list-style-type: decimal">
<li>The probability that <span class="math inline">\(X\)</span> takes value over the interval <span class="math inline">\(\{a,b\}\)</span> is given by:</li>
</ol>
<p><span class="math display">\[P(a\leq X \leq b)=\int_a^bf(x) \ dx\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The c.d.f (the probability that <span class="math inline">\(X\leq x\)</span>) is given by:</li>
</ol>
<p><span class="math display">\[F(x)=P(X\leq x)=\int_{-\infty}^xf(x) \ dx\]</span></p>
<p>Below I plot the empirical c.d.f for Apple’s stock return. Let <span class="math inline">\(X\)</span> denotes this stock return. From Fig 3.2 we can infer that <span class="math inline">\(P(X\leq 0)=0.47\)</span> and <span class="math inline">\(P(X\leq 3)=0.95\)</span>.</p>
<div class="figure"><span id="fig:unnamed-chunk-41"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-41-1.png" alt="Empirical c.d.f of daily Apple Stock Return (2007-2019)" width="672" />
<p class="caption">
Figure B.2: Empirical c.d.f of daily Apple Stock Return (2007-2019)
</p>
</div>
<p>Figure 3.3 below presents the <em>p.d.f</em> of the daily stock return that corresponds to the <em>c.d.f</em> plotted in Figure 3.2. Using this we can work the probability of stock returns falling in any given interval. For instance, the probability that Apple stock return will fall between 0 and 3% is the area under the p.d.f. between these two values. Figure 3.3 higlights this area and we can see that this probability is equal to 0.47.</p>
<div class="figure"><span id="fig:unnamed-chunk-42"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-42-1.png" alt="Empirical p.d.f of daily Apple Stock Return (2007-2019)" width="672" />
<p class="caption">
Figure B.3: Empirical p.d.f of daily Apple Stock Return (2007-2019)
</p>
</div>
</div>
</div>
<div id="moments-of-a-probability-distribution-function" class="section level2">
<h2><span class="header-section-number">B.4</span> Moments of a probability distribution function</h2>
<p>The information contained in a probability distribution can be meaningfully summarized into measures that are called <strong>moments</strong> of that distribution. There are three moments we often use in economics:</p>
<ol style="list-style-type: decimal">
<li><p>Center of the distribution: this is first moment of a given probability distribution and it gives us the most likely value of the random variable. It can be measured by mean, median or mode. We will use mean as a measure of the center of the distribution.</p></li>
<li><p>Width of the distribution: this is the second moment and it measures the average distance from mean under a given probability distribution. We will use standard deviation as a measure of the width of the distribution.</p></li>
<li><p>Shape of the distribution: this feature relates to role played by <strong>tail events</strong> , i.e., events that have very low probability of happening under a given probability distribution. Two relevant measures are Skewness and Kurtosis</p></li>
</ol>
<div id="first-moment-of-a-probability-distribution-expected-value" class="section level3">
<h3><span class="header-section-number">B.4.1</span> First moment of a probability distribution: Expected value</h3>
<p>What is the most likely value of a random variable? To answer that we often compute <strong>expected value</strong> of the random variable which gives us the center (or peak) of the underlying probability distribution. We will use <strong>E</strong> to denote expected value. So <span class="math inline">\(E(X)\)</span> is the expected value of a random variable and we will use <span class="math inline">\(\mu_X\)</span> to denote the mean or average value of <span class="math inline">\(X\)</span>.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-43" class="definition"><strong>Definition B.6  (Expected Value)  </strong></span>Consider a discrete random variable <span class="math inline">\(X\)</span> that can take <span class="math inline">\(n\)</span> possible values and has the following probabilty distrubution:</p>
<table>
<thead>
<tr class="header">
<th align="center">X</th>
<th align="center">p(X)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_1\)</span></td>
<td align="center"><span class="math inline">\(p_1\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_2\)</span></td>
<td align="center"><span class="math inline">\(p_2\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_3\)</span></td>
<td align="center"><span class="math inline">\(p_3\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_n\)</span></td>
<td align="center"><span class="math inline">\(p_n\)</span></td>
</tr>
</tbody>
</table>
<p>Then, the expected value of <span class="math inline">\(X\)</span> is given by:</p>
<span class="math display">\[E(X)= x_1 p_1 + x_2 p_2+...+ x_n p_n = \sum_{i=1}^n y_ip_i\]</span>
</div>

Hence, expected value is a probability-weighted average of all possible values of a random variable.

<div class="example">
<span id="exm:unnamed-chunk-44" class="example"><strong>Example B.4  </strong></span>Suppose you toss a fair coin and receive $10 if tails shows and receive 0 if heads shows. What is the expected value of the winnings from a single toss of this coin?
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span> Let <span class="math inline">\(X\)</span> denotes winnings from this game. It can take a value of $10 with a probability of 0.5 and 0 with a probability of half. So the expected value of X is:</p>
<span class="math display">\[E(X) = x_1 p_1+x_2 p_2=10\times 0.5 + 0\times 0.5=\$5\]</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-46" class="example"><strong>Example B.5  </strong></span>Suppose you can invest $10,000 in a mutual fund after 1 year can earn a return of 10%
with a probability of 0.1 or a return of 2% with a probability of 0.5 or a loss of 5% with a probability of 0.4. What is the expected return of investing $10,000 in this mutual fund?
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span> Let <span class="math inline">\(X\)</span> denotes expected return in dollars. It can take 3 possible values: $1000 with a probability of 0.1, $200 with a probability of 0.5, and -$400 with a probability of 0.4 The expected value is given by:</p>
<span class="math display">\[E(X) = 1000\times 0.1 + 200 \times 0.5 - 400 \times 0.4=\$40 \]</span>
</div>

<p>As mentioned earlier, the first moment of the probability distribution (i.e., the expected value) gives us the most likely value of the random variable. How useful is this knowledge will depend on how far any realiaztion of the random variable can be from its expected value. The average distance from the average measures the width of the distribution. Wider the distribution, less useful is the knowledge of the expected value.</p>
</div>
<div id="second-moment-of-the-distribution." class="section level3">
<h3><span class="header-section-number">B.4.2</span> Second moment of the distribution.</h3>
<p>To determine the width or <em>dispersion</em> of a probability distribution we use <strong>variance</strong> or <strong>standard deviation</strong>. The variance is the expected value of the squared deviation of each realization of the random variable from its average. We will denote the variance by <span class="math inline">\(Var(X)\)</span> or <span class="math inline">\(\sigma^2_X\)</span>:</p>
<p><span class="math display">\[Var(x)= \sigma^2_X=E[(X-\mu_x)^2]= (x_1-\mu_X)^2 \times p_1+(x_2-\mu_X)^2 \times p_2+...+ (x_n-\mu_X)^2 \times p_n =\sum_{i=1}^n (x_i-\mu_x)^2p_i\]</span></p>
<p>The standard deviation is simply the square root of the variance and is in the same units as the random variable. This allows easy comparison of the width and the center of the distribution. We will denote standard deviation by <span class="math inline">\(\sigma_X\)</span>.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-48" class="example"><strong>Example B.6  </strong></span>Using the mutual fund example, the variance will measure <strong>riskiness</strong> of the investment. It is given by:</p>
<p><span class="math display">\[Var(X)=(1000-40)^2\times 0.1 + (200-40)^2 \times 0.5 - (400-40)^2 \times 0.4=53120\]</span></p>
<p>Because variance is in square units and hence hard to interpret, we can easily compute the standard deviation as the square root of the variance:</p>
<p><span class="math display">\[\sigma_X=\sqrt{53120}=\$230.47\]</span></p>
Hence, even though the average return from this investment is $40, you can be $230 above or below this average.
</div>

<p>How much can we say about a random variable if we only know its mean and the standard deviation? That depends on the type of distribution the random variable follows. One of the most commonly used distribution in statistic is the <strong>Normal Distribution</strong> or the <strong>Gaussian Distribution</strong>. A random variable that follows normal distribution has a bell-shaped probability distribution with a given mean and standard deviation. One of the most useful features of such a distribution is that knowledge of the first two moments alone is sufficient to characterize the entire probability distribution. Figure 3.4 below shows a normal distribution with a mean of 5 and a standard deviation of 2.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-49"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-49-1.png" alt="Normal distribution with mean=5 and s.d.=2" width="672" />
<p class="caption">
Figure B.4: Normal distribution with mean=5 and s.d.=2
</p>
</div>
<p>Key features of the normal distribution that are very useful for us:</p>
<ol style="list-style-type: lower-alpha">
<li>95% of the values fall within 1.96 times the standard deviation of the mean:</li>
</ol>
<p><span class="math display">\[P(\mu_X -1.96\sigma_X \leq X \leq \mu_X + 1.96\sigma_X)=0.95\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>Tail events (low probability events) on either side of the mean are equally unlikely.</p></li>
<li><p>Central limit theorem: The distribution of sample means calculated from repeated random sampling from a given population approaches a normal distribution as the sample size approaches <span class="math inline">\(\infty\)</span>.</p></li>
</ol>
</div>
<div id="third-and-fourth-moments-skewness-and-kurtosis" class="section level3">
<h3><span class="header-section-number">B.4.3</span> Third and Fourth Moments: Skewness and Kurtosis</h3>
<p>In many cases, the distribution of a random variable is not normal and in such cases higher moments provide useful information about the shape of such probability distribution. The shape of the probability distribution plays an important role in many economic and financial applications. There are two measures of shape that are of interest:</p>
<ol style="list-style-type: decimal">
<li><strong>Skewness</strong>: this is the third moment of the distribution and it measures how skewed a distribution is. The formula for skewness is given by:</li>
</ol>
<p><span class="math display">\[Skewness=\frac{E[(X-\mu_X)^3]}{\sigma^2_X}\]</span></p>
<p>A normal distribution has a skweness of zero. There are two possible types of skewed distributions:</p>
<ol style="list-style-type: lower-alpha">
<li><p>A positively skewed distribution will have a long right tail implying lower probability of very large values relative to the mean.</p></li>
<li><p>A neagtively skewed distribution will have a long left tail implying lower probablity of very small values relative to the mean.</p></li>
</ol>
<p>Figure 3.5 shows three probability distributions. For the left-skewed distribution, a longer left tail indicates low probability of obtaining values below the mean. Similarly, for the right-skewed distribution, a longer right tail indicates low probability of obtaining a value above the mean. For a normal distribution, the probability of obtaining a value above the mean is the same as the probability of obtaining a value below the mean.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-50"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-50-1.png" alt="Skewness of a Probablity distribution" width="384" />
<p class="caption">
Figure B.5: Skewness of a Probablity distribution
</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Kurtosis: this is the fourth moment of the distribution that captures the peakedness of the distribution (or thickness of the tail), i.e., how many observations fall on the extreme ends of a given probability distribution. As a result it tells us the role played by extreme values in driving the variance of a random variable. The formula is given by:</li>
</ol>
<p><span class="math display">\[Kurtosis=\frac{E[(X-\mu_X)^4]}{\sigma^4_X}\]</span></p>
<p>A normal distribution has a Kurtosis of 3. A value that is above or below 3 will give us excess or deficient Kurtosis. Two possiblities are:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Leptokurtic distribution: has a Kurtosis value greater than three. Such a distribution will have fat tails compared to a normal distribution indicating greater area under the tails.</p></li>
<li><p>Platykurtic distribution: has a Kurtosis value less than 3. Such a distribution will have thin tails compared to a normal distribution.</p></li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-51"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-51-1.png" alt="Kurtosis of a Probablity distribution" width="672" />
<p class="caption">
Figure B.6: Kurtosis of a Probablity distribution
</p>
</div>
<p>Fig 3.6 shows three types of distribution based on their Kurtosis. The leptokurtic distribution has a Kurtosis value of greater than 3 and is more <strong>heavy-tailed</strong> or <strong>peaked</strong> than a norma distribution.</p>
</div>
</div>
<div id="useful-probability-distributions" class="section level2">
<h2><span class="header-section-number">B.5</span> Useful probability distributions</h2>
<p>Using the normal distribution we can derive a few useful probability distributions that are utilized in hypothesis testing.</p>
<ol style="list-style-type: decimal">
<li><p>Standard Normal Distribution: A random variable that follows normal distribution with a mean of 0 and standard deviation of 1.</p></li>
<li><p>Chi-square distribution: is obtained by squaring and adding indpendent standard normal distribution. For example, is <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two standard normal random variables, then <span class="math inline">\(Z=X^2 + Y^2\)</span> follows a Chi-square distribution with two degrees of freedom.</p></li>
<li><p>F-distribution: is obtained by taking a ratio of two chi-square distribution. For example, if <span class="math inline">\(X\)</span> is Chi-square with <span class="math inline">\(v_1\)</span> degrees of freedom and <span class="math inline">\(Y\)</span> is a Chi-quare with <span class="math inline">\(v_2\)</span> degress of freedom, then <span class="math inline">\(\displaystyle{Z=\frac{X}{Y}}\)</span> follows F-distribution with <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> degrees of freddom.</p></li>
<li><p>t-distribution: Student’s t-distribution is obtained by taking a ratio of a standard normal and the square root of a Chi-square random variable. For example, if <span class="math inline">\(X\)</span> is a standard normal and <span class="math inline">\(Y\)</span> is a Chi-square with <span class="math inline">\(m\)</span> degrees of freedom, then <span class="math inline">\(Z=\displaystyle\frac{X}{\sqrt{Y/m}}\)</span> follows t-distribution with <span class="math inline">\(m\)</span> degrees of freedom. t-distribution has fatter tails when compared to normal.</p></li>
</ol>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-52-1.png" width="480" style="display: block; margin: auto;" />
<img src="bookdown-demo_files/figure-html/unnamed-chunk-53-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-54-1.png" width="480" style="display: block; margin: auto;" /><img src="bookdown-demo_files/figure-html/unnamed-chunk-54-2.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-55-1.png" width="480" style="display: block; margin: auto;" /><img src="bookdown-demo_files/figure-html/unnamed-chunk-55-2.png" width="480" style="display: block; margin: auto;" /><img src="bookdown-demo_files/figure-html/unnamed-chunk-55-3.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="joint-probability-distribution" class="section level2">
<h2><span class="header-section-number">B.6</span> Joint Probability Distribution</h2>
<p>In economics, often we are interested in the relationship between a pair of variables. For example, how does interest rate affects consumption spending? Or how does education affect wages? In order to statistically answer such questions, we need to understand the meaning of statistical relationship between two or more variables. One way to move forward is to assume that both variables jointly follow some given probability distribution which can be used to infer their relationship with one another.</p>
<p>For simplicity, I will use the discrete random variables case but the concepts covered can be easily extended for the continuous random variables case.</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> denote two random variables of interest, both from a common probablity distribution denoted by <span class="math inline">\(F(x,y)\)</span>. This function gives us the probability that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> simultaneously take on certain values:</p>
<p><span class="math display">\[F(x,y)=P(X=x, Y=y)\]</span></p>

<div class="example">
<p><span id="exm:unnamed-chunk-56" class="example"><strong>Example B.7  </strong></span>
Suppose you are an investment banker and you are considering investment into two assets: a stock listed in NYSE (<span class="math inline">\(X\)</span>) and a cotton futures (<span class="math inline">\(Y\)</span>) listed in Chicago Mercantile Exchange. Suppose <span class="math inline">\(X\)</span> can take three possible values: 2/%, 3/%, or 4/%. Similarly <span class="math inline">\(Y\)</span> can take three possible values given by 6/%,4/%, or 1/%. The value will depend on the state of the economy. Suppose there are three possiblities for the economy next year: boom, expansion, and status quo. The joint probability distribution for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by:</p>
<table>
<thead>
<tr class="header">
<th align="center">State of Economy</th>
<th align="center">X/Y</th>
<th align="center">6</th>
<th align="center">4</th>
<th align="center">1</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Recession</td>
<td align="center">2</td>
<td align="center">0.15</td>
<td align="center">0.2</td>
<td align="center">0.1</td>
<td align="center">0.45</td>
</tr>
<tr class="even">
<td align="center">Expansion</td>
<td align="center">3</td>
<td align="center">0.1</td>
<td align="center">0.1</td>
<td align="center">0.2</td>
<td align="center">0.4</td>
</tr>
<tr class="odd">
<td align="center">Status quo</td>
<td align="center">4</td>
<td align="center">0.1</td>
<td align="center">0.05</td>
<td align="center">0</td>
<td align="center">0.15</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center"><strong>Total</strong></td>
<td align="center">0.35</td>
<td align="center">0.35</td>
<td align="center">0.3</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
</div>

<p>So in a recession, the probability of obtaining a return of 2/% on the stock and 6/% return on the commodity, i.e, <span class="math inline">\(P(X=2,Y=6)\)</span>, is 0.15.
Using the above joint probabity distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> we can compute two related distributions for each random variable:</p>
<ol style="list-style-type: decimal">
<li>Marginal distribution: For each random variable, we can extract its own probability distribution from the joint probability distribution. This is done by simply adding probabilities of all possible outcomes for a particular value of a given random variable. For example, the marginal distribution for <span class="math inline">\(X\)</span> is given by:</li>
</ol>
<p><span class="math display">\[P(X=x)=\sum_{i=1}^nP(X=x, Y=y_i)\]</span></p>
<p>Hence, in our example, the marginal distribution of <span class="math inline">\(X\)</span> is given by the last column, called Total in the table. For <span class="math inline">\(Y\)</span> it is the row called Total. We can use the marginal distribution to compute the unconditional expected value of each random variable. For example,</p>
<p><span class="math display">\[E(Y) = 6 \times P(Y=6) + 4 \times P(Y=4)+1 \times P(Y=1)=3.8\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Conditional distribution: For each random variable, we can also compute its probability distribution conditional on the other variable taking on a specific value. For exampl,e the conditional distribution of <span class="math inline">\(Y\)</span> given that <span class="math inline">\(X=x\)</span> is given by:</li>
</ol>
<p><span class="math display">\[ P(Y=y|X=x) =\frac{P(X=x,Y=y)}{P(X=x)}\]</span></p>
<p>From our example, what is the probability of obtaining 4% return on commodity under status quo if the return on the stock is 4%? So here we are interested in finding out:</p>
<p><span class="math display">\[ P(Y=4|X=4) =\frac{P(X=4,Y=4)}{P(X=4)}= \frac{0.05}{0.15}=0.33\]</span></p>
<p>To see this, note that from the table that P(X=4, Y=4) under status quo is given by 0.05. Also, using the definition of marginal distribution, we know that P(X=4)=0.15.</p>
<p>The conditional distribution of a random variable is a first step toward understanding the statistical relationship between two or more random variables. Just like the probability distribution of a random variable has a mean and a variance, the conditional distribution can similarly be characterized by conditional mean and conditional variance:</p>
<ol style="list-style-type: decimal">
<li>Conditional expected value (<span class="math inline">\(E(Y|X)\)</span>): Using the conditional distribution we can now compute the expected value of a random variable, given the value of another random variable. This is denoted by <span class="math inline">\(E(Y|X)\)</span> and can be computed as follows:</li>
</ol>
<p><span class="math display">\[E(Y|X)=y_1 \times P(Y=y_1|X=x) + y_2 \times P(Y=y_2|X=x)+...+ y_n \times P(Y=y_n|X=x)\]</span></p>
<p>As we can see, this expected value will be a function of <span class="math inline">\(X\)</span>. Depending on the realization of <span class="math inline">\(X\)</span> our expectation of <span class="math inline">\(Y\)</span> would change. In economics, we can imagine many such examples. For example, given our education level our expected wage will change. Similarly, given expenditure on advertising, expected sales will change. Hence, conditional expected value goes a long way in establishing statistical relationship between economic variables.</p>
<p>Going back to our example, let us compute the expected return on the commodity <span class="math inline">\(Y\)</span> conditional on the information that the return on <span class="math inline">\(X\)</span> is 3%:</p>
<p><span class="math display">\[E(Y|X) = 6 \times P(Y=6|X=3) + 4 \times P(Y=4|X=3) + 1 \times P(Y=1|X=3)\]</span></p>
<p>Here, <span class="math inline">\(P(Y=6|X=3)= \displaystyle\frac{0.1}{0.4}=0.25\)</span>, <span class="math inline">\(P(Y=4|X=3)= \displaystyle\frac{0.1}{0.4}=0.25\)</span> and <span class="math inline">\(P(Y=1|X=3)= \displaystyle\frac{0.1}{0.2}=0.5\)</span>. Hence, <span class="math inline">\(E(Y|X=3)=3\%\)</span>. Contrast this to the uncondtional expected value of <span class="math inline">\(Y\)</span> of 3.8% we computed earlier.</p>
<ol start="2" style="list-style-type: decimal">
<li>Conditional variance (<span class="math inline">\(Var(Y|X)\)</span>): Now even the variance of a random variable can be affected by another random variable. Here, we are interested in deviations of the random variable from its conditional mean:</li>
</ol>
<span class="math display">\[\begin{align}
Var(Y|X) = (y_1 -E(Y|X))^2 \times P(Y=y_1|X=x) + (y_2 -E(Y|X))^2\times P(Y=y_2|X=x)+...\\ \nonumber
+ (y_n -E(Y|X))^2 \times P(Y=y_n|X=x)
\end{align}\]</span>
</div>
<div id="measures-of-statistical-association" class="section level2">
<h2><span class="header-section-number">B.7</span> Measures of statistical association</h2>
<p>We can now define two measures of statistical relationship. The first one is called <strong>Covariance</strong> and the second is <strong>Correlation</strong>.</p>
<ol style="list-style-type: decimal">
<li>Covariance is a measure of association that captures how deviations from mean of one random variable are related to deviations of another random variable to its respective mean. For example, if your hours of study are above average, then what is your test score relative to average? Formally,</li>
</ol>
<p><span class="math display">\[Cov(X,Y) = E(Y-\mu_Y)(Y-\mu_X)\]</span></p>
<p>If the above number is positive, then there is a positive relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. That is, when <span class="math inline">\(X\)</span> is above its mean then <span class="math inline">\(Y\)</span> is also above its mean. If the number is negative then there is a negative relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Note that because <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are often in different units of measurement, the number we obtain for covariance has no meaning or implication for the strength of the relationship between two variables.</p>
<ol start="2" style="list-style-type: decimal">
<li>Correlation: is the value of covariance that is standardized by dividing this number by standard deviations of each random variable:</li>
</ol>
<p><span class="math display">\[Cor(X,Y) = \frac{Cov(X,Y)}{\sigma_X \times \sigma_Y }\]</span></p>
<p>This number is unit free and falls between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. The sign of the correlation tell us about the direction of the relationship whereas the value of the correlation gives information about the strength of the relationship. A higher absolute value indicates stronger statistical relationship between two variables.</p>
<div id="rules-of-expectation-and-variances" class="section level3">
<h3><span class="header-section-number">B.7.1</span> Rules of expectation and variances</h3>
<p>Here are some useful rules that are useful for our purpose:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E(\beta)=\beta\)</span> and <span class="math inline">\(Var(\beta)=0\)</span> where <span class="math inline">\(\beta\)</span> denotes a constant.</li>
<li><p><span class="math inline">\(E(\beta X)= \beta E(X)\)</span> and <span class="math inline">\(Var(\beta X)= \beta^2 Var(X)\)</span> where <span class="math inline">\(\beta\)</span> denotes a constant.</p></li>
<li><p>Consider two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and let <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> denotes two constants. Then,</p>
<p>3.1. <span class="math inline">\(E(aX+bY)=aE(X)+bE(Y)\)</span></p>
<p>3.2. <span class="math inline">\(E(aX-bY)=aE(X)-bE(Y)\)</span></p>
<p>3.3. <span class="math inline">\(Var(aX+bY)=a^2 Var(X)+b^2 Var(Y)+2abCor(X,Y)\sqrt{Var(X)}\sqrt{Var(Y)}\)</span></p>
<p>3.4. <span class="math inline">\(Var(aX-bY)=a^ 2Var(X)+b^2 Var(Y)-2abCor(X,Y)\sqrt{Var(X)}\sqrt{Var(Y)}\)</span></p></li>
</ol>
</div>
</div>
<div id="sampling-and-estimation" class="section level2">
<h2><span class="header-section-number">B.8</span> Sampling and Estimation</h2>
<p>An important distinction in statistics is between the population of interest and a sample of this population that we usually work with. Due to feasibility of data collection and cost both in terms of time and money, most real world analysis is based on a sample that is a subset of the population of interes. For example, to study how business major affects starting salary, the relevant population is all business majors from a graduating class in the U.S. in a given year. In practice however, we will most likely use a sample of this population, for example all business majors from JMU. How useful an analysis based on a sample is depends on how representative the chosen sample is of the entire population.</p>
<p>For our purpose, lack of data on population means that the true probablity distribution of a random variable is unknown and hence the true values of mean, variance, covarinace etc are also unknown to us. Statistics provides a way of using samples to <strong>estimate</strong> relevant moments of the probability distribution. The approach we take is as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Consider the unknown moments of the true probability distribution as ** population parameters** that we would like to estimate.</p></li>
<li><p>Draw a representative sample from the population. In simple random sampling we draw <span class="math inline">\(n\)</span> obeservations at random so that each member of the population is equally likely to be included in the sample. We can also use other complex sampling schemes where certain groups of population are more likely to be selected in the sample than others. Two examples:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose we are interested in finding out starting salary of CoB majors at JMU. The population will be every graduating student for a given year. However, we may work with a sample of students, where we draw randomly from every major ensuring that all graduating students have equal probability of selection.</p></li>
<li><p>Suppose we are interested in finding out usage of food stamps in Harrisonburg area. The population of interest will be all residents of Harrisonburg who use food stample. However, we may work with a sample where a certain demographic group is more likely to be part of the sample (and hence is <em>oversampled</em>).</p></li>
</ol></li>
<li><p>Use the sample to compute sample estimates for each population parameter of interest. For example for expected value we can use sample mean as an estimator, for variance we can use sample variance as an estimator and so on. There are following key differences between population parameters and their sample estimates:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Population parameters are true but unknown values that we are interested in measuring. In contrast, sample estimates can be computed using our sample data.</p></li>
<li><p>Population parameters are fixed whereas sample estimates change as we change our sample. For example, if we compute mean starting salary of business majors from JMU we get one number. If use data from UVA we get another number for mean starting salary.</p></li>
<li><p>Because different samples give us different sample estimates for the same population parameter, we need to ensure that our sample estimator from one sample data is reliable.</p></li>
</ol></li>
<li><p>Sampling distribution: Hypothetically, we can draw many samples from the same population and compute sample estimate for each sample. This will give us a distribution of for the sample estimate which will have its own mean and variance. We can use this sampling distribution to:</p>
<ol style="list-style-type: lower-alpha">
<li>Establish reliablity of the sample estimator. Specifically any sample estimator should be unbiased and efficient. More on this in the next section.</li>
<li>Statsitically test hypotheses about the true population parameter
### Unbiasedness and efficiency</li>
</ol></li>
</ol>
<p>Let <span class="math inline">\(\theta\)</span> denote a population parameter of interest. For example, it can be the mean of the random variable of interest. Let <span class="math inline">\(\widehat{\theta}\)</span> denotes a sample estimator of <span class="math inline">\(\theta\)</span> that can be computed using sample data. Then,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\widehat{\theta}\)</span> is an <strong>unbiased</strong> estimator of <span class="math inline">\(\theta\)</span> if:</li>
</ol>
<p><span class="math display">\[E(\widehat{\theta})=\theta\]</span></p>
<p>The idea here is that if we repeatedly draw a sample from the same population and compute <span class="math inline">\(\widehat{\theta}\)</span> for each such sample, the average of these estimators must be equal to the true population parameter for unbiasedness. In otherwords, the center of the sampling distribution is at the true population parameter value.</p>
<p>We can now define <strong>bias</strong> of an estimator as follows:</p>
<p><span class="math display">\[Bias(\widehat{\theta}) = E(\widehat{\theta})-\theta\]</span></p>
<p>For an unbiased estimator, <span class="math inline">\(Bias(\widehat{\theta})=0\)</span>. If <span class="math inline">\(Bias(\widehat{\theta})&gt;0\)</span> then we have an over-estimate and if <span class="math inline">\(Bias(\widehat{\theta})&lt;0\)</span> then we have an under-estimate.</p>
<ol start="2" style="list-style-type: decimal">
<li>Efficiency: Unbiasedness ensure that the average of sample estimator is equal to the true population parameter. But if the standard deviation of the sample estimator is too high, then knowing that the average is close to the true value is not very useful. In statistics, we call such an estimator unbiased but <strong>imprecise or inefficient</strong>. To be efficient the standard deviation (or variance) of the sample estimator should be as small as possible. Between two unbiased estimators, a more efficient estimator will have a lower variance.</li>
</ol>

<div class="example">
<p><span id="exm:unnamed-chunk-57" class="example"><strong>Example B.8  </strong></span>Suppose we have a random sample with <span class="math inline">\(n\)</span> observations: <span class="math inline">\(\{x_1,x_2,...,x_n\}\)</span> drawn from a population with a mean of <span class="math inline">\(\mu_x\)</span>. Sample mean is defined as:</p>
<p><span class="math display">\[\overline{X}=\frac{\sum_{i=1}^N x_i}{N}\]</span></p>
<p>The expected value of the sample mean is given by:</p>
<p><span class="math display">\[E(\overline{X})=E\left(\frac{\sum_{i=1}^N x_i}{N}\right)\]</span></p>
<p>Using properties of the expected value, we get:</p>
<p><span class="math display">\[E(\overline{X})=\frac{E(x_1)+E(x_2)+...+ E(x_N)}{N}\]</span></p>
<p>Note that because this is a random sample from the same population with a mean of <span class="math inline">\(mu_x\)</span>, we get <span class="math inline">\(E(x_1)=E(x_2)=..=E(x_n)=\mu_x\)</span>. Hence,</p>
<p><span class="math display">\[E(\overline{X})=\frac{\overbrace{\mu_x+\mu_x+...+\mu_x}^{\text{N terms}} }{N}=\mu_x\]</span></p>
<p>As a result the sample mean is an unbiased estimator of the population mean. However, there are many other possible unbiased estimators of the population mean. We can show that among all other unbiased estimator of the population mean, sample mean has the lowest variance and hence is most efficient estimator as well.</p>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-58" class="definition"><strong>Definition B.7  (Best Unbiased Estimator (BUE))  </strong></span>Let <span class="math inline">\(\theta\)</span> denote a population parameter of interest. Then, an sample estimator denoted by <span class="math inline">\(\hat{\theta}\)</span> is the  of <span class="math inline">\(\theta\)</span> if the following two conditions are satisfied:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\hat{theta}\)</span> is an unbiased estimator, i.e., <span class="math inline">\(E(\hat{\theta})=\theta\)</span>. In this case the sampling distribution is centered at the true value of the parameter.</p></li>
<li><span class="math inline">\(\hat{\theta}\)</span> is an efficient estimator, i.e., <span class="math inline">\(Var(\hat{\theta})&lt; Var(\hat{\theta_A})\)</span> for any other unbiased estimator denoted by <span class="math inline">\(\hat{\theta_A}\)</span>. In this case the width of the sampling distribution around the mean is smallest possible.</li>
</ol>
</div>

</div>
<div id="hypothesis-testing" class="section level2">
<h2><span class="header-section-number">B.9</span> Hypothesis testing</h2>
<p>An important part of any statistical analysis is testing various hypotheses about population parameters of interest. This is known as  and here we use the sampling distribution of the estimator to formally test whether the corresponding population of interest takes a certain value or not. This is important because even with an best unbiased estimator we do not know the true value of the population parameter of interest. In this section we will look at two types of hypotheses testing procedures that are most relevant for Econometrics. The procedure for any statistical test more or less consists of the following steps:</p>
<ol style="list-style-type: decimal">
<li><p>Formulate a hypothesis of interest. This typically manifest as a restriction on the value of a population parameter (or a combination of multiple parameters). The goal is to test whether there is support for this restriction in our sample or not. There are two types of hypotheses that we must formulate:</p>
<p>1.1. Null Hypothesis (<span class="math inline">\(H_0\)</span>): A null hypothesis is the statement about the population parameter we assume to be true until we find evidence otherwise. For example, we can test whether the population mean of starting salary for CoB majors is $60,000. Formally,</p>
<p><span class="math display">\[H_0: \mu_X = 60,000\]</span></p>
<p>Note that the null hypothesis statement is an equality condition.</p>
<p>1.2. Alternative Hypothesis (<span class="math inline">\(H_A\)</span>): This is the logical counterpart of the null hypothesis and here we specify. There are two types of alternative hypothesis we can specify:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Two-sided alternative: Here, the alternative hypothesis statement allows for both sides of the inequality. Going back to our example of starting salary, a two-sided alternative will be:</p>
<p><span class="math display">\[H_A: \mu_X \neq 60,000\]</span></p></li>
<li><p>One-sided alternative: Here, we either use a greater or less than sign for the alternative hypothesis. So for example, we can speficy the following one-sided alternative:</p>
<p><span class="math display">\[H_A: \mu_X &gt; 60,000\]</span></p></li>
</ol></li>
<li><p>Compute the relevant test statistic that is a function of the sample data. The formula for the test statistic is a function of the sample estimator and the value of the population parameter(s) we assumed in the null hypothesis.</p></li>
<li><p>The test statistic is assumed to follow a certain probability distribution under the assumption that the null hypothesis is correct. The tails of this distribution summarizes values of the test statistic that are less likely to realize. Such a value of the test statistic provides us a threshold level, called the , beyond which the test statistic values are less likely to realize if our hypothesis is true. The decision rule for rejecting or not rejecting the null hypothesis is based on the comparison between the computed test statistic and the associated critical value.</p></li>
</ol>
<p>Note that there is always a measure of uncertainty in any hypothesis testing: we may end up making a wrong decision. There are two types of errors we can make here:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Type I</strong> error: here we reject <span class="math inline">\(H_0\)</span> when it is true. The probability of this type of error is denoted by <span class="math inline">\(\alpha\)</span> and is called the <strong>level of significance</strong> of a test.</p></li>
<li><p><strong>Type II</strong> error: here we do reject <span class="math inline">\(H_0\)</span> when it is false. The probability of this type of error is related to the <strong>power</strong> of a test.</p></li>
</ol>
<p>Ideally we would like to minimize the probability of both types of errors but we cannot do that because reducing one error comes at the cost of increasing the other. As a result, we first specificy an <strong>acceptable</strong> level of significance (type one error probability) and then try to minimize the probability of type two error (or maximize the power of the test). It is common to assume a level of signficance of 5% or <span class="math inline">\(\alpha=0.05\)</span>. So here we are willing to tolerate a 5% chance of falsely rejecting the null hypothesis.</p>
<p>Once we have fixed the level of significance, we can use the distribution table of the test-statistic to obtain the corresponding critical value(s).</p>
<div id="testing-a-restriction-on-a-single-population-parameter" class="section level3">
<h3><span class="header-section-number">B.9.1</span> Testing a restriction on a single population parameter</h3>
<p>Here our goal is to develop tests for testing statements about a single population parameter of interest. So for example, we can either test a statement about a population mean or a population variance.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-59" class="example"><strong>Example B.9  (t-test for population mean)  </strong></span>
Suppose you are interested in measuring mean hourly wage of males aged 25-35. Accordingly, we collect a sample of 100 workers from the population of male in this age group with a mean of <span class="math inline">\(\mu_X\)</span> and a standard deviation of <span class="math inline">\(\sigma_X\)</span>. The sample mean is <span class="math inline">\(\hat{\mu}_X=\$25\)</span> and the sample standard deviation is <span class="math inline">\(\hat{\sigma}_X=\$7\)</span>. Now, suppose we want to test the following hypothesis:</p>
<p><span class="math display">\[H_0: \mu_X=27\]</span></p>
<p><span class="math display">\[H_0: \mu_X \neq 27\]</span></p>
<p>The test statistic is given by the <em>t-statistic</em> where:</p>
<p><span class="math display">\[t=\frac{\hat{\mu}_X-\mu_X}{s.e.(\hat{\mu}_X)} \]</span></p>
<p>where <span class="math inline">\(s.e.(\hat{\mu}_X)=\displaystyle \frac{\hat{\sigma}_X}{\sqrt{N}}\)</span> is the standard error of sample mean and N denotes sample size.</p>
<p>If the null hypothesis is true, this test statistic follows <strong>t-distribution</strong> with N-1 degrees of freedom. Using the t-distribution table we can then compute the critical value which is used in formulating the decision rule. Let <span class="math inline">\(t_c\)</span> denote this critical value from the distribution table. Then,</p>
<p><span class="math display">\[|t|&gt;t_c \quad \Rightarrow  \text{reject $H_0$} \]</span></p>
<p><span class="math display">\[|t|&lt;t_c \quad \Rightarrow  \text{do not reject $H_0$} \]</span></p>
<p>In our example, <span class="math inline">\(N=100\)</span>, and</p>
<p><span class="math display">\[t=\frac{25-27}{\frac{7}{\sqrt{100}}}=-2.86\]</span></p>
<p>The degrees of freedom is <span class="math inline">\(N-1=99\)</span> and at 5% level of significance the critical value from the t-distribution table is <span class="math inline">\(t_c=1.98\)</span>. Because |t| is larger than the critical value, we reject the null hypothesis. Hence, we find evidence against the statement that the mean hourly wage of male workers is $25.</p>
<p>Note that an alternative way of testing hypothesis like this is to use the <strong>p-value</strong> rule. The underlying idea is to find out the largest significance level at which we will fail to reject the null hypothesis. This value is called the p-value and most statistical softwares report this value. The decision-rule is then greatly simplified:</p>
<p><span class="math display">\[\text{If p-value is less than the chosen level of significance (value of $\alpha$) then reject $H_0$.}\]</span></p>
<p>In our case, the p-value is 0.0053. Because we chose <span class="math inline">\(\alpha=0.05\)</span>, according to the p-value rule we will reject the null hypothesis.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-60" class="example"><strong>Example B.10  (Chi-square test for population variance)  </strong></span>Using the same example, we also test a statement about the population variance. Suppose we want to test whether the variance of the hourly wage is 52.</p>
<p><span class="math display">\[H_0: \sigma^2_X=52\]</span></p>
<p><span class="math display">\[H_0: \sigma^2_X &gt;52 \]</span></p>
<p>The test statistic is given by the <em>V-statistic</em> where:
<span class="math display">\[V=\frac{(N-1)\times \hat{\sigma^2_X}}{\sigma^2_X}\]</span></p>
<p>If the null hypothesis is true, this test statistic follows <strong>Chi-square distribution</strong> with N-1 degrees of freedom. Using the distribution table we can then compute the critical value which is used in formulating the decision rule. Let <span class="math inline">\(V_c\)</span> denote this critical value from the distribution table. Then,</p>
<p><span class="math display">\[V&gt;V_c \quad \Rightarrow  \text{reject $H_0$} \]</span></p>
<p><span class="math display">\[V&lt;V_c \quad \Rightarrow  \text{do not reject $H_0$} \]</span></p>
<p>In our example,</p>
<p><span class="math display">\[V=\frac{(100-1)\times 7^2}{52}=93.29\]</span></p>
The degrees of freedom is <span class="math inline">\(N-1=99\)</span> and at 5% level of significance the critical value from the Chi-square distribution table is <span class="math inline">\(V_c=43.77\)</span>. Because <span class="math inline">\(V\)</span> is larger than the critical value, we reject the null hypothesis. Hence, we find evidence against the statement that the variance of the hourly wage of male workers is 52.
</div>

</div>
<div id="testing-a-restriction-on-multiple-population-parameter" class="section level3">
<h3><span class="header-section-number">B.9.2</span> Testing a restriction on multiple population parameter</h3>
<p>Often we are interested in testing a restriction that is a linear combination of two or more population means. Similarly, we maybe interested in comparing the variance of two different populations. In such cases we need to develop statistical tests that allow for comparison between parameters of different populations with given means and variances.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-61" class="example"><strong>Example B.11  (t-test for comparing population mean of two populations)  </strong></span>Suppose you are interested in comparing mean weekly hours studied by Econ majors (X) and non-Econ majors in the college of business. For this purpose, you collect a sample of 25 econ majors and a sample of 30 non-econ majors. The sample mean of weekly hours studied by econ majors is 10 hours with a standard deviation of 4 hours. The sample mean of weekly hours studied by non-econ majors is 8 hours with a standard deviation of 2 hours. Also suppose that the covariance between weekly hours studied by econ and non-econ majors is 0.12. Test whether mean weekly hours studied by econ majors is more than the mean weekly hours studied by non-Econ majors.</p>
<p>Let <span class="math inline">\(X\)</span> denote hours studied, <span class="math inline">\(N_X\)</span> denotes sample size, <span class="math inline">\(\hat{\mu}_X\)</span>, and <span class="math inline">\(\hat{\sigma}_X\)</span> denote sample mean and standard deviation, respectively for econ majors. Similarly, let <span class="math inline">\(Y\)</span> denote hours studied, <span class="math inline">\(N_Y\)</span> denotes sample size, <span class="math inline">\(\hat{\mu}_Y\)</span>, and <span class="math inline">\(\hat{\sigma}_Y\)</span> denote sample mean and standard deviation, respectively for non-econ majors.</p>
<p>The first step, as usual, is to formulate the null and the alternative hypotheses:</p>
<p><span class="math display">\[H_0= \mu_X - \mu_Y = 0\]</span>
<span class="math display">\[H_A= \mu_X - \mu_Y &gt; 0\]</span></p>
<p>The next step is to compute the relevant test statistic, which in this case is the t-ratio given by:</p>
<p><span class="math display">\[t= \frac{(\hat{\mu_X}-\hat{\mu_Y})-0}{s.e.(\hat{\mu_X}-\hat{\mu_Y})}\]</span></p>
<p>Using the properties of variance, we get:
<span class="math display">\[s.e.(\hat{\mu_X}-\hat{\mu_Y})=\sqrt{Var(\hat{\mu_X})+Var(\hat{\mu_Y})-2 \times Cor(X,Y)\times s.e.(\hat{\mu_X}) \times s.e.(\hat{\mu_Y})}=0.84\]</span></p>
<p>So, <span class="math inline">\(t=\displaystyle \frac{10-8}{0.84}=2.38\)</span></p>
The sample size here is <span class="math inline">\(N_X+N_Y=55\)</span>. Using 5% level of significance and degrees of freedom of 53, the critical value from the t-distribution table for the one-sided alternative is 1.67. Because the |t| is more than 1.67, we reject the null hypothesis. We find evidence for econ majors studying more on average than non-econ majors in our sample.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-62" class="example"><strong>Example B.12  (F-test for comparing population variance of two populations)  </strong></span>Often we may be interested in comparing the variability between two populations. Using our previous example, we may want to test whether variability in hours studied is bigger for econ majors versus non-econ majors. This can be tested by comparing the ratio of two variances against the value of 1. As before, we start by formulating the null and the alternative hypotheses:</p>
<p><span class="math display">\[H_0: \sigma^2_X/sigma^2_Y = 1\]</span>
<span class="math display">\[H_0: \sigma^2_X/sigma^2_Y &gt; 1\]</span></p>
<p>The corresponding test statistic is the F-ratio:</p>
<p><span class="math display">\[F = \frac{\hat{\sigma^2_X}}{\hat{\sigma^2_Y}}=\frac{4^2}{2^2}=4\]</span></p>
<p>If the null hypothesis is true, the above test statistic follows F-distribution with <span class="math inline">\(N_x-1\)</span> degrees of freedom for the numerator and <span class="math inline">\(N_y-1\)</span> degrees of freedom for the denominator. At 5% level of significance, the critical value for <span class="math inline">\(\nu_1=24\)</span> and <span class="math inline">\(\nu_2=29\)</span> from the F-distribution table is 3. Because the computed F-ratio exceeds the critical value we reject the null hypothesis.</p>
</div>

</div>
<div id="confidence-interval-and-hypothesis-testing" class="section level3">
<h3><span class="header-section-number">B.9.3</span> Confidence interval and Hypothesis testing</h3>
<p>One issue with using a sample to estimate population parameters is that by definition a sample estimator will be different for different samples. Thus, sample mean provides no information about how close this estimator is to the true population mean. This uncertainty in estimation can be summarized by computing the standard deviation, with higher value of standar deviation indicating greater uncertainty about the true population parameter. A better measure of this uncertainty is the <strong>confidence interval</strong>.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-63" class="definition"><strong>Definition B.8  (Confidence Interval)  </strong></span>Suppose we draw a random sample <span class="math inline">\(\{x_1, x_2,...,x_N\}\)</span> from a normally distributed population with mean of <span class="math inline">\(\mu_X\)</span> and a standard deviation of <span class="math inline">\(\sigma_X\)</span>. Let <span class="math inline">\(\hat{\mu_X}\)</span> denotes the sample mean and <span class="math inline">\(\hat{\sigma_X}\)</span> denotes sample standard deviation. Then, the 95% confidence interval for <span class="math inline">\(\hat{\mu_X}\)</span> is given by:</p>
<p><span class="math display">\[\left[\hat{\mu_X}-t_{c,2-sided} \times \frac{\hat{\sigma_X}}{\sqrt{N}},\hat{\mu_X}+t_{c,2-sided} \times \frac{\hat{\sigma_X}}{\sqrt{N}} \right]\]</span></p>
where <span class="math inline">\(t_{c,2-sided}\)</span> is the critical value that can be obtained from the t-distribution table for a given level of signicance and degrees of freedom. For example, for a 95% confidence interval we will use 5% level of significance.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-64" class="example"><strong>Example B.13  </strong></span>Suppose N=20, <span class="math inline">\(\hat{\mu_X}=5\)</span>, and <span class="math inline">\(\hat{\sigma_X}=2\)</span>. Then, the 95% confidence interval for <span class="math inline">\(\hat{\mu_X}\)</span> is given by:</p>
<p><span class="math display">\[\left[5-2.093 \times \frac{2}{\sqrt{20}}, 5+2.093 \times \frac{2}{\sqrt{20}} \right]=[4.06,5.94]\]</span></p>
<p>Hence, before we drew our sample from the population, there is a 95% chance that the true population parameter (<span class="math inline">\(\mu_X\)</span>) will fall between 4.12 and 5.94. Note that:</p>
<ol style="list-style-type: decimal">
<li><p>Wider the confidence interval, greater is the uncertainty about the true value of the population mean.</p></li>
<li><p>We can use the confidence interval to conduct hypothesis testing for a <strong>two-sided</strong> alternative hypothesis. If the null hypothesis value does not fall in the confidence interval, then with 95% confidence (or at 5% level of significance) we can reject the null hypothesis. For example, consider the following test:</p></li>
</ol>
<p><span class="math display">\[H_0: \mu_X=3.8\]</span>
<span class="math display">\[H_A: \mu_X\neq 3.8\]</span></p>
<p>Because 3.8 is not in the confidence interval we will reject the null hypothesis at 5% level of significance. Note that we will obtain the same conclusion if we were to compute the t-ratio and compare it with the corresponding critical value from the t-distribution table.</p>
</div>

</div>
</div>
<div id="problems-1" class="section level2 unnumbered">
<h2>Problems</h2>

<div class="exercise">
<p><span id="exr:unnamed-chunk-65" class="exercise"><strong>Exercise B.1  </strong></span>Suppose you roll a 6-sided fair dice. If an odd number shows you win $10. If either 2 or 4 shows you lose $5. If 6 shows, you neither gain nor lose anything.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Denote the winnings from this game as <span class="math inline">\(X\)</span>. Tabulate the probability distribiution of the random variable <span class="math inline">\(X\)</span>.</p></li>
<li>Compute the expected value and the standard deviation for <span class="math inline">\(X\)</span>.</li>
</ol>
</div>


<div class="exercise">
<p><span id="exr:unnamed-chunk-66" class="exercise"><strong>Exercise B.2  </strong></span>
Consider a population with a mean of <span class="math inline">\(\mu\)</span> and variance of <span class="math inline">\(\sigma^2\)</span>. Suppose you draw a random sample <span class="math inline">\(X_1, X_2,..,X_N\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Show that <span class="math inline">\(\hat{\mu_A}=0.25\times X_1 +0.25\times X_3+ 0.25 \times X_8 + 0.25 X_20\)</span> is an unbiased estimator of <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>Show that <span class="math inline">\(\hat{\mu_B}=0.1\times X_1 +0.1\times X_3+ 0.5 \times X_8+0.3 \times X_11\)</span> is an unbiased estimator of <span class="math inline">\(\mu\)</span>.</p></li>
<li>Now compute variance of <span class="math inline">\(\hat{\mu_A}\)</span> and <span class="math inline">\(\hat{\mu_B}\)</span>. Which one is more efficient estimator of <span class="math inline">\(\mu\)</span>.</li>
</ol>
</div>


<div class="exercise">
<p><span id="exr:unnamed-chunk-67" class="exercise"><strong>Exercise B.3  </strong></span>Suppose you collect a random sample of 100 observations and find that sample mean is -25 and sample variance is 350.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Test whether the population mean is -22.</p></li>
<li>Test whether the population variance is 400.</li>
</ol>
</div>


<div class="exercise">
<p><span id="exr:unnamed-chunk-68" class="exercise"><strong>Exercise B.4  </strong></span>Suppose you are interested in comparing performance of two different mutual funds, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Let <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\(mu_Y\)</span> denote unknown population mean returns on investment in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively. Suppose you collect past 20 months data for both mutual funds and find that sample mean for fund <span class="math inline">\(X\)</span> is 2% with a standard deviation of 0.5%. In contrast, the sample mean for fund <span class="math inline">\(Y\)</span> is 5% with a standard deviation of 2%. Suppose that the correlation between returns on these two funds is 0.2.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Test whether mean return on <span class="math inline">\(Y\)</span> is greater than that on <span class="math inline">\(X\)</span>.</p></li>
<li><p>Test whether variance of <span class="math inline">\(Y\)</span> is greater than that of <span class="math inline">\(X\)</span>.</p></li>
<li>Compute the 95% confidence interval for <span class="math inline">\(\hat{\mu_X}\)</span>. Using the confidence interval, what can you say about the population mean return for fund <span class="math inline">\(X\)</span>?</li>
</ol>
</div>


</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="appendix-a-review-of-differential-calculus-and-optimization.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
