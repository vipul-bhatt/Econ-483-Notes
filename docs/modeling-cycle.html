<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 6 Modeling Cycle | Applied Time Series Analysis</title>
  <meta name="description" content="Lecture notes for Applied Time Series Analysis">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 6 Modeling Cycle | Applied Time Series Analysis />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes for Applied Time Series Analysis" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Modeling Cycle | Applied Time Series Analysis />
  
  <meta name="twitter:description" content="Lecture notes for Applied Time Series Analysis" />
  

<meta name="author" content="Vipul Bhatt">


<meta name="date" content="2019-05-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="modeling-trend-and-seasonal-components.html">
<link rel="next" href="modeling-volatility.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; min-height: 1.25em; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; }
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
@media screen {
a.sourceLine::before { text-decoration: underline; color: initial; }
}
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.bn { color: #40a070; } /* BaseN */
code span.fl { color: #40a070; } /* Float */
code span.ch { color: #4070a0; } /* Char */
code span.st { color: #4070a0; } /* String */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.ot { color: #007020; } /* Other */
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.fu { color: #06287e; } /* Function */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code span.cn { color: #880000; } /* Constant */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.ss { color: #bb6688; } /* SpecialString */
code span.im { } /* Import */
code span.va { color: #19177c; } /* Variable */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.op { color: #666666; } /* Operator */
code span.bu { } /* BuiltIn */
code span.ex { } /* Extension */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.at { color: #7d9029; } /* Attribute */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Time Series Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Forecasting</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#time-series"><i class="fa fa-check"></i><b>1.1</b> Time Series</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#serial-correlation"><i class="fa fa-check"></i><b>1.2</b> Serial Correlation</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#testing-for-serial-correlion"><i class="fa fa-check"></i><b>1.3</b> Testing for Serial Correlion</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#white-noise-process"><i class="fa fa-check"></i><b>1.4</b> White Noise Process</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#important-elements-of-forecasting"><i class="fa fa-check"></i><b>1.5</b> Important Elements of Forecasting</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#loss-function-and-optimal-forecast"><i class="fa fa-check"></i><b>1.6</b> Loss Function and Optimal Forecast</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html"><i class="fa fa-check"></i><b>2</b> Regression-based Forecasting</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#scenario-analysis-and-conditional-forecasts"><i class="fa fa-check"></i><b>2.1</b> Scenario Analysis and Conditional Forecasts</a></li>
<li class="chapter" data-level="2.2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#unconditional-forecasts"><i class="fa fa-check"></i><b>2.2</b> Unconditional Forecasts</a></li>
<li class="chapter" data-level="2.3" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#some-practical-issues"><i class="fa fa-check"></i><b>2.3</b> Some practical issues</a></li>
<li class="chapter" data-level="2.4" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#distributed-lag-regression-models"><i class="fa fa-check"></i><b>2.4</b> Distributed Lag Regression Models</a><ul>
<li class="chapter" data-level="2.4.1" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#dynamic-effect-of-x-on-y"><i class="fa fa-check"></i><b>2.4.1</b> Dynamic Effect of X on Y</a></li>
<li class="chapter" data-level="2.4.2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#model-selection-criterion"><i class="fa fa-check"></i><b>2.4.2</b> Model Selection Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#application-a-model-of-investment-expenditure"><i class="fa fa-check"></i><b>2.5</b> Application: A Model of Investment Expenditure</a><ul>
<li class="chapter" data-level="2.5.1" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#a-multiple-regression-model-of-invesment-expenditure"><i class="fa fa-check"></i><b>2.5.1</b> A Multiple Regression Model of Invesment Expenditure</a></li>
<li class="chapter" data-level="2.5.2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#a-distributed-lag-model-of-investment-expenditure"><i class="fa fa-check"></i><b>2.5.2</b> A Distributed Lag Model of Investment Expenditure</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="components-of-a-time-series.html"><a href="components-of-a-time-series.html"><i class="fa fa-check"></i><b>3</b> Components of a Time Series</a><ul>
<li class="chapter" data-level="3.1" data-path="components-of-a-time-series.html"><a href="components-of-a-time-series.html#decomposing-a-time-series"><i class="fa fa-check"></i><b>3.1</b> Decomposing a time series</a></li>
<li class="chapter" data-level="3.2" data-path="components-of-a-time-series.html"><a href="components-of-a-time-series.html#uses-of-decomposition-of-a-time-series"><i class="fa fa-check"></i><b>3.2</b> Uses of Decomposition of a time series</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="smoothing-methods.html"><a href="smoothing-methods.html"><i class="fa fa-check"></i><b>4</b> Smoothing Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="smoothing-methods.html"><a href="smoothing-methods.html#moving-average-method"><i class="fa fa-check"></i><b>4.1</b> Moving Average Method</a></li>
<li class="chapter" data-level="4.2" data-path="smoothing-methods.html"><a href="smoothing-methods.html#simple-exponential-smoothing"><i class="fa fa-check"></i><b>4.2</b> Simple Exponential Smoothing</a></li>
<li class="chapter" data-level="4.3" data-path="smoothing-methods.html"><a href="smoothing-methods.html#holt-winters-smoothing"><i class="fa fa-check"></i><b>4.3</b> Holt-Winters Smoothing</a></li>
<li class="chapter" data-level="4.4" data-path="smoothing-methods.html"><a href="smoothing-methods.html#holt-winters-smoothing-with-seasonality"><i class="fa fa-check"></i><b>4.4</b> Holt-Winters Smoothing with Seasonality</a></li>
<li class="chapter" data-level="4.5" data-path="smoothing-methods.html"><a href="smoothing-methods.html#application"><i class="fa fa-check"></i><b>4.5</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html"><i class="fa fa-check"></i><b>5</b> Modeling Trend and Seasonal Components</a><ul>
<li class="chapter" data-level="5.1" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#trend-estimation"><i class="fa fa-check"></i><b>5.1</b> Trend Estimation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#parametrizing-a-deterministic-trend"><i class="fa fa-check"></i><b>5.1.1</b> Parametrizing a deterministic trend</a></li>
<li class="chapter" data-level="5.1.2" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#uses-of-the-deterministic-trend-model"><i class="fa fa-check"></i><b>5.1.2</b> Uses of the Deterministic Trend Model</a></li>
<li class="chapter" data-level="5.1.3" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#application-estimating-a-polynomial-trend-for-u.s.-real-gdp"><i class="fa fa-check"></i><b>5.1.3</b> Application: Estimating a polynomial trend for U.S. Real GDP</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#seasonal-model"><i class="fa fa-check"></i><b>5.2</b> Seasonal Model</a><ul>
<li class="chapter" data-level="5.2.1" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#regression-model-with-seasonal-dummy-variables"><i class="fa fa-check"></i><b>5.2.1</b> Regression Model with Seasonal Dummy Variables</a></li>
<li class="chapter" data-level="5.2.2" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#application-seasonal-model-of-housing-starts"><i class="fa fa-check"></i><b>5.2.2</b> Application: Seasonal Model of Housing Starts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modeling-cycle.html"><a href="modeling-cycle.html"><i class="fa fa-check"></i><b>6</b> Modeling Cycle</a><ul>
<li class="chapter" data-level="6.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#stationarity-and-autocorrelation"><i class="fa fa-check"></i><b>6.1</b> Stationarity and Autocorrelation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#covariance-stationary-time-series"><i class="fa fa-check"></i><b>6.1.1</b> Covariance Stationary Time Series</a></li>
<li class="chapter" data-level="6.1.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#correlation-vs-autocorrelation"><i class="fa fa-check"></i><b>6.1.2</b> Correlation vs Autocorrelation</a></li>
<li class="chapter" data-level="6.1.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#partial-autocorrelation"><i class="fa fa-check"></i><b>6.1.3</b> Partial Autocorrelation</a></li>
<li class="chapter" data-level="6.1.4" data-path="modeling-cycle.html"><a href="modeling-cycle.html#lag-operator"><i class="fa fa-check"></i><b>6.1.4</b> Lag operator</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#autoregressive-ar-model"><i class="fa fa-check"></i><b>6.2</b> Autoregressive (AR) Model</a><ul>
<li class="chapter" data-level="6.2.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#unit-root-and-stationarity"><i class="fa fa-check"></i><b>6.2.1</b> Unit root and Stationarity</a></li>
<li class="chapter" data-level="6.2.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#properties-of-an-ar1-model"><i class="fa fa-check"></i><b>6.2.2</b> Properties of an AR(1) model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#estimating-an-ar-model"><i class="fa fa-check"></i><b>6.3</b> Estimating an AR model</a><ul>
<li class="chapter" data-level="6.3.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>6.3.1</b> Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="6.3.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#mle-of-an-arp-model"><i class="fa fa-check"></i><b>6.3.2</b> MLE of an AR(p) model</a></li>
<li class="chapter" data-level="6.3.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#selection-of-optimal-order-of-the-ar-model"><i class="fa fa-check"></i><b>6.3.3</b> Selection of optimal order of the AR model</a></li>
<li class="chapter" data-level="6.3.4" data-path="modeling-cycle.html"><a href="modeling-cycle.html#forecasting-using-arp-model"><i class="fa fa-check"></i><b>6.3.4</b> Forecasting using AR(p) model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="modeling-cycle.html"><a href="modeling-cycle.html#moving-average-ma-model"><i class="fa fa-check"></i><b>6.4</b> Moving Average (MA) Model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#invertibility-of-an-ma-process"><i class="fa fa-check"></i><b>6.4.1</b> Invertibility of an MA process</a></li>
<li class="chapter" data-level="6.4.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#properties-of-an-invetible-ma1"><i class="fa fa-check"></i><b>6.4.2</b> Properties of an invetible MA(1)</a></li>
<li class="chapter" data-level="6.4.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#forecast-based-on-maq"><i class="fa fa-check"></i><b>6.4.3</b> Forecast based on MA(q)</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="modeling-cycle.html"><a href="modeling-cycle.html#armap-q"><i class="fa fa-check"></i><b>6.5</b> ARMA(p, q)</a></li>
<li class="chapter" data-level="6.6" data-path="modeling-cycle.html"><a href="modeling-cycle.html#integrated-arma-or-arimapdq"><i class="fa fa-check"></i><b>6.6</b> Integrated ARMA or ARIMA(p,d,q)</a></li>
<li class="chapter" data-level="6.7" data-path="modeling-cycle.html"><a href="modeling-cycle.html#trend-stationary-vs-difference-stationary-time-series"><i class="fa fa-check"></i><b>6.7</b> Trend Stationary vs Difference Stationary Time Series</a></li>
<li class="chapter" data-level="6.8" data-path="modeling-cycle.html"><a href="modeling-cycle.html#testing-for-a-unit-root"><i class="fa fa-check"></i><b>6.8</b> Testing for a unit root</a><ul>
<li class="chapter" data-level="6.8.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#testing-for-unit-root-in-usdcad-exchange-rate"><i class="fa fa-check"></i><b>6.8.1</b> Testing for unit root in USD/CAD exchange rate</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="modeling-cycle.html"><a href="modeling-cycle.html#box-jenkins-method-for-estimating-arimapdq"><i class="fa fa-check"></i><b>6.9</b> Box-Jenkins Method for estimating ARIMA(p,d,q)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modeling-volatility.html"><a href="modeling-volatility.html"><i class="fa fa-check"></i><b>7</b> Modeling Volatility</a><ul>
<li class="chapter" data-level="7.1" data-path="modeling-volatility.html"><a href="modeling-volatility.html#some-stylized-facts-about-stock-market-volatility"><i class="fa fa-check"></i><b>7.1</b> Some stylized facts about stock market volatility</a></li>
<li class="chapter" data-level="7.2" data-path="modeling-volatility.html"><a href="modeling-volatility.html#archq-autoregressive-conditional-heteroscedasticiy-of-order-q"><i class="fa fa-check"></i><b>7.2</b> ARCH(q): Autoregressive Conditional Heteroscedasticiy of order <span class="math inline">\(q\)</span></a></li>
<li class="chapter" data-level="7.3" data-path="modeling-volatility.html"><a href="modeling-volatility.html#garchpq-generalized-autoregressive-conditional-heteroscedasicity-of-order-p-and-q"><i class="fa fa-check"></i><b>7.3</b> GARCH(p,q): Generalized Autoregressive Conditional Heteroscedasicity of order <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span></a></li>
<li class="chapter" data-level="7.4" data-path="modeling-volatility.html"><a href="modeling-volatility.html#extensions-of-standard-garch-model"><i class="fa fa-check"></i><b>7.4</b> Extensions of standard GARCH model</a><ul>
<li class="chapter" data-level="7.4.1" data-path="modeling-volatility.html"><a href="modeling-volatility.html#gjr-garch11"><i class="fa fa-check"></i><b>7.4.1</b> GJR-GARCH(1,1)</a></li>
<li class="chapter" data-level="7.4.2" data-path="modeling-volatility.html"><a href="modeling-volatility.html#exponential-garch-or-egarch11"><i class="fa fa-check"></i><b>7.4.2</b> Exponential GARCH or EGARCH(1,1)</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="modeling-volatility.html"><a href="modeling-volatility.html#application-of-garch-model-estimating-volatility-of-sp500-return"><i class="fa fa-check"></i><b>7.5</b> Application of GARCH model: Estimating volatility of SP500 return</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://vipul-bhatt.github.io/Econ-483-Notes/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Time Series Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling-cycle" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Modeling Cycle</h1>
<p>In this chapter we will focus on the cyclical component of a time series and hence focus on data that either has no trend and seasonal components, or data that is filtered to eliminate any trend and seasonality. One of the most commonly used method to model cyclicality is the <em>Autogressive Moving Average (ARMA)</em>. This model has two distinct components:</p>
<ol style="list-style-type: decimal">
<li><p><em>Autoregressive (AR) component</em>: the current period value of a time series variable depends on its past (lagged) observations. We use <span class="math inline">\(p\)</span> to denote the <strong>order</strong> of the AR component and is the number of lags of a variable that directly affect the current period value. For example, a firm’s production in the current period maybe impacted by past levels of production. If last year’s production exceeded demand, the stock of unsold goods may be used to meet this period demand first, hence lowering the current period production.</p></li>
<li><p><em>Moving average (MA) component</em>: the current period value of a time series variable depends on current period <strong>shock</strong> as well as past shocks to this variable. We use <span class="math inline">\(q\)</span> to denote the <strong>order</strong> of the MA component and is the number of past period shocks that affect the current period value of the variable of interest. For example, if the Federal Reserve Bank raises the interest in 2016, the effects of that policy shock may impact investment and consumption spending in 2017.</p></li>
</ol>
<p>Before we consider these time series model in details it is useful to discuss certain properties of time series that allow us a better understanding of these models.</p>
<div id="stationarity-and-autocorrelation" class="section level2">
<h2><span class="header-section-number">6.1</span> Stationarity and Autocorrelation</h2>
<div id="covariance-stationary-time-series" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Covariance Stationary Time Series</h3>

<div class="definition">
<span id="def:d7" class="definition"><strong>Definition 6.1  (Covariance Stationary Time Series)  </strong></span>
</div>

<p>A time series <span class="math inline">\(\{y_t\}\)</span> is said to be a <em>covariance stationary process</em> if:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E(y_t)=\mu_y \quad \forall \quad t\)</span></li>
<li><span class="math inline">\(Var(y_t)=\sigma_y^2 \quad \forall \quad t\)</span></li>
<li><span class="math inline">\(Cov(y_t,y_{t-s})=\gamma(s) \quad \forall \quad s\neq t\)</span></li>
</ol>
<p>One way to think about stationarity is <em>mean-reversion</em>, i.e, the tendency of a time series to return to its <em>long-run</em> unconditional mean following a shock (or a series of shock). Figure @(fig:ch6-figure1) below shows this property graphically.</p>
<div class="figure" style="text-align: center"><span id="fig:ch6-figure1"></span>
<img src="bookdown-demo_files/figure-html/ch6-figure1-1.png" alt="Reversion to mean" width="80%" />
<p class="caption">
Figure 6.1: Reversion to mean
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:ch6-figure2"></span>
<img src="bookdown-demo_files/figure-html/ch6-figure2-1.png" alt="Reversion to mean in practice" width="80%" />
<p class="caption">
Figure 6.2: Reversion to mean in practice
</p>
</div>
<p>In practice however, you will not be able to visualize a mean-reverting stationary process this clearly. For example, in Figure <a href="modeling-cycle.html#fig:ch6-figure2">6.2</a> we plot real GDP growth for the U.S. which is a stationary process with a mean of 0.7%. In this chapter we will only consider stationary time series data. Later on we will learn how to work with non-stationary data.</p>
<!-- Note: -->
<!-- 1. By definition a time series with trend is non-stationary. For example, consider the following linear trend model: -->
<!-- \[ y_t = \beta_0 + \beta_1 t + \epsilon_t \quad where \ \epsilon_t \sim WN(0,\sigma^2_\epsilon) \] -->
<!-- Here it is easy to show that the unconditional mean of this model changes over time and hence is not constant. If after removing the trend (i.e., the residual from the above model) we get stationarity, then such data is called *trend stationary*. -->
<!-- 2. Similarly, a model with seasonality will be non-stationary. -->
<!--  -->
</div>
<div id="correlation-vs-autocorrelation" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Correlation vs Autocorrelation</h3>
<p>In statistics, correlation is a measure of relationship between two variables. In the time series setting, we can think of the current period value and the past period value of a variable as two <strong>separate</strong> variables, and compute correlation between them. Such a correlation, between current and lagged observation of a time series is called <strong>serial correlation</strong> or <strong>autocorrelation</strong>. In general, for a time series, <span class="math inline">\(\{y_t\}\)</span>, the autocorrelation is given by:</p>
<p><span class="math display">\[\begin{align}
    Cor(y_t,y_{t-s})=\frac{ Cov(y_t,y_{t-s})}{\sqrt{\sigma^2_{y_t} \times \sigma^2_{y_{t-s}}}}
        \end{align}\]</span>
where <span class="math inline">\(Cov(y_t,y_{t-s})= E(y_t-\mu_{y_t})(y_{t-s}-\mu_{y_{t-s}})\)</span> and <span class="math inline">\(\sigma^2_{y_t}=E(y_t-\mu_{y_t})^2\)</span></p>
<p>For a stationary time series, using the three conditions the <strong>Autocorrelation Function (ACF)</strong> denoted by <span class="math inline">\(\rho(s)\)</span> is given by:</p>

<p>Non-zero values of the ACF indicates presences of serial correlation in the data. Figure <a href="modeling-cycle.html#fig:ch6-figure3">6.3</a> shows the ACF for a stationary time series with positive serial correlation. If your data is stationary then the ACF should eventually converge to 0. For a non-stationary data, the ACF function will not decay over time.</p>
<div class="figure" style="text-align: center"><span id="fig:ch6-figure3"></span>
<img src="bookdown-demo_files/figure-html/ch6-figure3-1.png" alt="ACF for a Stationary Time Series" width="80%" />
<p class="caption">
Figure 6.3: ACF for a Stationary Time Series
</p>
</div>
</div>
<div id="partial-autocorrelation" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Partial Autocorrelation</h3>

<div class="definition">
<span id="def:d9" class="definition"><strong>Definition 6.2  (Partial Auto Correlation Function (PACF))  </strong></span>
</div>

<p>The ACF captures the relationship between the current period value of a time series and all of its past observations. It includes both direct as well as indirect effects of the past observations on the current period value. Often times it is of interest to measure the direct relationship between the current and past observations, <strong>partialing</strong> out all indirect effects. The <em>partial autocorrelation function (PACF)</em> for a stationary time series <span class="math inline">\(y_t\)</span> at lag <span class="math inline">\(s\)</span> is the direct correlation between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-s}\)</span>, after filtering out the linear influence of <span class="math inline">\(y_{t-1},\ldots,y_{t-s-1}\)</span> on <span class="math inline">\(y_t\)</span>. Figure <a href="modeling-cycle.html#fig:ch6-figure4">6.4</a> below shows the PACF for a stationary time series where only one lag directly affects the time series in the current period.</p>
<div class="figure" style="text-align: center"><span id="fig:ch6-figure4"></span>
<img src="bookdown-demo_files/figure-html/ch6-figure4-1.png" alt="PACF for a Stationary Time Series" width="80%" />
<p class="caption">
Figure 6.4: PACF for a Stationary Time Series
</p>
</div>
</div>
<div id="lag-operator" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Lag operator</h3>
<p>A <strong>lag operator</strong> denoted by <span class="math inline">\(L\)</span> allows us to write ARMA models in a more concise way. Applying lag operator once moves the time index by one period; applying it twice moves the time index back by two period; applying it <span class="math inline">\(s\)</span> times moves the index back by <span class="math inline">\(s\)</span> periods.
<span class="math display">\[ Ly_t=y_{t-1} \]</span>
<span class="math display">\[ L^2y_t=y_{t-2} \]</span>
<span class="math display">\[ L^3y_t=y_{t-3} \]</span>
<span class="math display">\[\vdots\]</span>
<span class="math display">\[ L^sy_t=y_{t-s} \]</span></p>
</div>
</div>
<div id="autoregressive-ar-model" class="section level2">
<h2><span class="header-section-number">6.2</span> Autoregressive (AR) Model</h2>
<p>A <em>stationary</em>time series <span class="math inline">\(\{x_t\}\)</span> can be modeled as an AR process. In general, an AR(p) model is given by:</p>

<p>Here <span class="math inline">\(\phi_i\)</span> captures the effect of <span class="math inline">\(y_{t-i}\)</span> on <span class="math inline">\(y_t\)</span>. The order of the AR process is not known apriori. It is common to use either AIC or BIC to determine the optimal lag length for an AR process.</p>
<p>Using the Lag operator, we can rewrite the above AR(p) model as follows:
<span class="math display">\[ \Phi(L)y_t=\phi_0+\epsilon_t \]</span></p>
<p>where <span class="math inline">\(\displaystyle \Phi(L)\)</span> is a polynomial of degree <span class="math inline">\(p\)</span> in L:</p>
<p><span class="math display">\[ \Phi(L) = 1-\phi_1 L - \phi_2 L^2- \ldots\ldots\ldots\ldots -\phi_p L^p\]</span></p>
<p>For example, an AR(1) model can be written as:
<span class="math display">\[y_t=\phi_0+\phi_1 y_{t-1} + \epsilon_t \Rightarrow  \Phi(L)y_t=\phi_0+\epsilon_t\]</span>
where,
<span class="math display">\[ \Phi(L) = 1-\phi_1 L \]</span></p>
<p><strong>Characteristic equation</strong>: A characteristic equation is given by:</p>
<p><span class="math display">\[\Phi(L)=0\]</span></p>
<p>The roots of this equation play an important role in determining the dynamic behavior of a time series.</p>
<div id="unit-root-and-stationarity" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Unit root and Stationarity</h3>
<p>For a time series to be stationary there should be no <strong>unit root</strong> in its <em>characteristic equation</em>. In other words, all roots of the characteristic equation must fall outside the unit circle. Consider the following AR(1) model:
<span class="math display">\[\Phi(L)y_t = \phi_0 + \epsilon_t\]</span></p>
<p>The characteristic equation is given by:
<span class="math display">\[\Phi(L)=1-\phi_1L=0 \]</span></p>
<p>The root that satisfies the above equation is:
<span class="math display">\[ L^*=\frac{1}{\phi_1}\]</span></p>
<p>For no unit root to be present, <span class="math inline">\(L^*&gt;|1|\)</span> which implies that <span class="math inline">\(|\phi_1|&lt;1\)</span>.</p>
<p>Typically, for any AR process to be stationary, some restrictions will be imposed on the values of <span class="math inline">\(\phi_i&#39;s\)</span>, the coefficients of the lagged variables in the model.</p>
</div>
<div id="properties-of-an-ar1-model" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Properties of an AR(1) model</h3>
<p>A stationary AR(1) model is given by:
<span class="math display">\[ y_t=\phi_0 +\phi_1 y_{t-1}+ \epsilon_t \quad ; \ \epsilon_t\sim WN(0, \sigma_\epsilon^2) \ and \  |\phi_1|&lt;1\]</span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\displaystyle \phi_1\)</span> measures the persistence in data. A larger value indicates shocks to <span class="math inline">\(y_t\)</span> dissipate slowly over time.</p></li>
<li><p>Stationarity of <span class="math inline">\(y_t\)</span> implies certain restrictions on the AR(1) model.</p>
<ol style="list-style-type: lower-roman">
<li>Constant long run mean: is the unconditional expectation of <span class="math inline">\(y_t\)</span>:
<span class="math display">\[ E(y_t) = \mu_y= \frac{\phi_0}{1-\phi_1}  \]</span></li>
<li>Constant long run variance: is the unconditional variance of <span class="math inline">\(y_t\)</span>:
<span class="math display">\[ Var(y_t)=\sigma^2_y= \frac{\sigma^2_\epsilon}{1-\phi_1^2}\]</span></li>
<li>ACF function:
<span class="math display">\[ \rho(s) = \phi_1^s\]</span></li>
<li>PACF function:
<span class="math display">\[\begin{equation*}
  PACF(s) =
  \begin{cases}
\phi_1 &amp; \text{if  s=1}\\
0 &amp; \text{if s&gt;1}
  \end{cases}
 \end{equation*}\]</span></li>
</ol></li>
</ol>
</div>
</div>
<div id="estimating-an-ar-model" class="section level2">
<h2><span class="header-section-number">6.3</span> Estimating an AR model</h2>
<p>When estimating the AR model we have two alternatives:</p>
<ol style="list-style-type: decimal">
<li><p>OLS: biased (but consistent) estimates. Also, later on when we add MA components we cannot use OLS.</p></li>
<li><p>Maximum Likelihood Estimation (MLE): can be used to estimate AR as well as MA components</p></li>
</ol>
<div id="maximum-likelihood-estimation-mle" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Maximum Likelihood Estimation (MLE)</h3>
<ul>
<li>MLE approach is based on the following idea:</li>
</ul>
<p><em>what set of values of our parameters maximize the likelihood of observing our data if the model we have was used to generate this data.</em></p>
<p><strong>Likelihood function</strong>: is a function that gives us the probability of observing our data given a model with some parameters.</p>
<div id="likelihood-vs-probability" class="section level4">
<h4><span class="header-section-number">6.3.1.1</span> Likelihood vs Probability</h4>
<p>Consider a simple example of tossing a coin. Let <span class="math inline">\(X\)</span> denotes the random variable that is the outcome of this experiment being either heads or tails. Let <span class="math inline">\(\theta\)</span> denote the probability of heads which implies <span class="math inline">\(1-\theta\)</span> is the probability of obtaining tails. Here, <span class="math inline">\(\theta\)</span> is our parameter of interest. Suppose we toss the coin 10 times and obtain the following data on <span class="math inline">\(X\)</span>:
<span class="math display">\[X=\{H,H,H,H,H,H,T,T,T,T\}\]</span></p>
<p>Then, the probability of obtaining this sequence of X is given by:
<span class="math display">\[Prob (X|\theta)=\theta^6 (1-\theta)^4\]</span></p>
<p>This is the probability distribution function the variable <span class="math inline">\(X\)</span>. As we change <span class="math inline">\(X\)</span>, we get a different probability for a given value of <span class="math inline">\(\theta\)</span>.</p>
<p>Now let us ask a different question. Once we have observed the sequence of heads and tails, lets call it our data which is fixed. Then, what is probability of observing this data, if our probability distribution function is given by the equation above? That gives us the likelihood function:</p>
<p><span class="math display">\[ L(\theta)=Prob(X|\theta)=\theta^6(1-\theta)^4\]</span></p>
<p>Note that with fixed <span class="math inline">\(X\)</span>, as we change <span class="math inline">\(\theta\)</span> the likelihood of observing this data will change.</p>
<p><strong>This is an important point that distinguishes likelihood function from the probability distribution function. Although both have the same equation, the probability function is a function of the data with the value of the parameter fixed, while the likelihood function is a function of the parameter with the data fixed.</strong></p>
</div>
<div id="maximum-likelhood-estimation" class="section level4">
<h4><span class="header-section-number">6.3.1.2</span> Maximum Likelhood Estimation</h4>
<p>Now we are in a position to formally define the likelihood function.</p>

<div class="definition">
<span id="def:unnamed-chunk-4" class="definition"><strong>Definition 6.3  </strong></span>Let <span class="math inline">\(X\)</span> denotes a random variable with a given probability distribution function denoted by <span class="math inline">\(f(x_i|\theta)\)</span>. Let <span class="math inline">\(D=\{x_1, x_2,\dots,x_n\}\)</span> denote a sample realization of <span class="math inline">\(X\)</span>. Then, the likelhood function, denoted by <span class="math inline">\(L(\theta)\)</span> is given by:
<span class="math display">\[L(\theta)=f(x_1,x_2,\dots,x_n|\theta)\]</span>
</div>

<p>If we further assume that each realization of <span class="math inline">\(X\)</span> is independent of the others, we get:
<span class="math display">\[L(\theta)=f(x_1,x_2,\dots,x_n|\theta)=f(x_1|\theta)\times f(x_2|\theta) \times \dots \times f(x_n|\theta)\]</span></p>
<p>A mathematical simplification is to work with natural logs of the likelihood function, which assuming independently distributed random sample, gives us:</p>
<p><span class="math display">\[ lnL(\theta)=ln(f(x_1|\theta)\times f(x_2|\theta) \times \dots \times f(x_n|\theta))=\sum_{i=1}^{N}ln(f(x_i|\theta))\]</span></p>

<div class="definition">
<span id="def:unnamed-chunk-5" class="definition"><strong>Definition 6.4  </strong></span>The maximum likelihood estimator, denoted by <span class="math inline">\(\hat{\theta}_{MLE}\)</span>, maximizes the log likelihood function:
<span class="math display">\[ \hat{\theta}_{MLE} \equiv arg \max_{\theta} lnL(\theta) \]</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-6" class="example"><strong>Example 6.1  </strong></span>Compute maximum likelihood estimator of <span class="math inline">\(\mu\)</span> of an indpendently distributed random variable that is normally distributed with a mean of <span class="math inline">\(\mu\)</span> and a variance of <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[ f(y_t|\mu)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (y_t-\mu)^2}\]</span></p>
<p>Solution: The log likelihood function is given by:</p>
<p><span class="math display">\[lnL= -Tln2\pi-\frac{1}{2}\sum_{t=1}^T(y_t-\mu)^2 \]</span></p>
From the first order condition, we get
<span class="math display">\[ \frac{\partial LnL}{\partial \mu}=\sum_{t=1}^T(y_t-\mu)=0\Rightarrow \hat{\mu}_{MLE}=\frac{\sum_{t=1}^T y_t}{T}\]</span>
</div>

</div>
</div>
<div id="mle-of-an-arp-model" class="section level3">
<h3><span class="header-section-number">6.3.2</span> MLE of an AR(p) model</h3>
<p>One complication we face in estimating an AR(p) model is that by definition the realizations of the variable are not independent of each other. As a result we cannot simplify the likelihood function by multiplying individual probability density functions to obtain the joint probability density function, i.e.,
<span class="math display">\[ f(y_1,y_2,\dots,y_T|\theta) \neq f(y_1|\theta)\times f(y_2|\theta)\times \dots \times f(y_T|\theta)\]</span></p>
<p>Furthermore, as the order of AR increases, the joint density function we need to estimate becomes even more complicated. In this class we will focus on the method that divides the joint density into the product of conditional densities and density of a set of initial values. The idea comes from the conditional probability formula for two related events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>:</p>
<p><span class="math display">\[ P(A|B) =\frac{P(\text{A and B})}{P(B)} \Rightarrow P(\text{A and B}) = P(A|B)\times P(B) \]</span></p>
<p>In the time series context, I will explain this for a stationary AR(1) model. We know that in this model only last period observation directly affects the current period value. Hence, consider the first two observations of a stationary time series: <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>. Then the joint density of these adjacent observations is given by,</p>
<p><span class="math display">\[ f(y_1,y_2;\theta)= f(y_2|y_1; \theta)\times f(y_1;\theta)\]</span></p>
<p>Similarly, for the first three observations we get:</p>
<p><span class="math display">\[ f(y_1,y_2,y_3;\theta)= f(y_3|y_2; \theta)\times f(y_2|y_1; \theta) \times f(y_1; \theta)\]</span></p>
<p>Hence, for <span class="math inline">\(T\)</span> observations we get:</p>
<p><span class="math display">\[ f(y_1,y_2,y_3, ...,y_T; \theta)= f(y_T|y_{T-1};\theta)\times f(y_{T-1}|y_{T-2}; \theta)\times.... \times f(y_1; \theta)\]</span></p>
<p>The log-likelihood function is given by:</p>
<p><span class="math display">\[ ln \ L(\theta) = ln \ f(y_1;\theta) + \sum_{t=2}^{T} ln \ f(y_t|y_{t-1}; \theta)  \]</span></p>
<p>We can then maximize the above likelihood function to obtain an MLE estimator for the AR(1) model.</p>
</div>
<div id="selection-of-optimal-order-of-the-ar-model" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Selection of optimal order of the AR model</h3>
<p>Note that apriori we do not know the order of the AR model for any given time series. We can determine the optimal lag order by using either AIC or BIC. The process is as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Set <span class="math inline">\(p=p_{max}\)</span> where <span class="math inline">\(p_{max}\)</span> is an integer. A rule of thumb is to set
<span class="math display">\[p_{max}=integer\left[12\times \left(\frac{T}{100}\right)^{0.25}\right]\]</span></p></li>
<li><p>Estimate all AR models from <span class="math inline">\(p=1\)</span> to <span class="math inline">\(p=p_{max}\)</span>.</p></li>
<li><p>Select the final model as the one with lowest AIC or lowest BIC.</p></li>
</ol>
</div>
<div id="forecasting-using-arp-model" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Forecasting using AR(p) model</h3>
<p>Having estimated our AR(p) model with the optimal lag length, we can use the conditional mean to compute the forecast and conditional variance to compute the forecast errors. Consider an AR(1) model:</p>
<p><span class="math display">\[y_t=\phi_0+\phi_1 y_{t-1} +\epsilon_t\]</span></p>
<p>Then, the 1-period ahead forecast is given by:
<span class="math display">\[f_{t,1}=E(y_{t+1}|\Omega_t)=\phi_0+\phi_1 y_t\]</span>
Similarly, the 2-period ahead forecast is given by:
<span class="math display">\[f_{t,2}=E(y_{t+2}|\Omega_t)=\phi_0+\phi_1 E(y_{t+1}|\Omega_t) =\phi_0+\phi_1f_{t,1}\]</span></p>
<p>In general, we can get the following recursive forecast equation for h-period’s ahead:
<span class="math display">\[f_{t,h}=\phi_0+\phi_1 f_{t,h-1}\]</span></p>
<p>Correspondingly, the h-period ahead forecast error is given by:
<span class="math display">\[e_{t,h}=y_{t+h}- f_{t,h}=\epsilon_{t+h}+\phi_1 e_{t,h-1}\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-7" class="theorem"><strong>Theorem 6.1  </strong></span>The h-period ahead forecast converges to the unconditional mean of <span class="math inline">\(y_t\)</span>, i.e., <span class="math display">\[\lim_{h\to\infty} f_{t,h}=\mu_y=\frac{\phi_0}{1-\phi_1}\]</span>
</div>


<div class="theorem">
<span id="thm:unnamed-chunk-8" class="theorem"><strong>Theorem 6.2  </strong></span>The variance of the h-period ahead forecast error converges to the unconditional variance of <span class="math inline">\(y_t\)</span>, i.e., <span class="math display">\[\lim_{h\to\infty} Var(e_{t,h})=\sigma^2_y=\frac{\sigma^2_\epsilon}{1-\phi_1^2}\]</span>
</div>

</div>
</div>
<div id="moving-average-ma-model" class="section level2">
<h2><span class="header-section-number">6.4</span> Moving Average (MA) Model</h2>
<p>Another commonly used method for capturing the cyclical component of the time series is the <strong>moving average (MA)</strong> model where the current value of a time series linearly depends on current and past shocks. Formally, a <em>stationary</em> time series <span class="math inline">\(\{y_t\}\)</span> can be modeled as an MA(q) process:
<span class="math display">\[\begin{equation}
  y_t = \theta_0 + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ...... + \theta_q \epsilon_{t-q}
    \end{equation}\]</span></p>
<p>Using lag operator, we can write this in more compact form as:</p>
<p><span class="math display">\[y_t = \theta_0 +\Theta(L) \epsilon_t\]</span></p>
<p>where <span class="math inline">\(\Theta(L)=1+\theta_1 L+ \theta_2 L^2+...+\theta_q L^q\)</span> is lag polynomial of order <span class="math inline">\(q\)</span>.</p>
<p>Note that because each one of the current and past shocks are white noise processes, an MA(q) model is always stationary.</p>
<div id="invertibility-of-an-ma-process" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Invertibility of an MA process</h3>
<p>Consider the following MA(1) process with<span class="math inline">\(\theta_0=0\)</span> for simplicity:
<span class="math display">\[y_t=\epsilon_t +\theta_1 \epsilon_{t-1}\]</span></p>
<p>Using the lag operator we can rewrite this equation as follows:</p>
<p><span class="math display">\[y_t= (1+\theta_1L)\epsilon_t \Rightarrow y_t(1+\theta_1 L) ^{-1}=\epsilon_t\]</span></p>
<p>Note that if <span class="math inline">\(|\theta_1|&lt;1\)</span>, then we can use the Taylor series expansion centered at 0 and get:</p>
<p><span class="math display">\[(1+\theta_1 L)^{-1}=1-\theta_1 L+(\theta_1L)^2-(\theta_1L)^3+ (\theta_1L)^4-...... \]</span></p>
<p>Hence, an MA(1) can be rewritten as follows:</p>
<p><span class="math display">\[y_t (1-\theta_1 L+(\theta_1L)^2-(\theta_1L)^3+ (\theta_1L)^4-......)=\epsilon_t\]</span>
<span class="math display">\[\Rightarrow y_t -\theta_1 y_{t-1} +\theta_1^2y_{t-2}-\theta_1^3 y_{t-3}....=\epsilon_t\]</span></p>
<p>Rearranging terms, we get the <span class="math inline">\(AR(\infty)\)</span> representation for an invertible MA(1) model:
<span class="math display">\[y_t=-\sum_{i=1}^{\infty}(-\theta_1)^i \ y_{t-i}+\epsilon_t\]</span></p>

<div class="definition">
<span id="def:unnamed-chunk-9" class="definition"><strong>Definition 6.5  </strong></span>An MA process is invertible if it can be represented as a stationary <span class="math inline">\(AR(\infty)\)</span>.
</div>

</div>
<div id="properties-of-an-invetible-ma1" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Properties of an invetible MA(1)</h3>
<p>An invertible MA(1) model is given by:</p>
<p><span class="math display">\[ y_t = \theta_0 + \epsilon_t + \theta_1 \epsilon_{t-1} \quad ; \ \epsilon_t\sim WN(0, \sigma_\epsilon^2) \ and \  |\theta_1|&lt;1\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Constant unconditional mean of <span class="math inline">\(y_t\)</span>:
<span class="math display">\[E(y_t)=\mu_y =\theta_0 \]</span></p></li>
<li><p>Constant unconditional variance of <span class="math inline">\(y_t\)</span>:</p></li>
</ol>
<p><span class="math display">\[Var(y_t)=\sigma^2_y=\sigma^2_\epsilon(1+\theta_1^2)\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p>ACF function:
<span class="math display">\[\begin{equation*}
  ACF(s) =
  \begin{cases}
 \frac{\theta_1}{1+\theta_1^2} &amp; \text{if  s=1}\\
 0 &amp; \text{if s&gt;1}
  \end{cases}
 \end{equation*}\]</span></p></li>
<li><p>PACF function: using the invertibility it is evident that PACF of an MA(1) decays with <span class="math inline">\(s\)</span>.</p></li>
</ol>
</div>
<div id="forecast-based-on-maq" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Forecast based on MA(q)</h3>
<p>Like before, the h-period ahead forecast is the conditional expected value of the time series. Consider an MA(1) model:</p>
<p><span class="math display">\[y_t=\theta_0 +\epsilon_t + \theta_1 \epsilon_{t-1}\]</span></p>
<p>Then, the 1-period ahead forecast is given by:</p>
<p>The h-period ahead forecast for <span class="math inline">\(h&gt;1\)</span> is given by:
<span class="math display">\[f_{t,h}=E(y_{t+h}|\Omega_t)=\theta_0\]</span></p>
<p>In general, for an MA(q) model, the forecast for <span class="math inline">\(h&gt;q\)</span> is the long run mean <span class="math inline">\(\theta_0\)</span>. This is why we say that an MA(q) process has a memory of <em>q</em> periods.</p>
</div>
</div>
<div id="armap-q" class="section level2">
<h2><span class="header-section-number">6.5</span> ARMA(p, q)</h2>
<p>An ARMA model simply combines both AR and MA components to model the dynamics of a time series. Formula,</p>

<p>Note that:</p>
<ol style="list-style-type: decimal">
<li><p>Estimation is done by maximum likelihood method.</p></li>
<li><p>Optimal order for AR and MA components is selected using AIC and/or BIC.</p></li>
<li><p>The forecast of <span class="math inline">\(y_t\)</span> from an ARMA(p,q) model will be dominated by the AR component for <span class="math inline">\(h&gt;q\)</span>. To see this consider the following ARMA(1,1) model:</p></li>
</ol>
<p><span class="math display">\[y_t = \phi_0 +\phi_1 y_{t-1}+ \epsilon_t + \theta_1 \epsilon_{t-1}\]</span></p>
<p>Then, the 1-period ahead forecast is:
<span class="math display">\[f_{t,1} = E(y_{t+1}|\Omega_t) = \phi_0 + \phi_1 y_t + \theta_1 \epsilon_{t-1}\]</span></p>
<p>Here both MA and AR component affect the forecast. But now consider the 2-period ahead forecast:</p>
<p><span class="math display">\[f_{t,2} = E(y_{t+2}|\Omega_t) = \phi_0 + \phi_1 f_{t,1}\]</span></p>
<p>Hence, no role is played by the MA component in determining the 2-period ahead forecast. For any <span class="math inline">\(h&gt;1\)</span> only the AR component affects the forecast from this model.</p>
</div>
<div id="integrated-arma-or-arimapdq" class="section level2">
<h2><span class="header-section-number">6.6</span> Integrated ARMA or ARIMA(p,d,q)</h2>
<p>Thus far we have assumed that our data is stationary. However, often we may find that this assumption is not supported in practice. In such a case we need to tranform our data appropriately before estimating an ARMA model. The procedure can be summarized as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Determine whether there is a unit root in data or not. Presence of unit root indicates non-stationarity. We will use Augmented Dickey-Fuller (ADF) test for this purpose.</p></li>
<li><p>If data is non-stationary, then we need to appropriately transform our data to make it stationary.</p></li>
<li><p>Once we have obtained a stationary transformation of our original data, we can proceed and estimate the ARMA model as before.</p></li>
</ol>
</div>
<div id="trend-stationary-vs-difference-stationary-time-series" class="section level2">
<h2><span class="header-section-number">6.7</span> Trend Stationary vs Difference Stationary Time Series</h2>
<p>There are two types of time series we often encounter in real world:</p>
<ol style="list-style-type: decimal">
<li>Trend-stationary: a time-series variable is non-stationary becuase it has a deterministic trend. Once we detrend our data then it will become stationary. In this case the appropriate transformation is to estimate a trend model and then use the residual as the detrended stationary data. For example, suppose our data has a linear trend given by:</li>
</ol>
<p><span class="math display">\[y_t = \beta_0 +\beta_1 t +\epsilon_t\]</span></p>
<p>Then the OLS residual from this model, <span class="math inline">\(e_t=y_t-\hat{y_t}\)</span> is the detrended <span class="math inline">\(y_t\)</span> which will be stationary. Hence, we will estimate an ARMA(p,q) model using this detrended variable.</p>
<ol start="2" style="list-style-type: decimal">
<li>Difference-stationary: a time-series variable is non-stationary because it contains a stochastic trend. Here, the transformation requires us to difference the orignial data until we obtain a stationary time series. Let <span class="math inline">\(d\)</span> denote the minimum number of differences needed to obtain a stationary time series:</li>
</ol>
<p><span class="math display">\[\Delta_d \ y_t=(1-L)^d \ y_t\]</span></p>
<p>In this case, we say that <span class="math inline">\(y_t\)</span> is intergrated of order <span class="math inline">\(d\)</span> or more formally, <span class="math inline">\(y_t\)</span> is an <span class="math inline">\(I(d)\)</span> process. Hence, for <span class="math inline">\(d=1\)</span> we obtain an <span class="math inline">\(I(1)\)</span> process implying that:</p>
<p><span class="math display">\[\Delta_1 \ y_t=(1-L)^1 \ y_t=y_t-y_{t-1} \quad  \text{is stationary}\]</span></p>
<p>In otherwords, the first difference of an I(1) process is stationary. Similarly for <span class="math inline">\(d=2\)</span>, we obtain an <span class="math inline">\(I(2)\)</span> process where second difference will be stationary and so forth.</p>
</div>
<div id="testing-for-a-unit-root" class="section level2">
<h2><span class="header-section-number">6.8</span> Testing for a unit root</h2>
<p>Consider the following AR(1) model with no trend and intercept:</p>
<p><span class="math display">\[y_t=\phi_1 y_{t-1} +\epsilon_t  \ quad ; \epsilon_t\sim WN(0,\sigma^2_\epsilon)\]</span></p>
<p>We know that if <span class="math inline">\(\phi_1=1\)</span> we have a unit root in this data. Lets subtract <span class="math inline">\(y_{t-1}\)</span> from both sides and rewrite this model as:</p>
<p><span class="math display">\[y_t-y_{t-1}= (\phi_1-1)y_{t-1}+\epsilon_t \]</span></p>
<p>Define <span class="math inline">\(\rho=phi_1-1\)</span>. Then, we get:
<span class="math display">\[\Delta y_t= \rho \ y_{t-1}+\epsilon_t \]</span></p>
<p>We can now estimate the above model and carry out the following test known as the Dickey-Fuller (DF) test:</p>
<p><span class="math display">\[H_0: \rho=0 \]</span>
<span class="math display">\[H_A: \rho&lt;0\]</span></p>
<p>If the null hypothesis is not rejected, then we do not have sample evidence against the statement that <span class="math inline">\(\rho=0 \Rightarrow \phi_1=1\)</span>. Hence, we conclude that there is no evidence against the statement that there is unit root in the data. In contrast, if we reject the null hypothesis, then we can conclude that there is no unit root and hence the data is stationary.</p>
<p>The t- statistic is for the above test is denoted by <span class="math inline">\(\tau_1\)</span> and is given by:
<span class="math display">\[\tau_1 =\frac{\hat{\rho}}{se(\hat{\rho})}\]</span></p>
<p>Under the null hypothesis this test statistic follows the DF-distribution and the critical values are provided in most statistical softwares. Given that this is a left-tail test, the decision rule is that if the test statistic is less than the critical value then we reject the null hypothesis.</p>
<p>There are two issues we face when implementing this test in practice:</p>
<ol style="list-style-type: decimal">
<li>First, the above procedure assumes that there is no intercept and trend in the data. In real world, we cannot make that assumption and must extend the test procedure to accomodate a non-zero intercept and trend. Hence, we have the following two additional versions of the DF test:
<ol style="list-style-type: lower-roman">
<li>Constant and no trend model: Here our AR(1) model is</li>
</ol>
<p><span class="math display">\[\Delta y_t= \phi_0 + \rho \ y_{t-1}+\epsilon_t \]</span></p>
<p>Now we can do two possible tests. The first test is that of the unit root:</p>
<p><span class="math display">\[H_0: \rho=0 \]</span>
<span class="math display">\[H_A: \rho&lt;0\]</span></p>
<p>The t statistic for this test is denoted by <span class="math inline">\(\tau_2\)</span> and is given by:
<span class="math display">\[\tau_2 =\frac{\hat{\rho}}{se(\hat{\rho})}\]</span></p>
<p>If the test statisitic is less than the critical value, we reject the null.</p>
<p>The second test we can do is:
<span class="math display">\[H_0: \rho=\phi_0=0 \]</span>
<span class="math display">\[H_A: Not \ H_0\]</span>
The test statistic for this test is denoted by <span class="math inline">\(\phi_1\)</span>. If the test statistic exceeds the critical value then we reject the null.</p>
<ol start="2" style="list-style-type: lower-roman">
<li>Constant and linear trend model: Here our AR(1) model is</li>
</ol>
<p><span class="math display">\[\Delta y_t= \phi_0 + \beta \ t+ \rho \ y_{t-1}+\epsilon_t \]</span></p>
<p>Now we can do three possible tests. The first test is that of the unit root;
<span class="math display">\[H_0: \rho=0 \]</span>
<span class="math display">\[H_A: \rho&lt;0\]</span>
The t statistic for this test is denoted by <span class="math inline">\(\tau_3\)</span> and is given by:</p>
<p><span class="math display">\[\tau_3 =\frac{\hat{\rho}}{se(\hat{\rho})}\]</span></p>
<p>If the test statisitic is less than the critical value, we reject the null.</p>
<p>The second test is:
<span class="math display">\[H_0: \rho=\phi_0=\beta=0 \]</span>
<span class="math display">\[H_A: Not \ H_0\]</span>
The test statistic for this test is denoted by <span class="math inline">\(\phi_2\)</span>. If the test statistic exceeds the critical value then we reject the null.</p>
<p>Finally the third test is:
<span class="math display">\[H_0: \rho=\beta=0 \]</span>
<span class="math display">\[H_A: Not \ H_0\]</span>
The test statistic for this test is denoted by <span class="math inline">\(\phi_3\)</span>. If the test statistic exceeds the critical value then we reject the null.</p></li>
<li><p>Second, we only have allowed for AR(1). We need to extend the above testing procedure for higher order AR models. The Augmented DF (ADF) test allows for higher order lags in testing for a unit root. For example, the model with an intercept, trend, and <span class="math inline">\(p\)</span> lags is given by:
<span class="math display">\[\Delta y_t= \phi_0 + \beta \ t+ \rho \ y_{t-1}+\sum_{i=2}^p\delta_i  y_{t-i}+\epsilon_t  \quad where \ \rho=\sum_{i=1}^p \phi_i-1\]</span></p></li>
</ol>
<div id="testing-for-unit-root-in-usdcad-exchange-rate" class="section level3">
<h3><span class="header-section-number">6.8.1</span> Testing for unit root in USD/CAD exchange rate</h3>
<p>In this application we will test for unit root in US-Canada exchange rate. For this purpose we work with monthly data from Jan 1971 through Oct 2018. Below I show the results for 3 models using the <strong>urca</strong> package in R.</p>
<pre><code>## 
## ############################################### 
## # Augmented Dickey-Fuller Test Unit Root Test # 
## ############################################### 
## 
## Test regression none 
## 
## 
## Call:
## lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.069547 -0.008871  0.000358  0.010441  0.124996 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## z.lag.1    0.0001959  0.0005796   0.338    0.736    
## z.diff.lag 0.2758988  0.0400346   6.892 1.45e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01717 on 577 degrees of freedom
## Multiple R-squared:  0.07658,    Adjusted R-squared:  0.07337 
## F-statistic: 23.92 on 2 and 577 DF,  p-value: 1.043e-10
## 
## 
## Value of test-statistic is: 0.3379 
## 
## Critical values for test statistics: 
##       1pct  5pct 10pct
## tau1 -2.58 -1.95 -1.62</code></pre>
<pre><code>## 
## ############################################### 
## # Augmented Dickey-Fuller Test Unit Root Test # 
## ############################################### 
## 
## Test regression drift 
## 
## 
## Call:
## lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.067687 -0.009468 -0.000592  0.010039  0.123436 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.010401   0.005319   1.955   0.0510 .  
## z.lag.1     -0.008173   0.004319  -1.892   0.0589 .  
## z.diff.lag   0.279076   0.039970   6.982 8.04e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01713 on 576 degrees of freedom
## Multiple R-squared:  0.08169,    Adjusted R-squared:  0.0785 
## F-statistic: 25.62 on 2 and 576 DF,  p-value: 2.192e-11
## 
## 
## Value of test-statistic is: -1.8924 1.9691 
## 
## Critical values for test statistics: 
##       1pct  5pct 10pct
## tau2 -3.43 -2.86 -2.57
## phi1  6.43  4.59  3.78</code></pre>
<pre><code>## 
## ############################################### 
## # Augmented Dickey-Fuller Test Unit Root Test # 
## ############################################### 
## 
## Test regression trend 
## 
## 
## Call:
## lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.067715 -0.009336 -0.000484  0.009933  0.123282 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.040e-02  5.324e-03   1.954   0.0512 .  
## z.lag.1     -8.356e-03  4.448e-03  -1.879   0.0608 .  
## tt           7.638e-07  4.386e-06   0.174   0.8618    
## z.diff.lag   2.793e-01  4.002e-02   6.978 8.25e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.01714 on 575 degrees of freedom
## Multiple R-squared:  0.08174,    Adjusted R-squared:  0.07695 
## F-statistic: 17.06 on 3 and 575 DF,  p-value: 1.257e-10
## 
## 
## Value of test-statistic is: -1.8787 1.3206 1.8027 
## 
## Critical values for test statistics: 
##       1pct  5pct 10pct
## tau3 -3.96 -3.41 -3.12
## phi2  6.09  4.68  4.03
## phi3  8.27  6.25  5.34</code></pre>
<p>For the first model with no constant and trend, the test statistic <span class="math inline">\(\tau_1= 0.2469\)</span> and the 5% critical value is -1.95. For the second model with a constant, the test statistics is <span class="math inline">\(\tau_2= -1.9315\)</span> and the 5% critical value is -2.86. Finally, for the third model with a trend, the test statistic <span class="math inline">\(\tau_3= -1.8839\)</span> and the 5% critical value is -3.41. In each, because the test statistic is greater than the critical value, we do not reject the null hypothesis and conclude there is unit root.</p>
</div>
</div>
<div id="box-jenkins-method-for-estimating-arimapdq" class="section level2">
<h2><span class="header-section-number">6.9</span> Box-Jenkins Method for estimating ARIMA(p,d,q)</h2>
<p>Box-Jenkins is a three-step procedure for finding the best fitting ARIMA(p,d,q) for a non-statinary time series.</p>
<ol style="list-style-type: decimal">
<li><p>Model identification: here we determine the order of integration <span class="math inline">\(d\)</span>, and the optimal number of AR and MA components, <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> respectively.</p>
<ol style="list-style-type: lower-roman">
<li>To determine <span class="math inline">\(d\)</span>, we conduct ADF test on successive differences of the original time series. The order of integration is the number times we difference our data to obtain stationarity.</li>
<li>This is followed by estimating ARMA model for different combinations of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>. The optimal structure is chosen using eithr AIC or BIC.</li>
</ol></li>
<li><p>Parameter estimation: we estimate the identified model from the previous step using ML estimation.</p></li>
<li><p>Model Evaluation: mostly showing that the residuals from the optimal model is a white noise process. We can do this by using the Breusch-Godfrey LM test of serial correlation for the residuals. If residuals from the final model are white noise then there should be no serial correlation.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modeling-trend-and-seasonal-components.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modeling-volatility.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
