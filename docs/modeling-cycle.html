<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Applied Time Series Analysis</title>
  <meta name="description" content="Lecture notes for Applied Time Series Analysis">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Applied Time Series Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes for Applied Time Series Analysis" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Applied Time Series Analysis" />
  
  <meta name="twitter:description" content="Lecture notes for Applied Time Series Analysis" />
  

<meta name="author" content="Vipul Bhatt">


<meta name="date" content="2018-10-31">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="modeling-trend-and-seasonal-components.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Time Series Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Forecasting</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#time-series"><i class="fa fa-check"></i><b>1.1</b> Time Series</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#serial-correlation"><i class="fa fa-check"></i><b>1.2</b> Serial Correlation</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#testing-for-serial-correlion"><i class="fa fa-check"></i><b>1.3</b> Testing for Serial Correlion</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#white-noise-process"><i class="fa fa-check"></i><b>1.4</b> White Noise Process</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#important-elements-of-forecasting"><i class="fa fa-check"></i><b>1.5</b> Important Elements of Forecasting</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#loss-function-and-optimal-forecast"><i class="fa fa-check"></i><b>1.6</b> Loss Function and Optimal Forecast</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html"><i class="fa fa-check"></i><b>2</b> Regression-based Forecasting</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#scenario-analysis-and-conditional-forecasts"><i class="fa fa-check"></i><b>2.1</b> Scenario Analysis and Conditional Forecasts</a></li>
<li class="chapter" data-level="2.2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#unconditional-forecasts"><i class="fa fa-check"></i><b>2.2</b> Unconditional Forecasts</a></li>
<li class="chapter" data-level="2.3" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#some-practical-issues"><i class="fa fa-check"></i><b>2.3</b> Some practical issues</a></li>
<li class="chapter" data-level="2.4" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#distributed-lag-regression-models"><i class="fa fa-check"></i><b>2.4</b> Distributed Lag Regression Models</a><ul>
<li class="chapter" data-level="2.4.1" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#dynamic-effect-of-x-on-y"><i class="fa fa-check"></i><b>2.4.1</b> Dynamic Effect of X on Y</a></li>
<li class="chapter" data-level="2.4.2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#model-selection-criterion"><i class="fa fa-check"></i><b>2.4.2</b> Model Selection Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#application-a-model-of-investment-expenditure"><i class="fa fa-check"></i><b>2.5</b> Application: A Model of Investment Expenditure</a><ul>
<li class="chapter" data-level="2.5.1" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#a-multiple-regression-model-of-invesment-expenditure"><i class="fa fa-check"></i><b>2.5.1</b> A Multiple Regression Model of Invesment Expenditure</a></li>
<li class="chapter" data-level="2.5.2" data-path="regression-based-forecasting.html"><a href="regression-based-forecasting.html#a-distributed-lag-model-of-investment-expenditure"><i class="fa fa-check"></i><b>2.5.2</b> A Distributed Lag Model of Investment Expenditure</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="components-of-a-time-series.html"><a href="components-of-a-time-series.html"><i class="fa fa-check"></i><b>3</b> Components of a Time Series</a><ul>
<li class="chapter" data-level="3.1" data-path="components-of-a-time-series.html"><a href="components-of-a-time-series.html#decomposing-a-time-series"><i class="fa fa-check"></i><b>3.1</b> Decomposing a time series</a></li>
<li class="chapter" data-level="3.2" data-path="components-of-a-time-series.html"><a href="components-of-a-time-series.html#uses-of-decomposition-of-a-time-series"><i class="fa fa-check"></i><b>3.2</b> Uses of Decomposition of a time series</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="smoothing-methods.html"><a href="smoothing-methods.html"><i class="fa fa-check"></i><b>4</b> Smoothing Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="smoothing-methods.html"><a href="smoothing-methods.html#moving-average-method"><i class="fa fa-check"></i><b>4.1</b> Moving Average Method</a></li>
<li class="chapter" data-level="4.2" data-path="smoothing-methods.html"><a href="smoothing-methods.html#simple-exponential-smoothing"><i class="fa fa-check"></i><b>4.2</b> Simple Exponential Smoothing</a></li>
<li class="chapter" data-level="4.3" data-path="smoothing-methods.html"><a href="smoothing-methods.html#holt-winters-smoothing"><i class="fa fa-check"></i><b>4.3</b> Holt-Winters Smoothing</a></li>
<li class="chapter" data-level="4.4" data-path="smoothing-methods.html"><a href="smoothing-methods.html#holt-winters-smoothing-with-seasonality"><i class="fa fa-check"></i><b>4.4</b> Holt-Winters Smoothing with Seasonality</a></li>
<li class="chapter" data-level="4.5" data-path="smoothing-methods.html"><a href="smoothing-methods.html#application"><i class="fa fa-check"></i><b>4.5</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html"><i class="fa fa-check"></i><b>5</b> Modeling Trend and Seasonal Components</a><ul>
<li class="chapter" data-level="5.1" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#trend-estimation"><i class="fa fa-check"></i><b>5.1</b> Trend Estimation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#parametrizing-a-deterministic-trend"><i class="fa fa-check"></i><b>5.1.1</b> Parametrizing a deterministic trend</a></li>
<li class="chapter" data-level="5.1.2" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#uses-of-the-deterministic-trend-model"><i class="fa fa-check"></i><b>5.1.2</b> Uses of the Deterministic Trend Model</a></li>
<li class="chapter" data-level="5.1.3" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#application-estimating-a-polynomial-trend-for-u.s.-real-gdp"><i class="fa fa-check"></i><b>5.1.3</b> Application: Estimating a polynomial trend for U.S. Real GDP</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#seasonal-model"><i class="fa fa-check"></i><b>5.2</b> Seasonal Model</a><ul>
<li class="chapter" data-level="5.2.1" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#regression-model-with-seasonal-dummy-variables"><i class="fa fa-check"></i><b>5.2.1</b> Regression Model with Seasonal Dummy Variables</a></li>
<li class="chapter" data-level="5.2.2" data-path="modeling-trend-and-seasonal-components.html"><a href="modeling-trend-and-seasonal-components.html#application-seasonal-model-of-housing-starts"><i class="fa fa-check"></i><b>5.2.2</b> Application: Seasonal Model of Housing Starts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modeling-cycle.html"><a href="modeling-cycle.html"><i class="fa fa-check"></i><b>6</b> Modeling Cycle</a><ul>
<li class="chapter" data-level="6.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#stationarity-and-autocorrelation"><i class="fa fa-check"></i><b>6.1</b> Stationarity and Autocorrelation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#covariance-stationary-time-series"><i class="fa fa-check"></i><b>6.1.1</b> Covariance Stationary Time Series</a></li>
<li class="chapter" data-level="6.1.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#correlation-vs-autocorrelation"><i class="fa fa-check"></i><b>6.1.2</b> Correlation vs Autocorrelation</a></li>
<li class="chapter" data-level="6.1.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#partial-autocorrelation"><i class="fa fa-check"></i><b>6.1.3</b> Partial Autocorrelation</a></li>
<li class="chapter" data-level="6.1.4" data-path="modeling-cycle.html"><a href="modeling-cycle.html#lag-operator"><i class="fa fa-check"></i><b>6.1.4</b> Lag operator</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#autoregressive-ar-model"><i class="fa fa-check"></i><b>6.2</b> Autoregressive (AR) Model</a><ul>
<li class="chapter" data-level="6.2.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#unit-root-and-stationarity"><i class="fa fa-check"></i><b>6.2.1</b> Unit root and Stationarity</a></li>
<li class="chapter" data-level="6.2.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#properties-of-an-ar1-model"><i class="fa fa-check"></i><b>6.2.2</b> Properties of an AR(1) model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#estimating-an-ar-model"><i class="fa fa-check"></i><b>6.3</b> Estimating an AR model</a><ul>
<li class="chapter" data-level="6.3.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>6.3.1</b> Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="6.3.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#mle-of-an-arp-model"><i class="fa fa-check"></i><b>6.3.2</b> MLE of an AR(p) model</a></li>
<li class="chapter" data-level="6.3.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#selection-of-optimal-order-of-the-ar-model"><i class="fa fa-check"></i><b>6.3.3</b> Selection of optimal order of the AR model</a></li>
<li class="chapter" data-level="6.3.4" data-path="modeling-cycle.html"><a href="modeling-cycle.html#forecasting-using-arp-model"><i class="fa fa-check"></i><b>6.3.4</b> Forecasting using AR(p) model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="modeling-cycle.html"><a href="modeling-cycle.html#moving-average-ma-model"><i class="fa fa-check"></i><b>6.4</b> Moving Average (MA) Model</a><ul>
<li class="chapter" data-level="6.4.1" data-path="modeling-cycle.html"><a href="modeling-cycle.html#invertibility-of-an-ma-process"><i class="fa fa-check"></i><b>6.4.1</b> Invertibility of an MA process</a></li>
<li class="chapter" data-level="6.4.2" data-path="modeling-cycle.html"><a href="modeling-cycle.html#properties-of-an-invetible-ma1"><i class="fa fa-check"></i><b>6.4.2</b> Properties of an invetible MA(1)</a></li>
<li class="chapter" data-level="6.4.3" data-path="modeling-cycle.html"><a href="modeling-cycle.html#forecast-based-on-maq"><i class="fa fa-check"></i><b>6.4.3</b> Forecast based on MA(q)</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="modeling-cycle.html"><a href="modeling-cycle.html#armap-q"><i class="fa fa-check"></i><b>6.5</b> ARMA(p, q)</a></li>
<li class="chapter" data-level="6.6" data-path="modeling-cycle.html"><a href="modeling-cycle.html#testing-for-unit-root"><i class="fa fa-check"></i><b>6.6</b> Testing for Unit root</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://vipul-bhatt.github.io/Econ-483-Notes/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Time Series Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling-cycle" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Modeling Cycle</h1>
<p>In this chapter we will focus on the cyclical component of a time series and hence focus on data that either has no trend and seasonal components, or data that is filtered to eliminate any trend and seasonality. One of the most commonly used method to model cyclicality is the <em>Autogressive Moving Average (ARMA)</em>. This model has two distinct components:</p>
<ol style="list-style-type: decimal">
<li><p><em>Autoregressive (AR) component</em>: the current period value of a time series variable depends on its past (lagged) observations. We use <span class="math inline">\(p\)</span> to denote the <strong>order</strong> of the AR component and is the number of lags of a variable that directly affect the current period value. For example, a firm’s production in the current period maybe impacted by past levels of production. If last year’s production exceeded demand, the stock of unsold goods may be used to meet this period demand first, hence lowering the current period production.</p></li>
<li><p><em>Moving average (MA) component</em>: the current period value of a time series variable depends on current period <strong>shock</strong> as well as past shocks to this variable. We use <span class="math inline">\(q\)</span> to denote the <strong>order</strong> of the MA component and is the number of past period shocks that affect the current period value of the variable of interest. For example, if the Federal Reserve Bank raises the interest in 2016, the effects of that policy shock may impact investment and consumption spending in 2017.</p></li>
</ol>
<p>Before we consider these time series model in details it is useful to discuss certain properties of time series that allow us a better understanding of these models.</p>
<div id="stationarity-and-autocorrelation" class="section level2">
<h2><span class="header-section-number">6.1</span> Stationarity and Autocorrelation</h2>
<div id="covariance-stationary-time-series" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Covariance Stationary Time Series</h3>

<div class="definition">
<span id="def:d7" class="definition"><strong>Definition 6.1  (Covariance Stationary Time Series)  </strong></span>
</div>

<p>A time series <span class="math inline">\(\{y_t\}\)</span> is said to be a <em>covariance stationary process</em> if:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E(y_t)=\mu_y \quad \forall \quad t\)</span></li>
<li><span class="math inline">\(Var(y_t)=\sigma_y^2 \quad \forall \quad t\)</span></li>
<li><span class="math inline">\(Cov(y_t,y_{t-s})=\gamma(s) \quad \forall \quad s\neq t\)</span></li>
</ol>
<p>One way to think about stationarity is <em>mean-reversion</em>, i.e, the tendency of a time series to return to its <em>long-run</em> unconditional mean following a shock (or a series of shock). Figure @(fig:ch6-figure1) below shows this property graphically.</p>
<div class="figure" style="text-align: center"><span id="fig:ch6-figure1"></span>
<img src="bookdown-demo_files/figure-html/ch6-figure1-1.png" alt="Reversion to mean" width="80%" />
<p class="caption">
Figure 6.1: Reversion to mean
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:ch6-figure2"></span>
<img src="bookdown-demo_files/figure-html/ch6-figure2-1.png" alt="Reversion to mean in practice" width="80%" />
<p class="caption">
Figure 6.2: Reversion to mean in practice
</p>
</div>
<p>In practice however, you will not be able to visualize a mean-reverting stationary process this clearly. For example, in Figure <a href="modeling-cycle.html#fig:ch6-figure2">6.2</a> we plot real GDP growth for the U.S. which is a stationary process with a mean of 0.7%. In this chapter we will only consider stationary time series data. Later on we will learn how to work with non-stationary data.</p>
<!-- Note: -->
<!-- 1. By definition a time series with trend is non-stationary. For example, consider the following linear trend model: -->
<!-- \[ y_t = \beta_0 + \beta_1 t + \epsilon_t \quad where \ \epsilon_t \sim WN(0,\sigma^2_\epsilon) \] -->
<!-- Here it is easy to show that the unconditional mean of this model changes over time and hence is not constant. If after removing the trend (i.e., the residual from the above model) we get stationarity, then such data is called *trend stationary*. -->
<!-- 2. Similarly, a model with seasonality will be non-stationary. -->
<!--  -->
</div>
<div id="correlation-vs-autocorrelation" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Correlation vs Autocorrelation</h3>
<p>In statistics, correlation is a measure of relationship between two variables. In the time series setting, we can think of the current period value and the past period value of a variable as two <strong>separate</strong> variables, and compute correlation between them. Such a correlation, between current and lagged observation of a time series is called <strong>serial correlation</strong> or <strong>autocorrelation</strong>. In general, for a time series, <span class="math inline">\(\{y_t\}\)</span>, the autocorrelation is given by:</p>
<p><span class="math display">\[\begin{align}
    Cor(y_t,y_{t-s})=\frac{ Cov(y_t,y_{t-s})}{\sqrt{\sigma^2_{y_t} \times \sigma^2_{y_{t-s}}}}
        \end{align}\]</span>
where <span class="math inline">\(Cov(y_t,y_{t-s})= E(y_t-\mu_{y_t})(y_{t-s}-\mu_{y_{t-s}})\)</span> and <span class="math inline">\(\sigma^2_{y_t}=E(y_t-\mu_{y_t})^2\)</span></p>
<p>For a stationary time series, using the three conditions the <strong>Autocorrelation Function (ACF)</strong> denoted by <span class="math inline">\(\rho(s)\)</span> is given by:</p>
<p><span class="math display">\[\begin{align}
    ACF(s) \ or \ \rho(s)=\frac{\gamma(s)}{\gamma(0)}
    \end{align}\]</span></p>
<p>Non-zero values of the ACF indicates presences of serial correlation in the data. Figure <a href="modeling-cycle.html#fig:ch6-figure3">6.3</a> shows the ACF for a stationary time series with positive serial correlation. If your data is stationary then the ACF should eventually converge to 0. For a non-stationary data, the ACF function will not decay over time.</p>
<div class="figure" style="text-align: center"><span id="fig:ch6-figure3"></span>
<img src="bookdown-demo_files/figure-html/ch6-figure3-1.png" alt="ACF for a Stationary Time Series" width="80%" />
<p class="caption">
Figure 6.3: ACF for a Stationary Time Series
</p>
</div>
</div>
<div id="partial-autocorrelation" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Partial Autocorrelation</h3>

<div class="definition">
<span id="def:d9" class="definition"><strong>Definition 6.2  (Partial Auto Correlation Function (PACF))  </strong></span>
</div>

<p>The ACF captures the relationship between the current period value of a time series and all of its past observations. It includes both direct as well as indirect effects of the past observations on the current period value. Often times it is of interest to measure the direct relationship between the current and past observations, <strong>partialing</strong> out all indirect effects. The <em>partial autocorrelation function (PACF)</em> for a stationary time series <span class="math inline">\(y_t\)</span> at lag <span class="math inline">\(s\)</span> is the direct correlation between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-s}\)</span>, after filtering out the linear influence of <span class="math inline">\(y_{t-1},\ldots,y_{t-s-1}\)</span> on <span class="math inline">\(y_t\)</span>. Figure <a href="modeling-cycle.html#fig:ch6-figure4">6.4</a> below shows the PACF for a stationary time series where only one lag directly affects the time series in the current period.</p>
<div class="figure" style="text-align: center"><span id="fig:ch6-figure4"></span>
<img src="bookdown-demo_files/figure-html/ch6-figure4-1.png" alt="PACF for a Stationary Time Series" width="80%" />
<p class="caption">
Figure 6.4: PACF for a Stationary Time Series
</p>
</div>
</div>
<div id="lag-operator" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Lag operator</h3>
<p>A <strong>lag operator</strong> denoted by <span class="math inline">\(L\)</span> allows us to write ARMA models in a more concise way. Applying lag operator once moves the time index by one period; applying it twice moves the time index back by two period; applying it <span class="math inline">\(s\)</span> times moves the index back by <span class="math inline">\(s\)</span> periods.
<span class="math display">\[ Ly_t=y_{t-1} \]</span>
<span class="math display">\[ L^2y_t=y_{t-2} \]</span>
<span class="math display">\[ L^3y_t=y_{t-3} \]</span>
<span class="math display">\[\vdots\]</span>
<span class="math display">\[ L^sy_t=y_{t-s} \]</span></p>
</div>
</div>
<div id="autoregressive-ar-model" class="section level2">
<h2><span class="header-section-number">6.2</span> Autoregressive (AR) Model</h2>
<p>A <em>stationary</em>time series <span class="math inline">\(\{x_t\}\)</span> can be modeled as an AR process. In general, an AR(p) model is given by:</p>
<p><span class="math display">\[\begin{equation}
 y_t = \phi_0 +\phi_1 y_{t-1} + \phi_2 y_{t-2} + ...... + \phi_p y_{t-p}+\epsilon_t
 \end{equation}\]</span></p>
<p>Here <span class="math inline">\(\phi_i\)</span> captures the effect of <span class="math inline">\(y_{t-i}\)</span> on <span class="math inline">\(y_t\)</span>. The order of the AR process is not known apriori. It is common to use either AIC or BIC to determine the optimal lag length for an AR process.</p>
<p>Using the Lag operator, we can rewrite the above AR(p) model as follows:
<span class="math display">\[ \Phi(L)y_t=\phi_0+\epsilon_t \]</span></p>
<p>where <span class="math inline">\(\displaystyle \Phi(L)\)</span> is a polynomial of degree <span class="math inline">\(p\)</span> in L:</p>
<p><span class="math display">\[ \Phi(L) = 1-\phi_1 L - \phi_2 L^2- \ldots\ldots\ldots\ldots -\phi_p L^p\]</span></p>
<p>For example, an AR(1) model can be written as:
<span class="math display">\[y_t=\phi_0+\phi_1 y_{t-1} + \epsilon_t \Rightarrow  \Phi(L)y_t=\phi_0+\epsilon_t\]</span>
where,
<span class="math display">\[ \Phi(L) = 1-\phi_1 L \]</span></p>
<p><strong>Characteristic equation</strong>: A characteristic equation is given by:</p>
<p><span class="math display">\[\Phi(L)=0\]</span></p>
<p>The roots of this equation play an important role in determining the dynamic behavior of a time series.</p>
<div id="unit-root-and-stationarity" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Unit root and Stationarity</h3>
<p>For a time series to be stationary there should be no <strong>unit root</strong> in its <em>characteristic equation</em>. In other words, all roots of the characteristic equation must fall outside the unit circle. Consider the following AR(1) model:
<span class="math display">\[\Phi(L)y_t = \phi_0 + \epsilon_t\]</span></p>
<p>The characteristic equation is given by:
<span class="math display">\[\Phi(L)=1-\phi_1L=0 \]</span></p>
<p>The root that satisfies the above equation is:
<span class="math display">\[ L^*=\frac{1}{\phi_1}\]</span></p>
<p>For no unit root to be present, <span class="math inline">\(L^*&gt;|1|\)</span> which implies that <span class="math inline">\(|\phi_1|&lt;1\)</span>.</p>
<p>Typically, for any AR process to be stationary, some restrictions will be imposed on the values of <span class="math inline">\(\phi_i&#39;s\)</span>, the coefficients of the lagged variables in the model.</p>
</div>
<div id="properties-of-an-ar1-model" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Properties of an AR(1) model</h3>
<p>A stationary AR(1) model is given by:
<span class="math display">\[ y_t=\phi_0 +\phi_1 y_{t-1}+ \epsilon_t \quad ; \ \epsilon_t\sim WN(0, \sigma_\epsilon^2) \ and \  |\phi_1|&lt;1\]</span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\displaystyle \phi_1\)</span> measures the persistence in data. A larger value indicates shocks to <span class="math inline">\(y_t\)</span> dissipate slowly over time.</p></li>
<li><p>Stationarity of <span class="math inline">\(y_t\)</span> implies certain restrictions on the AR(1) model.</p>
<ol style="list-style-type: lower-roman">
<li>Constant long run mean: is the unconditional expectation of <span class="math inline">\(y_t\)</span>:
<span class="math display">\[ E(y_t) = \mu_y= \frac{\phi_0}{1-\phi_1}  \]</span></li>
<li>Constant long run variance: is the unconditional variance of <span class="math inline">\(y_t\)</span>:
<span class="math display">\[ Var(y_t)=\sigma^2_y= \frac{\sigma^2_\epsilon}{1-\phi_1^2}\]</span></li>
<li>ACF function:
<span class="math display">\[ \rho(s) = \phi_1^s\]</span></li>
<li>PACF function:
<span class="math display">\[\begin{equation*}
  PACF(s) =
  \begin{cases}
\phi_1 &amp; \text{if  s=1}\\
0 &amp; \text{if s&gt;1}
  \end{cases}
 \end{equation*}\]</span></li>
</ol></li>
</ol>
</div>
</div>
<div id="estimating-an-ar-model" class="section level2">
<h2><span class="header-section-number">6.3</span> Estimating an AR model</h2>
<p>When estimating the AR model we have two alternatives:</p>
<ol style="list-style-type: decimal">
<li><p>OLS: biased (but consistent) estimates. Also, later on when we add MA components we cannot use OLS.</p></li>
<li><p>Maximum Likelihood Estimation (MLE): can be used to estimate AR as well as MA components</p></li>
</ol>
<div id="maximum-likelihood-estimation-mle" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Maximum Likelihood Estimation (MLE)</h3>
<ul>
<li>MLE approach is based on the following idea:</li>
</ul>
<p><em>what set of values of our parameters maximize the likelihood of observing our data if the model we have was used to generate this data.</em></p>
<p><strong>Likelihood function</strong>: is a function that gives us the probability of observing our data given a model with some parameters.</p>
<div id="likelihood-vs-probability" class="section level4">
<h4><span class="header-section-number">6.3.1.1</span> Likelihood vs Probability</h4>
<p>Consider a simple example of tossing a coin. Let <span class="math inline">\(X\)</span> denotes the random variable that is the outcome of this experiment being either heads or tails. Let <span class="math inline">\(\theta\)</span> denote the probability of heads which implies <span class="math inline">\(1-\theta\)</span> is the probability of obtaining tails. Here, <span class="math inline">\(\theta\)</span> is our parameter of interest. Suppose we toss the coin 10 times and obtain the following data on <span class="math inline">\(X\)</span>:
<span class="math display">\[X=\{H,H,H,H,H,H,T,T,T,T\}\]</span></p>
<p>Then, the probability of obtaining this sequence of X is given by:
<span class="math display">\[Prob (X|\theta)=\theta^6 (1-\theta)^4\]</span></p>
<p>This is the probability distribution function the variable <span class="math inline">\(X\)</span>. As we change <span class="math inline">\(X\)</span>, we get a different probability for a given value of <span class="math inline">\(\theta\)</span>.</p>
<p>Now let us ask a different question. Once we have observed the sequence of heads and tails, lets call it our data which is fixed. Then, what is probability of observing this data, if our probability distribution function is given by the equation above? That gives us the likelihood function:</p>
<p><span class="math display">\[ L(\theta)=Prob(X|\theta)=\theta^6(1-\theta)^4\]</span></p>
<p>Note that with fixed <span class="math inline">\(X\)</span>, as we change <span class="math inline">\(\theta\)</span> the likelihood of observing this data will change.</p>
<p><strong>This is an important point that distinguishes likelihood function from the probability distribution function. Although both have the same equation, the probability function is a function of the data with the value of the parameter fixed, while the likelihood function is a function of the parameter with the data fixed.</strong></p>
</div>
<div id="maximum-likelhood-estimation" class="section level4">
<h4><span class="header-section-number">6.3.1.2</span> Maximum Likelhood Estimation</h4>
<p>Now we are in a position to formally define the likelihood function.</p>

<div class="definition">
<span id="def:unnamed-chunk-3" class="definition"><strong>Definition 6.3  </strong></span>Let <span class="math inline">\(X\)</span> denotes a random variable with a given probability distribution function denoted by <span class="math inline">\(f(x_i|\theta)\)</span>. Let <span class="math inline">\(D=\{x_1, x_2,\dots,x_n\}\)</span> denote a sample realization of <span class="math inline">\(X\)</span>. Then, the likelhood function, denoted by <span class="math inline">\(L(\theta)\)</span> is given by:
<span class="math display">\[L(\theta)=f(x_1,x_2,\dots,x_n|\theta)\]</span>
</div>

<p>If we further assume that each realization of <span class="math inline">\(X\)</span> is independent of the others, we get:
<span class="math display">\[L(\theta)=f(x_1,x_2,\dots,x_n|\theta)=f(x_1|\theta)\times f(x_2|\theta) \times \dots \times f(x_n|\theta)\]</span></p>
<p>A mathematical simplification is to work with natural logs of the likelihood function, which assuming independently distributed random sample, gives us:</p>
<p><span class="math display">\[ lnL(\theta)=ln(f(x_1|\theta)\times f(x_2|\theta) \times \dots \times f(x_n|\theta))=\sum_{i=1}^{N}ln(f(x_i|\theta))\]</span></p>

<div class="definition">
<span id="def:unnamed-chunk-4" class="definition"><strong>Definition 6.4  </strong></span>The maximum likelihood estimator, denoted by <span class="math inline">\(\hat{\theta}_{MLE}\)</span>, maximizes the log likelihood function:
<span class="math display">\[ \hat{\theta}_{MLE} \equiv arg \max_{\theta} lnL(\theta) \]</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-5" class="example"><strong>Example 6.1  </strong></span>Compute maximum likelihood estimator of <span class="math inline">\(\mu\)</span> of an indpendently distributed random variable that is normally distributed with a mean of <span class="math inline">\(\mu\)</span> and a variance of <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[ f(y_t|\mu)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (y_t-\mu)^2}\]</span></p>
<p>Solution: The log likelihood function is given by:</p>
<p><span class="math display">\[lnL= -Tln2\pi-\frac{1}{2}\sum_{t=1}^T(y_t-\mu)^2 \]</span></p>
From the first order condition, we get
<span class="math display">\[ \frac{\partial LnL}{\partial \mu}=\sum_{t=1}^T(y_t-\mu)=0\Rightarrow \hat{\mu}_{MLE}=\frac{\sum_{t=1}^T y_t}{T}\]</span>
</div>

</div>
</div>
<div id="mle-of-an-arp-model" class="section level3">
<h3><span class="header-section-number">6.3.2</span> MLE of an AR(p) model</h3>
<p>One complication we face in estimating an AR(p) model is that by definition the realizations of the variable are not independent of each other. As a result we cannot simplify the likelihood function by multiplying individual probability density functions to obtain the joint probability density function, i.e.,
<span class="math display">\[ f(y_1,y_2,\dots,y_T|\theta) \neq f(y_1|\theta)\times f(y_2|\theta)\times \dots \times f(y_T|\theta)\]</span></p>
<p>Furthermore, as the order of AR increases, the joint density function we need to estimate becomes even more complicated. In this class we will focus on the method that divides the joint density into the product of conditional densities and density of a set of initial values. The idea comes from the conditional probability formula for two related events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>:</p>
<p><span class="math display">\[ P(A|B) =\frac{P(\text{A and B})}{P(B)} \Rightarrow P(\text{A and B}) = P(A|B)\times P(B) \]</span></p>
<p>In the time series context, I will explain this for a stationary AR(1) model. We know that in this model only last period observation directly affects the current period value. Hence, consider the first two observations of a stationary time series: <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>. Then the joint density of these adjacent observations is given by,</p>
<p><span class="math display">\[ f(y_1,y_2;\theta)= f(y_2|y_1; \theta)\times f(y_1;\theta)\]</span></p>
<p>Similarly, for the first three observations we get:</p>
<p><span class="math display">\[ f(y_1,y_2,y_3;\theta)= f(y_3|y_2; \theta)\times f(y_2|y_1; \theta) \times f(y_1; \theta)\]</span></p>
<p>Hence, for <span class="math inline">\(T\)</span> observations we get:</p>
<p><span class="math display">\[ f(y_1,y_2,y_3, ...,y_T; \theta)= f(y_T|y_{T-1};\theta)\times f(y_{T-1}|y_{T-2}; \theta)\times.... \times f(y_1; \theta)\]</span></p>
<p>The log-likelihood function is given by:</p>
<p><span class="math display">\[ ln \ L(\theta) = ln \ f(y_1;\theta) + \sum_{t=2}^{T} ln \ f(y_t|y_{t-1}; \theta)  \]</span></p>
<p>We can then maximize the above likelihood function to obtain an MLE estimator for the AR(1) model.</p>
</div>
<div id="selection-of-optimal-order-of-the-ar-model" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Selection of optimal order of the AR model</h3>
<p>Note that apriori we do not know the order of the AR model for any given time series. We can determine the optimal lag order by using either AIC or BIC. The process is as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Set <span class="math inline">\(p=p_{max}\)</span> where <span class="math inline">\(p_{max}\)</span> is an integer. A rule of thumb is to set
<span class="math display">\[p_{max}=integer\left[12\times \left(\frac{T}{100}\right)^{0.25}\right]\]</span></p></li>
<li><p>Estimate all AR models from <span class="math inline">\(p=1\)</span> to <span class="math inline">\(p=p_{max}\)</span>.</p></li>
<li><p>Select the final model as the one with lowest AIC or lowest BIC.</p></li>
</ol>
</div>
<div id="forecasting-using-arp-model" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Forecasting using AR(p) model</h3>
<p>Having estimated our AR(p) model with the optimal lag length, we can use the conditional mean to compute the forecast and conditional variance to compute the forecast errors. Consider an AR(1) model:</p>
<p><span class="math display">\[y_t=\phi_0+\phi_1 y_{t-1} +\epsilon_t\]</span></p>
<p>Then, the 1-period ahead forecast is given by:
<span class="math display">\[f_{t,1}=E(y_{t+1}|\Omega_t)=\phi_0+\phi_1 y_t\]</span>
Similarly, the 2-period ahead forecast is given by:
<span class="math display">\[f_{t,2}=E(y_{t+2}|\Omega_t)=\phi_0+\phi_1 E(y_{t+1}|\Omega_t) =\phi_0+\phi_1f_{t,1}\]</span></p>
<p>In general, we can get the following recursive forecast equation for h-period’s ahead:
<span class="math display">\[f_{t,h}=\phi_0+\phi_1 f_{t,h-1}\]</span></p>
<p>Correspondingly, the 1-period ahead forecast error is given by:
<span class="math display">\[e_{t,1}=y_{t+1}- f_{t,1}=\epsilon_{t+1}\]</span></p>
<p>The 2-period ahead forecast error is given by:</p>
<p><span class="math display">\[e_{t,2}=y_{t+2}-f_{t,2}=\epsilon_{t+2}+\phi_1 \epsilon_{t+1} \]</span>
Hence, the h-period ahead forecast is given by:</p>
<p><span class="math display">\[e_{t,h}=\epsilon_{t+h} + \phi_1 \epsilon_{t+h-1}\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-6" class="theorem"><strong>Theorem 6.1  </strong></span>The h-period ahead forecast converges to the unconditional mean of <span class="math inline">\(y_t\)</span>, i.e., <span class="math display">\[\lim_{h\to\infty} f_{t,h}=\mu_y=\frac{\phi_0}{1-\phi_1}\]</span>
</div>


<div class="theorem">
<span id="thm:unnamed-chunk-7" class="theorem"><strong>Theorem 6.2  </strong></span>The variance of the h-period ahead forecast error converges to the unconditional variance of <span class="math inline">\(y_t\)</span>, i.e., <span class="math display">\[\lim_{h\to\infty} Var(e_{t,h})=\sigma^2_y=\frac{\sigma^2_\epsilon}{1-\phi_1^2}\]</span>
</div>

</div>
</div>
<div id="moving-average-ma-model" class="section level2">
<h2><span class="header-section-number">6.4</span> Moving Average (MA) Model</h2>
<p>Another commonly used method for capturing the cyclical component of the time series is the <strong>moving average (MA)</strong> model where the current value of a time series linearly depends on current and past shocks. Formally, a <em>stationary</em> time series <span class="math inline">\(\{y_t\}\)</span> can be modeled as an MA(q) process:
<span class="math display">\[\begin{equation}
  y_t = \theta_0 + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ...... + \theta_q \epsilon_{t-q}
    \end{equation}\]</span></p>
<p>Using lag operator, we can write this in more compact form as:</p>
<p><span class="math display">\[y_t = \theta_0 +\Theta(L) \epsilon_t\]</span></p>
<p>where <span class="math inline">\(\Theta(L)=1+\theta_1 L+ \theta_2 L^2+...+\theta_q L^q\)</span> is lag polynomial of order <span class="math inline">\(q\)</span>.</p>
<p>Note that because each one of the current and past shocks are white noise processes, an MA(q) model is always stationary.</p>
<div id="invertibility-of-an-ma-process" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Invertibility of an MA process</h3>
<p>Consider the following MA(1) process with<span class="math inline">\(\theta_0=0\)</span> for simplicity:
<span class="math display">\[y_t=\epsilon_t +\theta_1 \epsilon_{t-1}\]</span></p>
<p>Using the lag operator we can rewrite this equation as follows:</p>
<p><span class="math display">\[y_t= (1+\theta_1L)\epsilon_t \Rightarrow y_t(1+\theta_1 L) ^{-1}=\epsilon_t\]</span></p>
<p>Note that if <span class="math inline">\(|\theta_1|&lt;1\)</span>, then we can use the Taylor series expansion centered at 0 and get:</p>
<p><span class="math display">\[(1+\theta_1 L)^{-1}=1-\theta_1 L+(\theta_1L)^2-(\theta_1L)^3+ (\theta_1L)^4-...... \]</span></p>
<p>Hence, an MA(1) can be rewritten as follows:</p>
<p><span class="math display">\[y_t (1-\theta_1 L+(\theta_1L)^2-(\theta_1L)^3+ (\theta_1L)^4-......)=\epsilon_t\]</span>
<span class="math display">\[\Rightarrow y_t -\theta_1 y_{t-1} +\theta_1^2y_{t-2}-\theta_1^3 y_{t-3}....=\epsilon_t\]</span></p>
<p>Rearranging terms, we get the <span class="math inline">\(AR(\infty)\)</span> representation for an invertible MA(1) model:
<span class="math display">\[y_t=-\sum_{i=1}^{\infty}(-\theta_1)^i \ y_{t-i}+\epsilon_t\]</span></p>

<div class="definition">
<span id="def:unnamed-chunk-8" class="definition"><strong>Definition 6.5  </strong></span>An MA process is invertible if it can be represented as a stationary <span class="math inline">\(AR(\infty)\)</span>.
</div>

</div>
<div id="properties-of-an-invetible-ma1" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Properties of an invetible MA(1)</h3>
<p>An invertible MA(1) model is given by:</p>
<p><span class="math display">\[ y_t = \theta_0 + \epsilon_t + \theta_1 \epsilon_{t-1} \quad ; \ \epsilon_t\sim WN(0, \sigma_\epsilon^2) \ and \  |\theta_1|&lt;1\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Constant unconditional mean of <span class="math inline">\(y_t\)</span>:
<span class="math display">\[E(y_t)=\mu_y =\theta_0 \]</span></p></li>
<li><p>Constant unconditional variance of <span class="math inline">\(y_t\)</span>:</p></li>
</ol>
<p><span class="math display">\[Var(y_t)=\sigma^2_y=\sigma^2_\epsilon(1+\theta_1^2)\]</span>)</p>
<ol start="3" style="list-style-type: decimal">
<li><p>ACF function:
<span class="math display">\[\begin{equation*}
  ACF(s) =
  \begin{cases}
 \frac{\theta_1}{1+\theta_1^2} &amp; \text{if  s=1}\\
 0 &amp; \text{if s&gt;1}
  \end{cases}
 \end{equation*}\]</span></p></li>
<li><p>PACF function: using the invertibility it is evident that PACF of an MA(1) decays with <span class="math inline">\(s\)</span>.</p></li>
</ol>
</div>
<div id="forecast-based-on-maq" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Forecast based on MA(q)</h3>
<p>Like before, the h-period ahead forecast is the conditional expected value of the time series. Consider an MA(1) model:</p>
<p><span class="math display">\[y_t=\theta_0 +\epsilon_t + \theta_1 \epsilon_{t-1}\]</span></p>
<p>Then, the 1-period ahead forecast is given by:</p>
<p>The h-period ahead forecast for <span class="math inline">\(h&gt;1\)</span> is given by:
<span class="math display">\[f_{t,h}=E(y_{t+h}|\Omega_t)=\theta_0\]</span></p>
<p>In general, for an MA(q) model, the forecast for <span class="math inline">\(h&gt;q\)</span> is the long run mean <span class="math inline">\(\theta_0\)</span>. This is why we say that an MA(q) process has a memory of <em>q</em> periods.</p>
</div>
</div>
<div id="armap-q" class="section level2">
<h2><span class="header-section-number">6.5</span> ARMA(p, q)</h2>
<p>An ARMA model simply combines both AR and MA components to model the dynamics of a time series. Formula,</p>
<p><span class="math display">\[\begin{equation}
   y_t = \phi_0 +\phi_1 y_{t-1} + \phi_2 y_{t-2} + ...... + \phi_p y_{t-p}+\epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ...... + \theta_q \epsilon_{t-q}
   \end{equation}\]</span></p>
<p>Note that:</p>
<ol style="list-style-type: decimal">
<li><p>Estimation is done by maximum likelihood method.</p></li>
<li><p>Optimal order for AR and MA components is selected using AIC and/or BIC.</p></li>
<li><p>The forecast of <span class="math inline">\(y_t\)</span> from an ARMA(p,q) model will be dominated by the AR component for <span class="math inline">\(h&gt;q\)</span>. To see this consider the following ARMA(1,1) model:</p></li>
</ol>
<p><span class="math display">\[y_t = \phi_0 +\phi_1 y_{t-1}+ \epsilon_t + \theta_1 \epsilon_{t-1}\]</span></p>
<p>Then, the 1-period ahead forecast is:
<span class="math display">\[f_{t,1} = E(y_{t+1}|\Omega_t) = \phi_0 + \phi_1 y_t + \theta_1 \epsilon_{t-1}\]</span></p>
<p>Here both MA and AR component affect the forecast. But now consider the 2-period ahead forecast:</p>
<p><span class="math display">\[f_{t,2} = E(y_{t+2}|\Omega_t) = \phi_0 + \phi_1 f_{t,1}\]</span></p>
<p>Hence, no role is played by the MA component in determining the 2-period ahead forecast. For any <span class="math inline">\(h&gt;1\)</span> only the AR component affects the forecast from this model.</p>
</div>
<div id="testing-for-unit-root" class="section level2">
<h2><span class="header-section-number">6.6</span> Testing for Unit root</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modeling-trend-and-seasonal-components.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
