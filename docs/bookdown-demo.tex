\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Applied Time Series Analysis},
            pdfauthor={Vipul Bhatt},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{amsthm}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Applied Time Series Analysis}
\author{Vipul Bhatt}
\date{2020-05-21}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

These lecture notes are prepared for an upper level undergraduate course in time series econometrics. Every fall I teach a course on applied time series analysis at James Madison University. These notes borrow heavily from the teaching material that I have developed over several years of instruction of this course.

One of my main objective is to develop a primer on time series analysis that is more accessible to undergraduate students than standard textbooks available in the market. Most of these textbooks in my opinion are densely written and assume advanced mathematical skills on the part of our students. Further, I have also struggled with their topic selection and organization. Often I end up not following the chapters in order and modify content (by adding or subtracting) to meet my students needs. Such changes causes confusion for some students and more importantly discourages optimal use of the textbook. Hence, this is an undertaking to develop a primer on time series that is accessible, follows a more logical sequencing of topics, and covers content that is most useful for undergraduate students in business and economics.

\emph{Note: These notes have been prepared by me using various sources, published and unpublished. All errors that remain are mine.}

\hypertarget{intro}{%
\chapter{Introduction to Forecasting}\label{intro}}

\hypertarget{time-series}{%
\section{Time Series}\label{time-series}}

A time series is a specific kind of data where observations of a variable are recorded over time. For example, the data for the U.S. GDP for the last 30 years is a time series data.

Such data shows how a variable is changing over time. Depending on the variable of interest we can have data measured at different frequencies. Some commonly used frequencies are intra-day, daily, weekly, monthly, quarterly, semi-annual and annual. Figure \ref{fig:ch1-figure1} below plots data for quarterly and monthly frequency.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch1-figure1-1} 

}

\caption{Time Series at quarterly and monthly frequency}\label{fig:ch1-figure1}
\end{figure}

The first panel shows data for the real gross domestic product (GDP) for the US in billions of 2012 dollars, measured at a quarterly frequency. The second panel shows data for the advance retail sales (millions of dollars), measured at monthly frequency.

Formally, we denote a time series variable by \(y_t\), where \(t=0,1,2,..,T\) is the observation index. For example, at \(t=10\) we get the tenth observation of this time series, \(y_{10}\).

\hypertarget{serial-correlation}{%
\section{Serial Correlation}\label{serial-correlation}}

Serial correlation (or auto correlation) refers to the tendency of observations of a time series being correlated over time. It is a measure of the temporal dynamics of a time series and addresses the following question: what is the effect of past realizations of a time series on the current period value? Formally,

\begin{equation}
\rho(s)=Cor(y_t, y_{t-s}) =\frac{   Cov(y_t,y_{t-s})}{\sqrt{\sigma^2_{y_t} \times \sigma^2_{y_{t-s}}}}
\label{eq:sercor}
\end{equation}

where \(Cov(y_t,y_{t-s})= E(y_t-\mu_{y_t})(y_{t-s}-\mu_{y_{t-s}})\) and \(\sigma^2_{y_t}=E(y_t-\mu_{y_t})^2\)

Here, \(\rho(s)\) is the serial correlation of order \(s\). For example, \(s=1\) implies \emph{first order} serial correlation between \(y_t\) and \(y_{t-1}\), \(s=2\) implies \emph{second order} serial correlation between \(y_t\) and \(y_{t-2}\), and so on.

Note that often we use historical data to forecast. If there is no serial correlation, then past can offer no guidance for the present and future. In that sense, presence of serial correlation of some order is the first condition for being able to forecast a time series using its historical realizations.

Now, we can either have positive or negative serial correlation in data. Figure \ref{fig:ch1-figure2} plots two time series with positive and negative serial correlation, respectively.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch1-figure2-1} 

}

\caption{Serial Correlation}\label{fig:ch1-figure21}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch1-figure2-2} 

}

\caption{Serial Correlation}\label{fig:ch1-figure22}
\end{figure}

\hypertarget{testing-for-serial-correlion}{%
\section{Testing for Serial Correlion}\label{testing-for-serial-correlion}}

We can use a Lagrange-Multiplier (LM) test for detecting serial correlation. This test is also known as \emph{Breuch-Godfrey} test. I will use the linear regression model to explain this test. Consider the following regression model:
\begin{equation}
y_t=\beta_0 + \beta_1 X_{1t}+\epsilon_t
\end{equation}

Consider the following model for serial correlation of order \emph{p} for the error term:
\begin{equation}
\epsilon_t=\rho_1 \epsilon_{t-1}+\rho_2 \epsilon_{t-2}+...+ \rho_p \epsilon_{t-p}+\nu_t
\label{eq:bg}
\end{equation}

Then we are interested in the following test:

\[H_0=\rho_1=\rho_2=...=\rho_p=0 \]
\[H_A = Not \ H_0 \]

To implement this test, we estimate the BG regression model given by:
\begin{equation}
e_t=\alpha_0 + \alpha_1 X_{1t}+ \rho_1 e_{t-1}+\rho_2 e_{t-2}+...+ \rho_p e_{t-p}+\nu_t
\label{eq:bg1}
\end{equation}

where we replace the error term with the OLS residuals (denoted by \(e\)). The LM test statistic is given by:

\[ LM  = N\times R^2_{BG}  \sim \chi^2_p  \]

If the test statistic value is greater than the critical value then we reject the null hypothesis.

\hypertarget{white-noise-process}{%
\section{White Noise Process}\label{white-noise-process}}

A time series is a \emph{white noise} process if it has zero mean, constant and finite variance, and is serially uncorrelated. Formally, \(y_t\) is a white noise process if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E(y_t)=0\)
\item
  \(Var(y_t)=\sigma^2_y\)
\item
  \(Cov(y_t,y_{t-s})= 0 \forall s\neq t\)
\end{enumerate}

We can compress the above definition as: \(y_t\sim WN(0,\sigma^2_y)\).
Often we assume that the unexplained part of a time series follows a white noise process. Formally,

\begin{equation}
Time \ Series \ = \  Explained  \ + \ White \ Noise
\end{equation}

By definition we cannot forecast a white noise process. An important diagnostics of model adequacy is to test whether the estimated residuals are white noise (more on this later).

\hypertarget{important-elements-of-forecasting}{%
\section{Important Elements of Forecasting}\label{important-elements-of-forecasting}}

\begin{definition}[Forecast]
\protect\hypertarget{def:d1}{}{\label{def:d1} \iffalse (Forecast) \fi{} }
\end{definition}

A \emph{forecast} is an \emph{informed} guess about the unknown future value of a time series of interest. For example, what is the stock price of Facebook next Monday?

There are three possible types of forecasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Density Forecast}: we forecast the entire probability distribution of the possible future value of the time series of interest. Hence,
\end{enumerate}

\begin{equation}
F(a)=P[y_{t+1}\leq a]
\end{equation}

give us the probability that the 1-period ahead future value of \(y_{t+1}\) will be less than or equal to \(a\). For example, the future real GDP growth could be normally distributed with a mean of 1.3\% and a standard deviation of 1.83\%. Figure \ref{fig:ch1-figure3} below plots the density forecast for real GDP growth.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch1-figure3-1} 

}

\caption{Density Forecast for Future Real GDP Growth}\label{fig:ch1-figure3}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \emph{Point Forecast}: our forecast at each horizon is a single number. Often we use the expected value or mean as the point forecast. For example, the point forecast for the 1-period ahead real GDP growth can be the mean of the probability distribution of the future real GDP growth:
  \begin{equation}
  f_{t,1}=1.3%
  \end{equation}
\item
  \emph{Interval Forecast}: our forecast at each horizon is a range which is obtained by adding \emph{margin of errors} to the point forecast. With some probability we expect our future value to fall withing this range. For example, the 95\% interval forecast for the next period real GDP growth is (-2.36\%,4.96\%). Hence, with 95\% confidence we expect next period GDP to fall between -2.36\% and 4.96\%.
\end{enumerate}

\begin{definition}[Forecast Horizon]
\protect\hypertarget{def:d2}{}{\label{def:d2} \iffalse (Forecast Horizon) \fi{} }
\end{definition}

\emph{Forecast Horizon} is the number of periods into the future for which we forecast a time series. We will denote it by \(h\). Hence, for \(h=1\), we are looking at 1-period ahead forecast, for \(h=2\) we are looking at 2-period ahead forecast and so on.

Formally, for a given time series \(y_t\), the h-period ahead unknown value is denoted by \(y_{t+h}\). The forecast of this value is denoted \(f_{t,h}\).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch1-figure4-1} 

}

\caption{Forecast Horizon}\label{fig:ch1-figure4}
\end{figure}

\begin{definition}[Forecast Error]
\protect\hypertarget{def:d3}{}{\label{def:d3} \iffalse (Forecast Error) \fi{} }
\end{definition}

A \emph{forecast error} is the difference between the realization of the future value and the previously made forecast. Formally, the \(h\)-period ahead forecast error is given by:

\begin{equation}
e_{t,h}=y_{t+h}-f_{t,h}
\end{equation}

Hence, for every horizon, we will have a forecast and a corresponding forecast error. These errors can be negative (indicating over prediction) or positive (indicating under prediction).

\begin{definition}[Information Set]
\protect\hypertarget{def:d4}{}{\label{def:d4} \iffalse (Information Set) \fi{} }
\end{definition}

Forecasts are based on \emph{information} available at the time of making the forecast. \emph{Information Set} contains all the relevant information about the time series we would like to forecast. We denote the set of information available at time \(T\) by \(\Omega_T\). There are two types of information sets:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Univariate Information set: Only includes historical data on the time series of interest:
  \begin{equation}
  \Omega_T=\{y_T, y_{T-1}, y_{T-2}, ...., y_1\}
  \end{equation}
\item
  Multivariate Information set: Includes historical data on the time series of interest as well as any other variable(s) of interest. For example, suppose we have one more variable \(x\) that is relevant for forecasting \(y\). Then:
  \begin{equation}
  \Omega_T=\{y_T, x_T, y_{T-1}, x_{T-1}, y_{T-2},x_{T-2}. ...., y_1, x_1\}
  \end{equation}
\end{enumerate}

\hypertarget{loss-function-and-optimal-forecast}{%
\section{Loss Function and Optimal Forecast}\label{loss-function-and-optimal-forecast}}

Think of a forecast as a solution to an \emph{optimization} problem. When forecasts are wrong, the person making the forecast will suffer some \emph{loss}. This loss will be a function of the magnitude as well as the sign of the \emph{forecast error}. Hence, we can think of an \emph{optimal forecast} as a solution to a minimization problem where the forecaster is minimizing the loss from the forecast error.

\begin{definition}[Loss Function]
\protect\hypertarget{def:d5}{}{\label{def:d5} \iffalse (Loss Function) \fi{} }
\end{definition}

A \emph{loss} function is a mapping between forecast errors and their associated losses. Formally, we denote the h-period ahead loss function by \(L(e_{t,h})\). For a function to be used as a loss function, three properties must be satisfied:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(L(0)=0\)
\item
  \(\frac{dL}{de}>0\)
\item
  \(L(e)\) is a continuous function.
\end{enumerate}

Two types of loss functions are:

\begin{itemize}
\tightlist
\item
  Symmetric Loss Function: both positive and negative forecast errors lead to same loss. See Figure \ref{fig:ch1-figure5}. A commonly used loss function is \emph{quadratic loss function} given by:
\end{itemize}

\begin{equation}
L(e_{t,h})=e_{t,h}^2 = (y_{t+h}-f{t,h})^2
\end{equation}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch1-figure5-1} 

}

\caption{Quadratic Loss Functions}\label{fig:ch1-figure5}
\end{figure}

\begin{itemize}
\tightlist
\item
  Asymmetric Loss Function: loss depends on the sign of the forecast error. For example, it could be that positive errors produce greater loss when compared to negative errors. See the function below and Figure \ref{fig:ch1-figure6} that attaches a higher loss to positive errors:
\end{itemize}

\begin{equation}
L(e_{t,h})=e_{t,h}^2+4 \times e_{t,h}
\end{equation}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch1-figure6-1} 

}

\caption{Asymmetric Loss Function}\label{fig:ch1-figure6}
\end{figure}

Once we have chosen our loss function, the optimal forecast can be obtained by minimizing the expected loss function.

\begin{definition}[Optimal Forecast]
\protect\hypertarget{def:d6}{}{\label{def:d6} \iffalse (Optimal Forecast) \fi{} }
\end{definition}

An \emph{optimal forecast} minimizes the expected loss from the forecast, given the information available at the time. Mathematically, we denote it by \(f^*_{t,h}\) and it solves the following minimization problem:
\begin{equation}
min_{f_{t,h}} E(L(e_{t,h})|\Omega_t)
\end{equation}

In theory we can assume any functional form for the loss function and that will lead to a different \emph{optimal forecast}. An important result that follows from a specific functional form is stated as Theorem 1.1.

\begin{theorem}
\protect\hypertarget{thm:unnamed-chunk-2}{}{\label{thm:unnamed-chunk-2} }If the loss function is quadtratic then the optimal forecast is the conditional mean of the time series of interest. Formally, if \(L(e_{t,h})=e_{t,h}^2\) then,
\begin{equation}
f^*_{t,h}=E(y_{t+h}|\Omega_t)
\end{equation}
\end{theorem}

Note that \(E(e_{t,h}^2)\) is known as \emph{mean squared errors (MSE)}. Hence, the expected loss from a quadratic loss function is the same as the MSE. In this course, we assume that the forecaster faces a quadratic loss function and hence based on Theorem 1.1, we will learn different models for estimating the conditional mean of the future value of the time series of interest, i.e., \(E(y_{t+h}|\Omega_t)\).

\hypertarget{regression-based-forecasting}{%
\chapter{Regression-based Forecasting}\label{regression-based-forecasting}}

One way to compute the conditional expectation is the linear regression model. Here, our information set contains data on all relevant explanatory variables available at the time of forecast, i.e,

\begin{equation}
\Omega_t={X_{1t}, X_{2t},...X_{Kt}}
\end{equation}

Hence, we get the following equality:

\begin{equation}
E(y_t|\Omega_t)=E(y_{t}|X_{1t}, X_{2t}, X_{3t},...,X_{Kt})
\end{equation}

The right hand side of the above equation is the multiple regression model of the form:
\begin{equation}
 y_{t}=\beta_0+\beta_1 X_{1t}+\beta_2 X_{2t}+..+\beta_K X_{Kt}+\epsilon_t
 \end{equation}

We can easily estimate the above model using Ordinary Least Squares (OLS) and compute the \emph{predicted value} of \(y\):
\begin{equation}
    \widehat{y}_t = \widehat{\beta_0} +\widehat{\beta_1} X_{1t} +\widehat{\beta_2} X_{2t}+...+ \widehat{\beta_k} X_{Kt}
  \end{equation}

The above equation can be used to compute the optimal forecast. Suppose, we are interested in computed the \(h\) period ahead forecast for \(y\). Then, using the above equation we get:
\begin{equation}
        \widehat{y}_{t+h} =  \widehat{\beta_0} +\widehat{\beta_1} X_{1t+h} +\widehat{\beta_2} X_{2t+h}+...+ \widehat{\beta_k} X_{Kt+h}
    \end{equation}

\hypertarget{scenario-analysis-and-conditional-forecasts}{%
\section{Scenario Analysis and Conditional Forecasts}\label{scenario-analysis-and-conditional-forecasts}}

One way to use a regression model to produce forecasts is called
\emph{scenario analysis} where we produce a different forecast for the dependent variable under each possible scenario about the future values of the independent variables. For example, what will be the forecast for inflation if the Federal Reserve Bank raises the interest rate? Would our forecast differ depending on the size of the increase in the interest rate?

\hypertarget{unconditional-forecasts}{%
\section{Unconditional Forecasts}\label{unconditional-forecasts}}

An alternative is to separately forecast each independent variable and then compute the forecast for the dependent variable. Yet another alternative is to use lagged variables as independent variables. Depending on the number of lags, we can forecast that much ahead into future (see Distributed Lag Section for details).

\hypertarget{some-practical-issues}{%
\section{Some practical issues}\label{some-practical-issues}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  To forecast the dependent variable we first need to compute a forecast for the independent variable. Errors in this step induce errors later.
\item
  \emph{Spurious regression}: It is quite possible to find a strong linear relationship between two completely unrelated variables over time if they share a common time trend.
\item
  \emph{Model Uncertainty}: We do not know the true functional form for the regression model and hence our estimated model is only a proxy for the true model.
\item
  \emph{Parameter Uncertainty}: This kind of forecast uses regression coefficients that are computed using a fixed sample. Over time with new data, there will be changes in these coefficients.
\end{enumerate}

\hypertarget{distributed-lag-regression-models}{%
\section{Distributed Lag Regression Models}\label{distributed-lag-regression-models}}

Consider the following simple regression model:

\begin{equation}
y_t= \beta_0 +\beta_1 x_t + \epsilon_t
\end{equation}

Here, if want to forecast \(y_{t+1}\) then we must either consider different scenarios for \(x_{t+1}\) or independently forecast \(x_{t+1}\) first, and then use it to compute forecast for \(y_{t+1}\). An alternative is to estimate the following lagged regression model:

\begin{equation}
y_t= \beta_0 +\beta_1 x_{t-1} + \epsilon_t
\end{equation}

Note that by estimating the above model we get the following predicted value equation for \(t+1\):

\begin{equation}
\widehat{y_{t+1}}=\widehat{\beta_0}+\widehat{\beta_1}x_{t}
\end{equation}

Hence, we can easily produce 1-period ahead forecast from this model. In order to produce forecast farther into future we would need to add more lags of the independent variable to the model. A generalized model of this kind is called \emph{distributed lag model} and is given by:
\begin{equation}
y_t= \beta_0 +\sum_{s=1}^p\beta_s x_{t-s} + \epsilon_t
\end{equation}

The number of lags to include can be determined using some kind of goodness of fit measure.

\hypertarget{dynamic-effect-of-x-on-y}{%
\subsection{Dynamic Effect of X on Y}\label{dynamic-effect-of-x-on-y}}

A very useful benefit of estimating a distributed lag model is that it allows us to measure how changes in \(x\) in the current period can impact the dependent variable over time. Consider a simple distributed lag model with two lags:
\begin{equation}
y_t=\beta_0 + \beta_1 x_{t-1} + \beta_2 x_{t-2} +\epsilon_t
\end{equation}

In this model the lag structure implies that any change in \(x\) will persist for two periods in terms of its effect on \(y\). In fact we now have to consider the \emph{dynamic} effect of \(x\) on \(y\). Formally, there are two types of effects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{dynamic effect} of \(x\) on \(y\) given by:
  \[\frac{\partial y_{t+s}}{\partial x_t} \quad s=0,1,2,...\]
\end{enumerate}

In our example, the sequence of dynamic effects are:
\begin{equation}
\frac{\partial y_{t}}{\partial x_t}  =0; \ \frac{\partial y_{t+1}}{\partial x_t}=\beta_1; \ \frac{\partial y_{t+2}}{\partial x_t}=\beta_2; \ \frac{\partial y_{t+s}}{\partial x_t}=0 \ \forall \ s>2 
\end{equation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \emph{long run effect} of \(x\) on \(y\) given by:
  \begin{equation}
  \sum_{s=0}^p\frac{\partial y_{t+s}}{\partial x_t}   
  \end{equation}
\end{enumerate}

In our example, the long run effect is:

\[\beta_1+\beta_2\]

\hypertarget{model-selection-criterion}{%
\subsection{Model Selection Criterion}\label{model-selection-criterion}}

Most often we compare models that have different number of independent variables. For example, in our application, in order to select the number of lags for output and capital stock, we will essentially compare models with different number of independent variables. In such cases we must account for the trade-off between goodness of fit and degrees of freedom. Increasing the number of independent variables will:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  lower the MSE and hence leads to better fit.
\item
  lowers the degrees of freedom
\end{enumerate}

Two commonly used measures based on MSE incorporate this trade-off:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Akaike Information Criterion (AIC):
  \[ AIC= MSE \times e^{\frac{2k}{T}} \]
\end{enumerate}

where \(k\) is the number of estimated parameters, \(T\) is the sample size. Then, \(K/T\) is the number of parameters estimated per observation and \(e^{\frac{2k}{T}}\) is the \emph{penalty factor} imposed on adding more variables to the model. As we increase \(k\), this penalty factor will increase exponentially for a given value of \(T\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Bayesian Information Criterion (BIC):
\end{enumerate}

\[ BIC= MSE \times T^{\frac{k}{T}} \]

Lower values of either AIC or BIC indicates greater accuracy. So we select a model with lower value of either of these two criteria. Note that the penalty imposed by BIC is harsher and hence it will typically select a more parsimonious model (Figure \ref{fig:ch2-figure1}).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch2-figure1-1} 

}

\caption{Penalty Factor of AIC and BIC}\label{fig:ch2-figure1}
\end{figure}

\hypertarget{application-a-model-of-investment-expenditure}{%
\section{Application: A Model of Investment Expenditure}\label{application-a-model-of-investment-expenditure}}

\hypertarget{a-multiple-regression-model-of-invesment-expenditure}{%
\subsection{A Multiple Regression Model of Invesment Expenditure}\label{a-multiple-regression-model-of-invesment-expenditure}}

Suppose have annual data on private investment, private sector output, and capital stock. Our model specification is given by:
\begin{equation}
y_t= \beta_0 + \beta_1 x_{1t}+ \beta_2 x_{2t}+\epsilon_t
\end{equation}

We can estimate the above model using OLS and then conduct scenario-based forecasting. For ease of interpretation, we will convert all variables in natural logarithms.

Table \ref{tab:ch2-table1} below presents the estimated coefficients of our regression model. Higher output and capital stock leads to greater investment expenditure.

\begin{table}

\caption{\label{tab:ch2-table1}A Multiple Regression Model of Investment Expenditure}
\centering
\begin{tabular}[t]{lcccc}
\toprule
  & Estimated Coefficients & Std. Error & t-ratio & p-value\\
\midrule
(Intercept) & -4.8421855 & 0.9623332 & -5.031714 & 0.0000044\\
x1 & 0.9987751 & 0.2418282 & 4.130102 & 0.0001104\\
x2 & 0.4204833 & 0.3643054 & 1.154205 & 0.2528456\\
\bottomrule
\end{tabular}
\end{table}

Next, we compute forecast of investment expenditure under three different scenarios:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For next 3 years, both output and capital stock remain at the average of last 3 years.
\item
  For next 3 years, both output and capital stock remain at 1\% above the average of last 3 years.
\item
  For next 3 years, both output and capital stock remain at 1\% below the average of last 3 years.
\end{enumerate}

Figure \ref{fig:ch2-figure2} below present our investment expenditure outlook under these 3 scenarios.

\begin{verbatim}
## Error in forecast(fit, newdata = newdata, level = 95): unused arguments (newdata = newdata, level = 95)
\end{verbatim}

\begin{verbatim}
## Error in forecast(fit, newdata = newdata, level = 95): unused arguments (newdata = newdata, level = 95)
\end{verbatim}

\begin{verbatim}
## Error in forecast(fit, newdata = newdata, level = 95): unused arguments (newdata = newdata, level = 95)
\end{verbatim}

\begin{verbatim}
## Error in plot(s1, include = 25, main = "Scenario 1"): object 's1' not found
\end{verbatim}

\begin{verbatim}
## Error in plot(s2, include = 25, main = "Scenario 2"): object 's2' not found
\end{verbatim}

\begin{verbatim}
## Error in plot(s3, include = 25, main = "Scenario 3"): object 's3' not found
\end{verbatim}

\hypertarget{a-distributed-lag-model-of-investment-expenditure}{%
\subsection{A Distributed Lag Model of Investment Expenditure}\label{a-distributed-lag-model-of-investment-expenditure}}

In this application we will estimate a distributed lag model for investment expenditure. The idea here is that it takes time for investment to respond to output and capital stock changes. The model specification we want to estimate is:

\begin{equation}
y_t= \beta_0 + \sum_{i=1}^p\beta_i x_{1t-i}+\sum_{i=1}^p\alpha_i x_{2t-i}+\epsilon_t
\end{equation}

where \(y\) denotes real investment expenditure of the private sector, \(x_1\) denotes output of the private sector, and \(x_2\) denotes capital stock of the private sector.

We estimate our model by first selecting the optimal lag order for each independent variable, and selecting the one with lowest value for AIC/BIC. From @\ref(tab:ch2-table2) we find that the lowest BIC occurs at lag=2. Hence, we estimate a model with two lags for each independent variable in our model.

\begin{table}

\caption{\label{tab:ch2-table2}Optimal Order of the lags}
\centering
\begin{tabular}[t]{ccc}
\toprule
Lag & AIC & BIC\\
\midrule
1 & -98.42469 & -89.72714\\
2 & -127.36043 & -114.31411\\
3 & -129.95355 & -112.55845\\
4 & -127.49885 & -105.75498\\
\bottomrule
\end{tabular}
\end{table}

Hence, our final model is given by:
\begin{equation}
y_t= \beta_0 + \sum_{i=1}^2\beta_i x_{1t-i}+\sum_{i=1}^2\alpha_i x_{2t-i}
\end{equation}

The results of our estimation are presented below in Table \ref{tab:ch2-table3}

\begin{table}

\caption{\label{tab:ch2-table3}Distributed Lag Model of Investment Expenditure}
\centering
\begin{tabular}[t]{lcccc}
\toprule
  & Estimated Coefficients & Std. Error & t-ratio & p-value\\
\midrule
(Intercept) & -6.541336 & 0.8300877 & -7.880295 & 0.0000000\\
L(x1, 1:2)1 & 10.932987 & 1.9354346 & 5.648854 & 0.0000005\\
L(x1, 1:2)2 & -9.769632 & 1.9286226 & -5.065601 & 0.0000044\\
L(x2, 1:2)1 & 2.545657 & 0.7735580 & 3.290842 & 0.0017028\\
L(x2, 1:2)2 & -2.189088 & 0.7904689 & -2.769354 & 0.0075309\\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Using our estimated model we can easily compute the dynamic effect as well as the long run effect of each independent variable on the dependent variable.
\item
  Given the lag structure of our estimated model, we can also produce forecasts for \(y_{t+1}\) by computing the following equation:
\end{enumerate}

\begin{equation}
f_{t,1}=\widehat{y_{t+1}}=\hat{\beta_0}+\hat{\beta_1}x_{1t} + \hat{\beta_2}x_{1t-1}+ \hat{\alpha_1}x_{2t}+\hat{\alpha_2}x_{2t-1}
\end{equation}

\hypertarget{components-of-a-time-series}{%
\chapter{Components of a Time Series}\label{components-of-a-time-series}}

A given time series can have four possible components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Trend: denoted by \(B_t\) captures the long run behavior of the time series of interest.
\item
  Season: denoted by \(S_t\) are \emph{periodic} fluctuations over \emph{seasons}. The period of the season is fixed and known. For example, rise in non-durable sales during Christmas.
\item
  Cycle: denoted by \(C_t\) are \emph{non-periodic} fluctuations in that they occur regularly but over periods that are not fixed in duration.
\item
  Irregular: denoted by \(\epsilon_t\) are random fluctuations, typically modeled as a white noise process.
\end{enumerate}

\hypertarget{decomposing-a-time-series}{%
\section{Decomposing a time series}\label{decomposing-a-time-series}}

We can decompose any given time series into its components. There are two ways to accomplish this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Additive Decomposition: Here it is assumed that all four components are added to obtain the underlying time series:
  \begin{equation}
  y_t= B_t+S_t+C_t +\epsilon_t
  \end{equation}
\item
  Multiplicative Decomposition: Here it is assumed that all four components are multiplied to obtain the underlying time series:
  \begin{equation}
  y_t= B_t \times S_t \times C_t \times \epsilon_t
  \end{equation}
\end{enumerate}

Note that using properties of logarithms, multiplicative decomposition is the same as additive decomposition in log terms:
\begin{equation}
log(y_t)= log(B_t) + log(S_t) + log(C_t) + log(\epsilon_t)
\end{equation}

Most statistical software can implement these decomposition using data on a time series variable as input. Typically they combine cyclical component with irregular component and provide a three-way decomposition. In Figure \ref{fig:ch3-figure1} I use R to decompose real GDP for the US into its components.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch3-figure1-1} 

}

\caption{Additive Decomposition of Retail Sales}\label{fig:ch3-figure1}
\end{figure}

\hypertarget{uses-of-decomposition-of-a-time-series}{%
\section{Uses of Decomposition of a time series}\label{uses-of-decomposition-of-a-time-series}}

The usefulness of decomposing a time series depends on our objective.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  It may be of interest to study each component separately or to simply improve our understanding of the temporal dynamics of a time series of interest. Decomposing it into different components is the first step towards achieving that goal.
\item
  We can also use the decomposition to filter out components that we are not interested in studying. If for example we are only interested in modeling the cyclical component of the time series, then we can assume some kind decomposition, additive or multiplicative, and filter out the trend and seasonal component. For example, assuming additive decomposition, the filtered time series is given by:
  \begin{equation}
  Filtered \ y_t= y_t-B_t-S_t
  \end{equation}
\end{enumerate}

We can then proceed to model the cyclical component using the filtered data.

\hypertarget{smoothing-methods}{%
\chapter{Smoothing Methods}\label{smoothing-methods}}

One way to approach forecasting is to \emph{average} out the fluctuations in the underlying time series to produce a \emph{smoothed} data which can be extrapolated to produce forecasts. These smoothing methods are essentially \emph{model-free} and may not even produce \emph{optimal forecasts}. Depending on the method used one can accommodate seasonal as well as trend components of the underlying time series.

\hypertarget{moving-average-method}{%
\section{Moving Average Method}\label{moving-average-method}}

We compute an average of most recent data values for the time series and use it as a forecast for the next period.

An important parameter is the \emph{window} over which we take the average. Let us denote this window by \(m\), then:
\begin{equation}
    y^s_{t+1}=\frac{\sum \limits_{i=t-m+1}^{t}{y_i}}{m}
    \end{equation}

A larger value of \(m\) produces greater smoothing and most software have a default value of this parameter which can be changed if needed.

\hypertarget{simple-exponential-smoothing}{%
\section{Simple Exponential Smoothing}\label{simple-exponential-smoothing}}

In the moving average method, all observations received same weight. However, it is reasonable to argue that more recent observations may have a greater influence than those in the remote past. In this method, the weight attached to past observations exponentially decay over time. Here is the algorithm for computing the smoothed data and its forecast:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialize at t=1:
  \[y_1^s=y_1\]
\item
  Update:
  \[y_{t}^{s}= \alpha y_t + (1-\alpha)y_{t-1}^{s}  \quad for \ t=2,3,...T\]
\end{enumerate}

3: h-period ahead forecast:
\[f_{T,h}= y_T^s\]

Here the h-period ahead forecast is:

\emph{Exercise: Can you show that \(y_{t}^{s}\) is a is the weighted moving average of all past observations? Use backward substitution method.}

Here \(\alpha \in (0,1)\) is the smoothing parameter, with smaller value indicating greater smoothing.

\hypertarget{holt-winters-smoothing}{%
\section{Holt-Winters Smoothing}\label{holt-winters-smoothing}}

We add trend component to the simple exponential smoothing. In step 2 the equation we use to update the smoothed data is given by:

\begin{align}
    y_{t}^{s}= \alpha y_t + (1-\alpha)(y_{t-1}^{s}+B_{t-1}) \\ \nonumber
    B_t = \beta (y_t^s -y_{t-1}^s) + (1-\beta) B_{t-1}
 \end{align}

We now have an additional parameter \(\beta\) that is the trend parameter. Here the h-period ahead forecast is:

\begin{align}
  f_{T,h} = y_T^s + h\times B_T
  \end{align}

\hypertarget{holt-winters-smoothing-with-seasonality}{%
\section{Holt-Winters Smoothing with Seasonality}\label{holt-winters-smoothing-with-seasonality}}

We now add seasonal component along with trend. Assuming multiplicative seasonality with period \(n\):

\begin{align}
    y_{t}^{s}= \alpha \frac{y_t}{S_{t-n}} + (1-\alpha)(y_{t-1}^{s}+B_{t-1})\\
    B_t = \beta (y_t^s -y_{t-1}^s) + (1-\beta) B_{t-1}\\
    S_t = \gamma\frac{y_t}{y_t^s}+(1-\gamma)S_{t-n}
  \end{align}

The h-period ahead forecast is given by:

\begin{equation}
    f_{T,h}= (y_T^s + h\times B_T) \times S_{T+h-n}
   \end{equation}

\hypertarget{application}{%
\section{Application}\label{application}}

We use R to implement a 12-period ahead forecast for new housing starts for the U.S. The data is at monthly frequency from Jan 1959 through March 2019. The resulting forecasts are plotted in Figure \ref{fig:ch4-figure1}.

\begin{verbatim}
## Error in forecast(s_exp1, h = 12): unused argument (h = 12)
\end{verbatim}

\begin{verbatim}
## Error in forecast(s_exp2, h = 12): unused argument (h = 12)
\end{verbatim}

\begin{verbatim}
## Error in forecast(s_exp3, h = 12): unused argument (h = 12)
\end{verbatim}

\begin{verbatim}
## Error in plot(f_exp1, include = 24, main = "Simple Exponential Smoothing"): object 'f_exp1' not found
\end{verbatim}

\begin{verbatim}
## Error in plot(f_exp2, include = 24, main = "Holt-Winters with Trend"): object 'f_exp2' not found
\end{verbatim}

\begin{verbatim}
## Error in plot(f_exp3, include = 24, main = "Hold-Winters with Trend and Season"): object 'f_exp3' not found
\end{verbatim}

\hypertarget{modeling-trend-and-seasonal-components}{%
\chapter{Modeling Trend and Seasonal Components}\label{modeling-trend-and-seasonal-components}}

\hypertarget{trend-estimation}{%
\section{Trend Estimation}\label{trend-estimation}}

An important component of a time series is \emph{trend} that captures the long run evolution of the variable of interest. There are two types of trends:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Deterministic Trend: the underlying trend component is a \emph{known} function of time with \emph{unknown} parameters.
\item
  Stochastic Trend: the trend component is random.
\end{enumerate}

In this note we will focus on estimating and forecasting deterministic trend models. We will come back to stochastic trend later when we talk about stationarity property of a time series.

\hypertarget{parametrizing-a-deterministic-trend}{%
\subsection{Parametrizing a deterministic trend}\label{parametrizing-a-deterministic-trend}}

Whether or not there is deterministic trend in the data can be typically gleaned by simply plotting the time series over time. For example, Figure @ref(fig: ch5-figure1) below plots real GDP for the US at quarterly frequency. We can observe a positive time trend with real GDP increasing with time. In this section we will learn to \emph{fit} a function that captures this relationship accurately.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch5-figure1-1} 

}

\caption{Real GDP (2012 Chained Billions of Dollars)}\label{fig:ch5-figure1}
\end{figure}

\emph{Note: The variable time is denoted by \(t\) and it is artificially created to take value of 1 for the first period, 2 for the second period and so on.}

There are two commonly used functional forms for capturing a deterministic trend:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Polynomial Trend: We fit a polynomial of appropriate order to capture the time trend. For example,
  A. Linear trend:
  \begin{equation}
  y_t=\beta_0 +\beta_1 t +\epsilon_t
  \end{equation}
\end{enumerate}

B. Quadratic trend:
\begin{equation}
y_t=\beta_0 +\beta_1 t + \beta_2 t^2 +\epsilon_t
\end{equation}

In general, we can fit a polynomial of order \(q\):
\begin{equation}
y_t=\beta_0 + \sum_{i=1}^q \beta_i t^i +\epsilon_t
\end{equation}

We can estimate this model using the OLS. One of the key component here is to determine the \emph{right} order of the polynomial. We can begin with a large enough number for \(q\) and then select the appropriate order using AIC or BIC criterion.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Exponential or log-linear trend: In some cases we may want to use an exponential trend or equivalently a log-linear trend.
  \begin{align}
  y_t=e^{(\beta_0 +\beta_1 t +\epsilon_t)}\\
  equivalently\\
  log(y_t)=\beta_0 +\beta_1 t +\epsilon_t
  \end{align}
\end{enumerate}

Again we can estimate the above model using OLS.

\hypertarget{uses-of-the-deterministic-trend-model}{%
\subsection{Uses of the Deterministic Trend Model}\label{uses-of-the-deterministic-trend-model}}

Once we have finalized our deterministic trend model i.e., either a polynomial of a specific order or log-liner trend, we can use the estimated model for the following two purposes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Detrending our data: Suppose we would like to eliminate trend from our data. The residual from our final trend model is the \emph{detrended} time series.
\item
  Forecasting: We can also forecast our time series based on the estimated trend. For example, suppose our final model is a quadratic trend. The predicted value is given by:
\end{enumerate}

\begin{equation}
\widehat{y_t}=\widehat{\beta_0}+\widehat{\beta_1} t + \widehat{\beta_2} t^2
\end{equation}

Then, the 1-period ahead forecast for \(y_{t+1}\) can be obtained by solving:
\begin{equation}
\widehat{y_{t+1}}=\widehat{\beta_0}+\widehat{\beta_1} (t+1) + \widehat{\beta_2} (t+1)^2
\end{equation}

\hypertarget{application-estimating-a-polynomial-trend-for-u.s.-real-gdp}{%
\subsection{Application: Estimating a polynomial trend for U.S. Real GDP}\label{application-estimating-a-polynomial-trend-for-u.s.-real-gdp}}

We will now fit a polynomial trend to the US real GDP data that was presented in Figure \ref{fig:figure10}. We first estimate polynomials of different orders and select the optimal order determined by the lowest possible AIC/BIC. Table \ref{tab:ch5-table1}. shows these statistics for up to 4th order polynomial. We find that the lowest value occur at \(q=4\).

\begin{table}

\caption{\label{tab:ch5-table1}Optimal Order of the Polynomial}
\centering
\begin{tabular}[t]{ccc}
\toprule
order & AIC & BIC\\
\midrule
1 & 4869.687 & 4880.727\\
2 & 4242.910 & 4257.631\\
3 & 4210.658 & 4229.059\\
4 & 4137.175 & 4159.256\\
\bottomrule
\end{tabular}
\end{table}

Hence, our final trend model is:

\begin{equation}
y_t=\beta_0 +\beta_1 t + \beta_2 t^2 + \beta_3 t^3 + \beta_4 t^4 +\epsilon_t
\end{equation}

The estimated trend model is presented in Table \ref{tab:ch5-table2}.

\begin{table}

\caption{\label{tab:ch5-table2}Regression Results}
\centering
\begin{tabular}[t]{lcccc}
\toprule
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\midrule
(Intercept) & 1792.111 & 83.029 & 21.584 & 0\\
trend & 37.057 & 3.899 & 9.504 & 0\\
I(trend\textasciicircum{}2) & -0.229 & 0.054 & -4.255 & 0\\
I(trend\textasciicircum{}3) & 0.002 & 0.000 & 8.281 & 0\\
I(trend\textasciicircum{}4) & 0.000 & 0.000 & -9.199 & 0\\
\bottomrule
\end{tabular}
\end{table}

Using the estimated model, we can compute the detrended data as the residual and also forecast \(y_t\). Figure \ref{fig:ch5-figure2} below plots the detrended real GDP obtained as a residual from our trend model.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch5-figure2-1} 

}

\caption{Detrended Real GDP}\label{fig:ch5-figure2}
\end{figure}

Figure \ref{fig:ch5-figure3} shows the forecast of real GDP for next 8 quarters along with the 95\% confidence bands.

\begin{verbatim}
## Error in forecast(fit, h = 8): unused argument (h = 8)
\end{verbatim}

\begin{verbatim}
## Error in plot(fcast, include = 24, main = ""): object 'fcast' not found
\end{verbatim}

\hypertarget{seasonal-model}{%
\section{Seasonal Model}\label{seasonal-model}}

We now focus on the \emph{seasonal} component of a time series, i.e., that is periodic fluctuations that repeat themselves every season. For example, increase in ice cream sales during summer season. Just like trend component, such seasonal pattern could be \emph{deterministic} or \emph{stochastic}. In this chapter we will focus on estimating deterministic seasonal component.

In Figure \ref{fig:ch5-figure4} we plot housing starts in the U.S. The data is at monthly frequency and we can see a clear seasonal pattern. Housing starts seem to increase in spring and summer months. This is followed by a decline in fall and winter months.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch5-figure4-1} 

}

\caption{Housing Starts in U.S.}\label{fig:ch5-figure4}
\end{figure}

One option to deal with seasonality is to obtain seasonally adjusted data (or deseasonalized data) from the source itself. Alternatively, we can use decomposition method and appropriately filter out the seasonal component. However, if our objective is to explicitly model the seasonal component of a time series then we must work with non-seasonally adjusted data.

\hypertarget{regression-model-with-seasonal-dummy-variables}{%
\subsection{Regression Model with Seasonal Dummy Variables}\label{regression-model-with-seasonal-dummy-variables}}

One way to account for seasonal patterns in data is to add dummy variables for season. To avoid perfect multicollinearity, is there are \(s\) seasons, we can include \(s-1\) dummy variables. For example, for quarterly data, \(s=4\) and hence we need \(s-1=3\) dummy variables in our regression model. Formally, for quarterly data, the seasonal regression model is given by:

\begin{equation}
y_t= \beta_0 + \beta_1 D_{1t}+ \beta_2 D_{2t} + \beta_3 D_{3t} + \epsilon_t
\end{equation}

In the above regression model, \(D_1,D_2,\) and \(D_3\) are dummy variables that capture first three quarters of the year. For example, \(D_1=1\) for the first quarter and \(D_1=0\) otherwise. Similarly, \(D_2=1\) for the second quarter and \(D_2=0\) otherwise. In this example, we use the fourth quarter as the \emph{base group}.

The above model can be estimated using OLS. Again, we can use the residual from our estimated model as a measure of \emph{deseasonlized} data. We can also forecast the dependent variable based on the seasonal component only.

\hypertarget{application-seasonal-model-of-housing-starts}{%
\subsection{Application: Seasonal Model of Housing Starts}\label{application-seasonal-model-of-housing-starts}}

We now estimate a seasonal regression model for the housing starts data presented in Figure \ref{fig:ch5-figure4}. The data is at monthly frequency which implies we can have 12 possible seasons and hence would need 11 dummy variables in our regression model. Formally, we use January as the base group and include dummy variables for the last 11 months of the year:

\begin{equation}
y_t=\beta_0 + \sum_{i=2}^{12}\beta_i D_{it} + \epsilon_t
\end{equation}

Table \ref{tab:ch5-table3} presents the estimation results for this exercise. In Figure \ref{fig:ch5-figure5} we plot the forecast of housing starts for next 12 months using our estimated model, along with 95\% confidence bands.

\begin{table}

\caption{\label{tab:ch5-table3}Regression Results}
\centering
\begin{tabular}[t]{lcccc}
\toprule
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\midrule
(Intercept) & 87.347 & 4.272 & 20.447 & 0.000\\
season2 & 2.866 & 6.041 & 0.474 & 0.635\\
season3 & 31.369 & 6.041 & 5.193 & 0.000\\
season4 & 46.897 & 6.041 & 7.763 & 0.000\\
season5 & 52.942 & 6.066 & 8.728 & 0.000\\
\addlinespace
season6 & 51.873 & 6.066 & 8.551 & 0.000\\
season7 & 46.216 & 6.066 & 7.619 & 0.000\\
season8 & 44.250 & 6.066 & 7.295 & 0.000\\
season9 & 37.593 & 6.066 & 6.197 & 0.000\\
season10 & 41.584 & 6.066 & 6.855 & 0.000\\
\addlinespace
season11 & 20.606 & 6.066 & 3.397 & 0.001\\
season12 & 4.647 & 6.066 & 0.766 & 0.444\\
\bottomrule
\end{tabular}
\end{table}

\begin{verbatim}
## Error in forecast(fit, h = 12): unused argument (h = 12)
\end{verbatim}

\begin{verbatim}
## Error in plot(fcast, include = 24, main = ""): object 'fcast' not found
\end{verbatim}

\hypertarget{modeling-cycle}{%
\chapter{Modeling Cycle}\label{modeling-cycle}}

In this chapter we will focus on the cyclical component of a time series and hence focus on data that either has no trend and seasonal components, or data that is filtered to eliminate any trend and seasonality. One of the most commonly used method to model cyclicality is the \emph{Autogressive Moving Average (ARMA)}. This model has two distinct components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Autoregressive (AR) component}: the current period value of a time series variable depends on its past (lagged) observations. We use \(p\) to denote the \textbf{order} of the AR component and is the number of lags of a variable that directly affect the current period value. For example, a firm's production in the current period maybe impacted by past levels of production. If last year's production exceeded demand, the stock of unsold goods may be used to meet this period demand first, hence lowering the current period production.
\item
  \emph{Moving average (MA) component}: the current period value of a time series variable depends on current period \textbf{shock} as well as past shocks to this variable. We use \(q\) to denote the \textbf{order} of the MA component and is the number of past period shocks that affect the current period value of the variable of interest. For example, if the Federal Reserve Bank raises the interest in 2016, the effects of that policy shock may impact investment and consumption spending in 2017.
\end{enumerate}

Before we consider these time series model in details it is useful to discuss certain properties of time series that allow us a better understanding of these models.

\hypertarget{stationarity-and-autocorrelation}{%
\section{Stationarity and Autocorrelation}\label{stationarity-and-autocorrelation}}

\hypertarget{covariance-stationary-time-series}{%
\subsection{Covariance Stationary Time Series}\label{covariance-stationary-time-series}}

\begin{definition}[Covariance Stationary Time Series]
\protect\hypertarget{def:d7}{}{\label{def:d7} \iffalse (Covariance Stationary Time Series) \fi{} }
\end{definition}

A time series \(\{y_t\}\) is said to be a \emph{covariance stationary process} if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E(y_t)=\mu_y \quad \forall \quad t\)
\item
  \(Var(y_t)=\sigma_y^2 \quad \forall \quad t\)
\item
  \(Cov(y_t,y_{t-s})=\gamma(s) \quad \forall \quad s\neq t\)
\end{enumerate}

One way to think about stationarity is \emph{mean-reversion}, i.e, the tendency of a time series to return to its \emph{long-run} unconditional mean following a shock (or a series of shock). Figure @\ref(fig:ch6-figure1) below shows this property graphically.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch6-figure1-1} 

}

\caption{Reversion to mean}\label{fig:ch6-figure1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch6-figure2-1} 

}

\caption{Reversion to mean in practice}\label{fig:ch6-figure2}
\end{figure}

In practice however, you will not be able to visualize a mean-reverting stationary process this clearly. For example, in Figure \ref{fig:ch6-figure2} we plot real GDP growth for the U.S. which is a stationary process with a mean of 0.7\%. In this chapter we will only consider stationary time series data. Later on we will learn how to work with non-stationary data.

\hypertarget{correlation-vs-autocorrelation}{%
\subsection{Correlation vs Autocorrelation}\label{correlation-vs-autocorrelation}}

In statistics, correlation is a measure of relationship between two variables. In the time series setting, we can think of the current period value and the past period value of a variable as two \textbf{separate} variables, and compute correlation between them. Such a correlation, between current and lagged observation of a time series is called \textbf{serial correlation} or \textbf{autocorrelation}. In general, for a time series, \(\{y_t\}\), the autocorrelation is given by:

\begin{align}
    Cor(y_t,y_{t-s})=\frac{ Cov(y_t,y_{t-s})}{\sqrt{\sigma^2_{y_t} \times \sigma^2_{y_{t-s}}}}
        \end{align}
where \(Cov(y_t,y_{t-s})= E(y_t-\mu_{y_t})(y_{t-s}-\mu_{y_{t-s}})\) and \(\sigma^2_{y_t}=E(y_t-\mu_{y_t})^2\)

For a stationary time series, using the three conditions the \textbf{Autocorrelation Function (ACF)} denoted by \(\rho(s)\) is given by:

\begin{align}
    ACF(s) \ or \ \rho(s)=\frac{\gamma(s)}{\gamma(0)}
    \end{align}

Non-zero values of the ACF indicates presences of serial correlation in the data. Figure \ref{fig:ch6-figure3} shows the ACF for a stationary time series with positive serial correlation. If your data is stationary then the ACF should eventually converge to 0. For a non-stationary data, the ACF function will not decay over time.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch6-figure3-1} 

}

\caption{ACF for a Stationary Time Series}\label{fig:ch6-figure3}
\end{figure}

\hypertarget{partial-autocorrelation}{%
\subsection{Partial Autocorrelation}\label{partial-autocorrelation}}

\begin{definition}[Partial Auto Correlation Function (PACF)]
\protect\hypertarget{def:d9}{}{\label{def:d9} \iffalse (Partial Auto Correlation Function (PACF)) \fi{} }
\end{definition}

The ACF captures the relationship between the current period value of a time series and all of its past observations. It includes both direct as well as indirect effects of the past observations on the current period value. Often times it is of interest to measure the direct relationship between the current and past observations, \textbf{partialing} out all indirect effects. The \emph{partial autocorrelation function (PACF)} for a stationary time series \(y_t\) at lag \(s\) is the direct correlation between \(y_t\) and \(y_{t-s}\), after filtering out the linear influence of \(y_{t-1},\ldots,y_{t-s-1}\) on \(y_t\). Figure \ref{fig:ch6-figure4} below shows the PACF for a stationary time series where only one lag directly affects the time series in the current period.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{bookdown-demo_files/figure-latex/ch6-figure4-1} 

}

\caption{PACF for a Stationary Time Series}\label{fig:ch6-figure4}
\end{figure}

\hypertarget{lag-operator}{%
\subsection{Lag operator}\label{lag-operator}}

A \textbf{lag operator} denoted by \(L\) allows us to write ARMA models in a more concise way. Applying lag operator once moves the time index by one period; applying it twice moves the time index back by two period; applying it \(s\) times moves the index back by \(s\) periods.
\[ Ly_t=y_{t-1} \]
\[ L^2y_t=y_{t-2} \]
\[ L^3y_t=y_{t-3} \]
\[\vdots\]
\[ L^sy_t=y_{t-s} \]

\hypertarget{autoregressive-ar-model}{%
\section{Autoregressive (AR) Model}\label{autoregressive-ar-model}}

A \emph{stationary}time series \(\{x_t\}\) can be modeled as an AR process. In general, an AR(p) model is given by:

\begin{equation}
 y_t = \phi_0 +\phi_1 y_{t-1} + \phi_2 y_{t-2} + ...... + \phi_p y_{t-p}+\epsilon_t
 \end{equation}

Here \(\phi_i\) captures the effect of \(y_{t-i}\) on \(y_t\). The order of the AR process is not known apriori. It is common to use either AIC or BIC to determine the optimal lag length for an AR process.

Using the Lag operator, we can rewrite the above AR(p) model as follows:
\[ \Phi(L)y_t=\phi_0+\epsilon_t \]

where \(\displaystyle \Phi(L)\) is a polynomial of degree \(p\) in L:

\[ \Phi(L) = 1-\phi_1 L - \phi_2 L^2- \ldots\ldots\ldots\ldots -\phi_p L^p\]

For example, an AR(1) model can be written as:
\[y_t=\phi_0+\phi_1 y_{t-1} + \epsilon_t \Rightarrow  \Phi(L)y_t=\phi_0+\epsilon_t\]
where,
\[ \Phi(L) = 1-\phi_1 L \]

\textbf{Characteristic equation}: A characteristic equation is given by:

\[\Phi(L)=0\]

The roots of this equation play an important role in determining the dynamic behavior of a time series.

\hypertarget{unit-root-and-stationarity}{%
\subsection{Unit root and Stationarity}\label{unit-root-and-stationarity}}

For a time series to be stationary there should be no \textbf{unit root} in its \emph{characteristic equation}. In other words, all roots of the characteristic equation must fall outside the unit circle. Consider the following AR(1) model:
\[\Phi(L)y_t = \phi_0 + \epsilon_t\]

The characteristic equation is given by:
\[\Phi(L)=1-\phi_1L=0 \]

The root that satisfies the above equation is:
\[ L^*=\frac{1}{\phi_1}\]

For no unit root to be present, \(L^*>|1|\) which implies that \(|\phi_1|<1\).

Typically, for any AR process to be stationary, some restrictions will be imposed on the values of \(\phi_i's\), the coefficients of the lagged variables in the model.

\hypertarget{properties-of-an-ar1-model}{%
\subsection{Properties of an AR(1) model}\label{properties-of-an-ar1-model}}

A stationary AR(1) model is given by:
\[ y_t=\phi_0 +\phi_1 y_{t-1}+ \epsilon_t \quad ; \ \epsilon_t\sim WN(0, \sigma_\epsilon^2) \ and \  |\phi_1|<1\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\displaystyle \phi_1\) measures the persistence in data. A larger value indicates shocks to \(y_t\) dissipate slowly over time.
\item
  Stationarity of \(y_t\) implies certain restrictions on the AR(1) model.

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Constant long run mean: is the unconditional expectation of \(y_t\):
    \[ E(y_t) = \mu_y= \frac{\phi_0}{1-\phi_1}  \]
  \item
    Constant long run variance: is the unconditional variance of \(y_t\):
    \[ Var(y_t)=\sigma^2_y= \frac{\sigma^2_\epsilon}{1-\phi_1^2}\]
  \item
    ACF function:
    \[ \rho(s) = \phi_1^s\]
  \item
    PACF function:
    \begin{equation*}
      PACF(s) =
      \begin{cases}
    \phi_1 & \text{if  s=1}\\
    0 & \text{if s>1}
      \end{cases}
     \end{equation*}
  \end{enumerate}
\end{enumerate}

\hypertarget{estimating-an-ar-model}{%
\section{Estimating an AR model}\label{estimating-an-ar-model}}

When estimating the AR model we have two alternatives:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  OLS: biased (but consistent) estimates. Also, later on when we add MA components we cannot use OLS.
\item
  Maximum Likelihood Estimation (MLE): can be used to estimate AR as well as MA components
\end{enumerate}

\hypertarget{maximum-likelihood-estimation-mle}{%
\subsection{Maximum Likelihood Estimation (MLE)}\label{maximum-likelihood-estimation-mle}}

\begin{itemize}
\tightlist
\item
  MLE approach is based on the following idea:
\end{itemize}

\emph{what set of values of our parameters maximize the likelihood of observing our data if the model we have was used to generate this data.}

\textbf{Likelihood function}: is a function that gives us the probability of observing our data given a model with some parameters.

\hypertarget{likelihood-vs-probability}{%
\subsubsection{Likelihood vs Probability}\label{likelihood-vs-probability}}

Consider a simple example of tossing a coin. Let \(X\) denotes the random variable that is the outcome of this experiment being either heads or tails. Let \(\theta\) denote the probability of heads which implies \(1-\theta\) is the probability of obtaining tails. Here, \(\theta\) is our parameter of interest. Suppose we toss the coin 10 times and obtain the following data on \(X\):
\[X=\{H,H,H,H,H,H,T,T,T,T\}\]

Then, the probability of obtaining this sequence of X is given by:
\[Prob (X|\theta)=\theta^6 (1-\theta)^4\]

This is the probability distribution function the variable \(X\). As we change \(X\), we get a different probability for a given value of \(\theta\).

Now let us ask a different question. Once we have observed the sequence of heads and tails, lets call it our data which is fixed. Then, what is probability of observing this data, if our probability distribution function is given by the equation above? That gives us the likelihood function:

\[ L(\theta)=Prob(X|\theta)=\theta^6(1-\theta)^4\]

Note that with fixed \(X\), as we change \(\theta\) the likelihood of observing this data will change.

\textbf{This is an important point that distinguishes likelihood function from the probability distribution function. Although both have the same equation, the probability function is a function of the data with the value of the parameter fixed, while the likelihood function is a function of the parameter with the data fixed.}

\hypertarget{maximum-likelhood-estimation}{%
\subsubsection{Maximum Likelhood Estimation}\label{maximum-likelhood-estimation}}

Now we are in a position to formally define the likelihood function.

\begin{definition}
\protect\hypertarget{def:unnamed-chunk-4}{}{\label{def:unnamed-chunk-4} }Let \(X\) denotes a random variable with a given probability distribution function denoted by \(f(x_i|\theta)\). Let \(D=\{x_1, x_2,\dots,x_n\}\) denote a sample realization of \(X\). Then, the likelhood function, denoted by \(L(\theta)\) is given by:
\[L(\theta)=f(x_1,x_2,\dots,x_n|\theta)\]
\end{definition}

If we further assume that each realization of \(X\) is independent of the others, we get:
\[L(\theta)=f(x_1,x_2,\dots,x_n|\theta)=f(x_1|\theta)\times f(x_2|\theta) \times \dots \times f(x_n|\theta)\]

A mathematical simplification is to work with natural logs of the likelihood function, which assuming independently distributed random sample, gives us:

\[ lnL(\theta)=ln(f(x_1|\theta)\times f(x_2|\theta) \times \dots \times f(x_n|\theta))=\sum_{i=1}^{N}ln(f(x_i|\theta))\]

\begin{definition}
\protect\hypertarget{def:unnamed-chunk-5}{}{\label{def:unnamed-chunk-5} }The maximum likelihood estimator, denoted by \(\hat{\theta}_{MLE}\), maximizes the log likelihood function:
\[ \hat{\theta}_{MLE} \equiv arg \max_{\theta} lnL(\theta) \]
\end{definition}

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-6}{}{\label{exm:unnamed-chunk-6} }Compute maximum likelihood estimator of \(\mu\) of an indpendently distributed random variable that is normally distributed with a mean of \(\mu\) and a variance of \(1\):

\[ f(y_t|\mu)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (y_t-\mu)^2}\]

Solution: The log likelihood function is given by:

\[lnL= -Tln2\pi-\frac{1}{2}\sum_{t=1}^T(y_t-\mu)^2 \]

From the first order condition, we get
\[ \frac{\partial LnL}{\partial \mu}=\sum_{t=1}^T(y_t-\mu)=0\Rightarrow \hat{\mu}_{MLE}=\frac{\sum_{t=1}^T y_t}{T}\]
\end{example}

\hypertarget{mle-of-an-arp-model}{%
\subsection{MLE of an AR(p) model}\label{mle-of-an-arp-model}}

One complication we face in estimating an AR(p) model is that by definition the realizations of the variable are not independent of each other. As a result we cannot simplify the likelihood function by multiplying individual probability density functions to obtain the joint probability density function, i.e.,
\[ f(y_1,y_2,\dots,y_T|\theta) \neq f(y_1|\theta)\times f(y_2|\theta)\times \dots \times f(y_T|\theta)\]

Furthermore, as the order of AR increases, the joint density function we need to estimate becomes even more complicated. In this class we will focus on the method that divides the joint density into the product of conditional densities and density of a set of initial values. The idea comes from the conditional probability formula for two related events \(A\) and \(B\):

\[ P(A|B) =\frac{P(\text{A and B})}{P(B)} \Rightarrow P(\text{A and B}) = P(A|B)\times P(B) \]

In the time series context, I will explain this for a stationary AR(1) model. We know that in this model only last period observation directly affects the current period value. Hence, consider the first two observations of a stationary time series: \(y_1\) and \(y_2\). Then the joint density of these adjacent observations is given by,

\[ f(y_1,y_2;\theta)= f(y_2|y_1; \theta)\times f(y_1;\theta)\]

Similarly, for the first three observations we get:

\[ f(y_1,y_2,y_3;\theta)= f(y_3|y_2; \theta)\times f(y_2|y_1; \theta) \times f(y_1; \theta)\]

Hence, for \(T\) observations we get:

\[ f(y_1,y_2,y_3, ...,y_T; \theta)= f(y_T|y_{T-1};\theta)\times f(y_{T-1}|y_{T-2}; \theta)\times.... \times f(y_1; \theta)\]

The log-likelihood function is given by:

\[ ln \ L(\theta) = ln \ f(y_1;\theta) + \sum_{t=2}^{T} ln \ f(y_t|y_{t-1}; \theta)  \]

We can then maximize the above likelihood function to obtain an MLE estimator for the AR(1) model.

\hypertarget{selection-of-optimal-order-of-the-ar-model}{%
\subsection{Selection of optimal order of the AR model}\label{selection-of-optimal-order-of-the-ar-model}}

Note that apriori we do not know the order of the AR model for any given time series. We can determine the optimal lag order by using either AIC or BIC. The process is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Set \(p=p_{max}\) where \(p_{max}\) is an integer. A rule of thumb is to set
  \[p_{max}=integer\left[12\times \left(\frac{T}{100}\right)^{0.25}\right]\]
\item
  Estimate all AR models from \(p=1\) to \(p=p_{max}\).
\item
  Select the final model as the one with lowest AIC or lowest BIC.
\end{enumerate}

\hypertarget{forecasting-using-arp-model}{%
\subsection{Forecasting using AR(p) model}\label{forecasting-using-arp-model}}

Having estimated our AR(p) model with the optimal lag length, we can use the conditional mean to compute the forecast and conditional variance to compute the forecast errors. Consider an AR(1) model:

\[y_t=\phi_0+\phi_1 y_{t-1} +\epsilon_t\]

Then, the 1-period ahead forecast is given by:
\[f_{t,1}=E(y_{t+1}|\Omega_t)=\phi_0+\phi_1 y_t\]
Similarly, the 2-period ahead forecast is given by:
\[f_{t,2}=E(y_{t+2}|\Omega_t)=\phi_0+\phi_1 E(y_{t+1}|\Omega_t) =\phi_0+\phi_1f_{t,1}\]

In general, we can get the following recursive forecast equation for h-period's ahead:
\[f_{t,h}=\phi_0+\phi_1 f_{t,h-1}\]

Correspondingly, the h-period ahead forecast error is given by:
\[e_{t,h}=y_{t+h}- f_{t,h}=\epsilon_{t+h}+\phi_1 e_{t,h-1}\]

\begin{theorem}
\protect\hypertarget{thm:unnamed-chunk-7}{}{\label{thm:unnamed-chunk-7} }The h-period ahead forecast converges to the unconditional mean of \(y_t\), i.e., \[\lim_{h\to\infty} f_{t,h}=\mu_y=\frac{\phi_0}{1-\phi_1}\]
\end{theorem}

\begin{theorem}
\protect\hypertarget{thm:unnamed-chunk-8}{}{\label{thm:unnamed-chunk-8} }The variance of the h-period ahead forecast error converges to the unconditional variance of \(y_t\), i.e., \[\lim_{h\to\infty} Var(e_{t,h})=\sigma^2_y=\frac{\sigma^2_\epsilon}{1-\phi_1^2}\]
\end{theorem}

\hypertarget{moving-average-ma-model}{%
\section{Moving Average (MA) Model}\label{moving-average-ma-model}}

Another commonly used method for capturing the cyclical component of the time series is the \textbf{moving average (MA)} model where the current value of a time series linearly depends on current and past shocks. Formally, a \emph{stationary} time series \(\{y_t\}\) can be modeled as an MA(q) process:
\begin{equation}
  y_t = \theta_0 + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ...... + \theta_q \epsilon_{t-q}
    \end{equation}

Using lag operator, we can write this in more compact form as:

\[y_t = \theta_0 +\Theta(L) \epsilon_t\]

where \(\Theta(L)=1+\theta_1 L+ \theta_2 L^2+...+\theta_q L^q\) is lag polynomial of order \(q\).

Note that because each one of the current and past shocks are white noise processes, an MA(q) model is always stationary.

\hypertarget{invertibility-of-an-ma-process}{%
\subsection{Invertibility of an MA process}\label{invertibility-of-an-ma-process}}

Consider the following MA(1) process with\(\theta_0=0\) for simplicity:
\[y_t=\epsilon_t +\theta_1 \epsilon_{t-1}\]

Using the lag operator we can rewrite this equation as follows:

\[y_t= (1+\theta_1L)\epsilon_t \Rightarrow y_t(1+\theta_1 L) ^{-1}=\epsilon_t\]

Note that if \(|\theta_1|<1\), then we can use the Taylor series expansion centered at 0 and get:

\[(1+\theta_1 L)^{-1}=1-\theta_1 L+(\theta_1L)^2-(\theta_1L)^3+ (\theta_1L)^4-...... \]

Hence, an MA(1) can be rewritten as follows:

\[y_t (1-\theta_1 L+(\theta_1L)^2-(\theta_1L)^3+ (\theta_1L)^4-......)=\epsilon_t\]
\[\Rightarrow y_t -\theta_1 y_{t-1} +\theta_1^2y_{t-2}-\theta_1^3 y_{t-3}....=\epsilon_t\]

Rearranging terms, we get the \(AR(\infty)\) representation for an invertible MA(1) model:
\[y_t=-\sum_{i=1}^{\infty}(-\theta_1)^i \ y_{t-i}+\epsilon_t\]

\begin{definition}
\protect\hypertarget{def:unnamed-chunk-9}{}{\label{def:unnamed-chunk-9} }An MA process is invertible if it can be represented as a stationary \(AR(\infty)\).
\end{definition}

\hypertarget{properties-of-an-invertible-ma1}{%
\subsection{Properties of an invertible MA(1)}\label{properties-of-an-invertible-ma1}}

An invertible MA(1) model is given by:

\[ y_t = \theta_0 + \epsilon_t + \theta_1 \epsilon_{t-1} \quad ; \ \epsilon_t\sim WN(0, \sigma_\epsilon^2) \ and \  |\theta_1|<1\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Constant unconditional mean of \(y_t\):
  \[E(y_t)=\mu_y =\theta_0 \]
\item
  Constant unconditional variance of \(y_t\):
\end{enumerate}

\[Var(y_t)=\sigma^2_y=\sigma^2_\epsilon(1+\theta_1^2)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  ACF function:
  \begin{equation*}
    ACF(s) =
    \begin{cases}
   \frac{\theta_1}{1+\theta_1^2} & \text{if  s=1}\\
   0 & \text{if s>1}
    \end{cases}
   \end{equation*}
\item
  PACF function: using the invertibility it is evident that PACF of an MA(1) decays with \(s\).
\end{enumerate}

\hypertarget{forecast-based-on-maq}{%
\subsection{Forecast based on MA(q)}\label{forecast-based-on-maq}}

Like before, the h-period ahead forecast is the conditional expected value of the time series. Consider an MA(1) model:

\[y_t=\theta_0 +\epsilon_t + \theta_1 \epsilon_{t-1}\]

Then, the 1-period ahead forecast is given by:

\[f_{t,1}=E(y_{t+1}|\Omega_t)=\theta_0+ \theta_1 \epsilon_{t-1}\]

The h-period ahead forecast for \(h>1\) is given by:
\[f_{t,h}=E(y_{t+h}|\Omega_t)=\theta_0\]

In general, for an MA(q) model, the forecast for \(h>q\) is the long run mean \(\theta_0\). This is why we say that an MA(q) process has a memory of \emph{q} periods.

\hypertarget{armap-q}{%
\section{ARMA(p, q)}\label{armap-q}}

An ARMA model simply combines both AR and MA components to model the dynamics of a time series. Formally,

\begin{equation}
   y_t = \phi_0 +\phi_1 y_{t-1} + \phi_2 y_{t-2} + ...... + \phi_p y_{t-p}+\epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ...... + \theta_q \epsilon_{t-q}
   \end{equation}

Note that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimation is done by maximum likelihood method.
\item
  Optimal order for AR and MA components is selected using AIC and/or BIC.
\item
  The forecast of \(y_t\) from an ARMA(p,q) model will be dominated by the AR component for \(h>q\). To see this consider the following ARMA(1,1) model:
\end{enumerate}

\[y_t = \phi_0 +\phi_1 y_{t-1}+ \epsilon_t + \theta_1 \epsilon_{t-1}\]

Then, the 1-period ahead forecast is:
\[f_{t,1} = E(y_{t+1}|\Omega_t) = \phi_0 + \phi_1 y_t + \theta_1 \epsilon_{t-1}\]

Here both MA and AR component affect the forecast. But now consider the 2-period ahead forecast:

\[f_{t,2} = E(y_{t+2}|\Omega_t) = \phi_0 + \phi_1 f_{t,1}\]

Hence, no role is played by the MA component in determining the 2-period ahead forecast. For any \(h>1\) only the AR component affects the forecast from this model.

\hypertarget{integrated-arma-or-arimapdq}{%
\section{Integrated ARMA or ARIMA(p,d,q)}\label{integrated-arma-or-arimapdq}}

Thus far we have assumed that our data is stationary. However, often we may find that this assumption is not supported in practice. In such a case we need to tranform our data appropriately before estimating an ARMA model. The procedure can be summarized as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Determine whether there is a unit root in data or not. Presence of unit root indicates non-stationarity. We will use Augmented Dickey-Fuller (ADF) test for this purpose.
\item
  If data is non-stationary, then we need to appropriately transform our data to make it stationary.
\item
  Once we have obtained a stationary transformation of our original data, we can proceed and estimate the ARMA model as before.
\end{enumerate}

\hypertarget{trend-stationary-vs-difference-stationary-time-series}{%
\section{Trend Stationary vs Difference Stationary Time Series}\label{trend-stationary-vs-difference-stationary-time-series}}

There are two types of time series we often encounter in real world:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Trend-stationary: a time-series variable is non-stationary becuase it has a deterministic trend. Once we detrend our data then it will become stationary. In this case the appropriate transformation is to estimate a trend model and then use the residual as the detrended stationary data. For example, suppose our data has a linear trend given by:
\end{enumerate}

\[y_t = \beta_0 +\beta_1 t +\epsilon_t\]

Then the OLS residual from this model, \(e_t=y_t-\hat{y_t}\) is the detrended \(y_t\) which will be stationary. Hence, we will estimate an ARMA(p,q) model using this detrended variable.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Difference-stationary: a time-series variable is non-stationary because it contains a stochastic trend. Here, the transformation requires us to difference the orignial data until we obtain a stationary time series. Let \(d\) denote the minimum number of differences needed to obtain a stationary time series:
\end{enumerate}

\[\Delta_d \ y_t=(1-L)^d \ y_t\]

In this case, we say that \(y_t\) is intergrated of order \(d\) or more formally, \(y_t\) is an \(I(d)\) process. Hence, for \(d=1\) we obtain an \(I(1)\) process implying that:

\[\Delta_1 \ y_t=(1-L)^1 \ y_t=y_t-y_{t-1} \quad  \text{is stationary}\]

In otherwords, the first difference of an I(1) process is stationary. Similarly for \(d=2\), we obtain an \(I(2)\) process where second difference will be stationary and so forth.

\hypertarget{testing-for-a-unit-root}{%
\section{Testing for a unit root}\label{testing-for-a-unit-root}}

Consider the following AR(1) model with no trend and intercept:

\[y_t=\phi_1 y_{t-1} +\epsilon_t  \ quad ; \epsilon_t\sim WN(0,\sigma^2_\epsilon)\]

We know that if \(\phi_1=1\) we have a unit root in this data. Lets subtract \(y_{t-1}\) from both sides and rewrite this model as:

\[y_t-y_{t-1}= (\phi_1-1)y_{t-1}+\epsilon_t \]

Define \(\rho=phi_1-1\). Then, we get:
\[\Delta y_t= \rho \ y_{t-1}+\epsilon_t \]

We can now estimate the above model and carry out the following test known as the Dickey-Fuller (DF) test:

\[H_0: \rho=0 \]
\[H_A: \rho<0\]

If the null hypothesis is not rejected, then we do not have sample evidence against the statement that \(\rho=0 \Rightarrow \phi_1=1\). Hence, we conclude that there is no evidence against the statement that there is unit root in the data. In contrast, if we reject the null hypothesis, then we can conclude that there is no unit root and hence the data is stationary.

The t- statistic is for the above test is denoted by \(\tau_1\) and is given by:
\[\tau_1 =\frac{\hat{\rho}}{se(\hat{\rho})}\]

Under the null hypothesis this test statistic follows the DF-distribution and the critical values are provided in most statistical softwares. Given that this is a left-tail test, the decision rule is that if the test statistic is less than the critical value then we reject the null hypothesis.

There are two issues we face when implementing this test in practice:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  First, the above procedure assumes that there is no intercept and trend in the data. In real world, we cannot make that assumption and must extend the test procedure to accomodate a non-zero intercept and trend. Hence, we have the following two additional versions of the DF test:

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Constant and no trend model: Here our AR(1) model is
  \end{enumerate}

  \[\Delta y_t= \phi_0 + \rho \ y_{t-1}+\epsilon_t \]

  Now we can do two possible tests. The first test is that of the unit root:

  \[H_0: \rho=0 \]
  \[H_A: \rho<0\]

  The t statistic for this test is denoted by \(\tau_2\) and is given by:
  \[\tau_2 =\frac{\hat{\rho}}{se(\hat{\rho})}\]

  If the test statisitic is less than the critical value, we reject the null.

  The second test we can do is:
  \[H_0: \rho=\phi_0=0 \]
  \[H_A: Not \ H_0\]
  The test statistic for this test is denoted by \(\phi_1\). If the test statistic exceeds the critical value then we reject the null.

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
    Constant and linear trend model: Here our AR(1) model is
  \end{enumerate}

  \[\Delta y_t= \phi_0 + \beta \ t+ \rho \ y_{t-1}+\epsilon_t \]

  Now we can do three possible tests. The first test is that of the unit root;
  \[H_0: \rho=0 \]
  \[H_A: \rho<0\]
  The t statistic for this test is denoted by \(\tau_3\) and is given by:

  \[\tau_3 =\frac{\hat{\rho}}{se(\hat{\rho})}\]

  If the test statisitic is less than the critical value, we reject the null.

  The second test is:
  \[H_0: \rho=\phi_0=\beta=0 \]
  \[H_A: Not \ H_0\]
  The test statistic for this test is denoted by \(\phi_2\). If the test statistic exceeds the critical value then we reject the null.

  Finally the third test is:
  \[H_0: \rho=\beta=0 \]
  \[H_A: Not \ H_0\]
  The test statistic for this test is denoted by \(\phi_3\). If the test statistic exceeds the critical value then we reject the null.
\item
  Second, we only have allowed for AR(1). We need to extend the above testing procedure for higher order AR models. The Augmented DF (ADF) test allows for higher order lags in testing for a unit root. For example, the model with an intercept, trend, and \(p\) lags is given by:
  \[\Delta y_t= \phi_0 + \beta \ t+ \rho \ y_{t-1}+\sum_{i=2}^p\delta_i  y_{t-i}+\epsilon_t  \quad where \ \rho=\sum_{i=1}^p \phi_i-1\]
\end{enumerate}

\hypertarget{testing-for-unit-root-in-usdcad-exchange-rate}{%
\subsection{Testing for unit root in USD/CAD exchange rate}\label{testing-for-unit-root-in-usdcad-exchange-rate}}

In this application we will test for unit root in US-Canada exchange rate. For this purpose we work with monthly data from Jan 1971 through Oct 2018. Below I show the results for 3 models using the \textbf{urca} package in R.

\begin{verbatim}
## 
## ############################################### 
## # Augmented Dickey-Fuller Test Unit Root Test # 
## ############################################### 
## 
## Test regression none 
## 
## 
## Call:
## lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.069649 -0.008966  0.000260  0.010369  0.124935 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## z.lag.1    0.0002568  0.0005774   0.445    0.657    
## z.diff.lag 0.2751721  0.0396745   6.936 1.07e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.01729 on 588 degrees of freedom
## Multiple R-squared:  0.07642,    Adjusted R-squared:  0.07328 
## F-statistic: 24.33 on 2 and 588 DF,  p-value: 7.068e-11
## 
## 
## Value of test-statistic is: 0.4447 
## 
## Critical values for test statistics: 
##       1pct  5pct 10pct
## tau1 -2.58 -1.95 -1.62
\end{verbatim}

\begin{verbatim}
## 
## ############################################### 
## # Augmented Dickey-Fuller Test Unit Root Test # 
## ############################################### 
## 
## Test regression drift 
## 
## 
## Call:
## lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.067835 -0.009434 -0.000696  0.009995  0.123391 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  0.010205   0.005344   1.910   0.0567 .  
## z.lag.1     -0.007944   0.004333  -1.833   0.0672 .  
## z.diff.lag   0.278600   0.039626   7.031 5.73e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.01725 on 587 degrees of freedom
## Multiple R-squared:  0.08083,    Adjusted R-squared:  0.0777 
## F-statistic: 25.81 on 2 and 587 DF,  p-value: 1.805e-11
## 
## 
## Value of test-statistic is: -1.8334 1.9227 
## 
## Critical values for test statistics: 
##       1pct  5pct 10pct
## tau2 -3.43 -2.86 -2.57
## phi1  6.43  4.59  3.78
\end{verbatim}

\begin{verbatim}
## 
## ############################################### 
## # Augmented Dickey-Fuller Test Unit Root Test # 
## ############################################### 
## 
## Test regression trend 
## 
## 
## Call:
## lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.067878 -0.009455 -0.000499  0.009920  0.123073 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  1.024e-02  5.349e-03   1.914   0.0561 .  
## z.lag.1     -8.355e-03  4.477e-03  -1.866   0.0625 .  
## tt           1.591e-06  4.308e-06   0.369   0.7121    
## z.diff.lag   2.789e-01  3.966e-02   7.031 5.72e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.01726 on 586 degrees of freedom
## Multiple R-squared:  0.08105,    Adjusted R-squared:  0.07634 
## F-statistic: 17.23 on 3 and 586 DF,  p-value: 9.866e-11
## 
## 
## Value of test-statistic is: -1.8663 1.3253 1.7464 
## 
## Critical values for test statistics: 
##       1pct  5pct 10pct
## tau3 -3.96 -3.41 -3.12
## phi2  6.09  4.68  4.03
## phi3  8.27  6.25  5.34
\end{verbatim}

For the first model with no constant and trend, the test statistic \(\tau_1= 0.2469\) and the 5\% critical value is -1.95. For the second model with a constant, the test statistics is \(\tau_2= -1.9315\) and the 5\% critical value is -2.86. Finally, for the third model with a trend, the test statistic \(\tau_3= -1.8839\) and the 5\% critical value is -3.41. In each, because the test statistic is greater than the critical value, we do not reject the null hypothesis and conclude there is unit root.

\hypertarget{box-jenkins-method-for-estimating-arimapdq}{%
\section{Box-Jenkins Method for estimating ARIMA(p,d,q)}\label{box-jenkins-method-for-estimating-arimapdq}}

Box-Jenkins is a three-step procedure for finding the best fitting ARIMA(p,d,q) for a non-statinary time series.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Model identification: here we determine the order of integration \(d\), and the optimal number of AR and MA components, \(p\) and \(q\) respectively.

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \item
    To determine \(d\), we conduct ADF test on successive differences of the original time series. The order of integration is the number times we difference our data to obtain stationarity.
  \item
    This is followed by estimating ARMA model for different combinations of \(p\) and \(q\). The optimal structure is chosen using either AIC or BIC.
  \end{enumerate}
\item
  Parameter estimation: we estimate the identified model from the previous step using ML estimation.
\item
  Model Evaluation: mostly showing that the residuals from the optimal model is a white noise process. We can do this by using the Breusch-Godfrey LM test of serial correlation for the residuals. If residuals from the final model are white noise then there should be no serial correlation.
\end{enumerate}

\hypertarget{vector-autoregression-var-model}{%
\chapter{Vector Autoregression (VAR) Model}\label{vector-autoregression-var-model}}

Thus far our analysis has been limited to a univariate time series. Often one maybe interested in understanding the dynamics of multiple time series variables. Vector Autoregression (VAR) model is one of the most commonly used model for this purpose. Suppose you have \(N\) time series variables in your sample Then a VAR model is a system of \(N\) linear equations where each variable is modeled as a linear function of its own lags and current and past values of the remaining \(N-1\) variables. In this sense a VAR model combines and extends the the AR model and distributed lag model we covered earlier.

There are two main ways we can specify a VAR model, each distinguished by the treatment of contemporaneous effects among variable included in the model. To make each type's main features salient, suppose that we have three variables in our model: inflation (denoted by \(\pi\)), unemployment (denoted by \(u\)), and the federal fund rate (denoted by \(i\)). In the remainder of this chapter will use this 3-variable system as our example.

\hypertarget{reduced-form-var}{%
\section{Reduced-form VAR}\label{reduced-form-var}}

In a reduced-form VAR we abstract away from any contemporaneous linkages among variables in the model and hence in that sense take an \textbf{atheoretical} approach to estimating dynamic relationships among these variables. Specifically, we assume that each variable in the model is a function of its own lags and lags of remaining variables in the system. For example, using our 3 variables, a reduced-form VAR(1) is given by:

\[\pi_t = a_{10} + a_{11} \pi_{t-1} + a_{12} u_{t-1} + a_{13} i_{t-1} + e_{1t}\]
\[u_t = a_{20} + a_{21} \pi_{t-1} + a_{22} u_{t-1} + a_{23} i_{t-1} + e_{2t}\]
\[i_t = a_{30} + a_{31} \pi_{t-1} + a_{32} u_{t-1} + a_{33} i_{t-1} + e_{3t}\]

Note that:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  The above system of 3-equations can easily be estimated as it only includes lagged data for each variable as independent variable, which by definition is pre-determined. Hence, we can estimate each equation separately by OLS.
\item
  In the above example we use 1 lag for simplicity. In practice the number of lagged values to include in each equation can be determined using AIC or BIC.
\item
  Each equation will provide a forecast for that particular variable. Hence, in our example we can generate forecasts for all three variables in our model.
\item
  One important drawback of the reduced-form VAR is that it cannot be used for structural inference and policy analysis. For example, suppose we are want to find out what will be the effect of raising the ffr by 50 basis points on inflation and unemployment? To answer this question, we need the shocks in each equation to be independent of each other. Only then, for example, we can say that \(e_{3t}\) represents shock to the FFR alone. However, in the reduced-form VAR, because each variable is related to each other, shocks in each equation are also correlated to each other.
\end{enumerate}

\hypertarget{structural-var}{%
\section{Structural VAR}\label{structural-var}}

The reduced-form VAR specified above is \textbf{atheoretical} in the sense that it does not impose any economic structure on the relationships between different variables. As a result a reduced-form model cannot be used to uncover causal relationships between correlated variables. A \textbf{structural VAR} model allows each variable to depend on past as well as current values of other variables in the system. As a result, a structural VAR adds contemporaneous linkages among variables included in the model. Continuing with our example, a structural version of the 3-variable VAR can be written as follows:

\[\pi_t = \beta_{11} + \beta_{12} u_t + \beta_{13} i_t + \phi_{11} \pi_{t-1} + \phi_{12} u_{t-1} + \phi_{13} i_{t-1} + \epsilon_{1t}\]
\[u_t = \beta_{21} + \beta_{22} \pi_t+ \beta_{23} i_t+\phi_{21} \pi_{t-1} + \phi_{22} u_{t-1} + \phi_{23} i_{t-1} + \epsilon_{2t}\]
\[i_t = \beta_{31} + \beta_{32}\pi_t + \beta_{33}u_t+ \phi_{31} \pi_{t-1} + \phi_{32} u_{t-1} + \phi_{33} i_{t-1} + \epsilon_{3t}\]

There are three key things to note here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In each equation we now have contemporaneous linkages among variables. For example, in the first equation, \(\beta_{12}\) and \(\beta_{13}\) capture the effect of changes in period \(t\) values of unemployment and ffr on the period \(t\) value of inflation.
\item
  By definition the error terms are uncorrelated across equations and hence each error term can be interpreted as representing a pure shock to that particular variable. For example, \(\epsilon_{3t}\) represents shocks to the ffr often caused by the monetary policy actions of the Fed.\\
\item
  OLS estimation of each equation separately would provide biased estimates as there is a simultaneity bias inherent in the specification of the structural VAR. For example, in the inflation equation, inflation depends on current period unemployment and interest rate. But, in the unemployment equation, unemployment depends on current period inflation and interest rate as well. This kind of reverse causality causes OLS estimator to be biased.
\end{enumerate}

Consequently, a structural VAR model cannot be estimated using OLS. In order to process we need to impose some kind of identifying restrictions on the structural VAR model to deal with the contemporaneous linkages among variables. Such restrictions often come from economic theory and may involve the entire model or a just a single equation. However, to conduct structural inference we have to use economic theory to afford structural interpretation to shocks of our VAR model.

\hypertarget{cholesky-decomposition-and-recursive-var-model}{%
\section{Cholesky Decomposition and Recursive VAR model}\label{cholesky-decomposition-and-recursive-var-model}}

One solution that falls in between the reduced-form VAR with no structural interpretation and the structural VAR with full set of contemporaneous linkages is the \textbf{recursive VAR} model. In this specification, the error term in each equation is assumed to be uncorrelated with the error term in the preceding equations. This is accomplished by assuming some sort of \textbf{ordering} among variables in the VAR model so that only some variables contemporaneously affect others. For example, suppose we use the following ordering: inflation, unemployment, and the federal fund rate (ffr). This ordering will imply the following recursive VAR model:

\[\pi_t = \beta_{11} + \phi_{11} \pi_{t-1} + \phi_{12} u_{t-1} + \phi_{13} i_{t-1} + \epsilon_{1t}\]
\[u_t = \beta_{21} + \beta_{22} \pi_t+\phi_{21} \pi_{t-1} + \phi_{22} u_{t-1} + \phi_{23} i_{t-1} + \epsilon_{2t}\]
\[i_t = \beta_{31} + \beta_{32}\pi_t + \beta_{33}u_t+ \phi_{31} \pi_{t-1} + \phi_{32} u_{t-1} + \phi_{33} i_{t-1} + \epsilon_{3t}\]

Note that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The ordering we assumed: inflation, unemployment, and ffr, implies that the first variable (inflation) has no contemporaneous linkage with unemployment and ffr. The second variable (unemployment) is affected by contemporaneous inflation and the third variable (ffr) is affected by both contemporaneous inflation and unemployment. The main implication of this ordering is that \(cor(\epsilon_{1t},\epsilon_{2t})=0\), \(cor(\epsilon_{2t},\epsilon_{3t})=0\), and \(cor(\epsilon_{1t},\epsilon_{3t})=0\).
\item
  In practical implementations of the recursive VAR model, it is common to first estimate the reduced-form VAR model using OLS for each equation separately. Then, we decompose the covariance matrix to reduced-form residuals using \textbf{Cholesky decomposition} method to obtain uncorrelated error structure consistent with the recursive VAR model specified above. This can be used to investigate how shocks to one variable affect other variables in the system.
\item
  If we change the ordering, we will get different results. In a model with \(N\) variables, there are N! possible orderings. So in our example, there are 3!= 6 possible alternative specifications for the VAR model. Hence, this approach requires us to judiciously choose the ordering of the variables.
\end{enumerate}

\hypertarget{impulse-response-function-forecast-error-variance-decomposition-and-granger-causality}{%
\section{Impulse Response Function, Forecast Error Variance Decomposition, and Granger Causality}\label{impulse-response-function-forecast-error-variance-decomposition-and-granger-causality}}

In addition to estimating each equation and providing a forecast of all variables in the system, there are two additional outputs produced by a VAR model which can be used for both descriptive and inference purposes. This analysis requires us to either use a recursive VAR model or impose some restrictions based on economic theory so that each shock can be interpreted as a shock to that particular variable. In this chapter we will use the recursive VAR structure when discussing impulse response functions and variance decomposition.

\hypertarget{impulse-response-function}{%
\subsection{Impulse response function}\label{impulse-response-function}}

Suppose we are interested in finding out how shock to one variable in the system at time \(t\) affects the dynamics of other variables. For example, suppose there is unanticipated increase in the ffr caused by the actions of the Fed. What would be the dynamic response of inflation and unemployment to this monetary policy shock? Using the recursive VAR, we can interpret the shock to the FFR (the last variable in our ordering) as a pure interest rate shock because it is by construction uncorrelated with shocks to inflation and unemployment. The \textbf{impuse response} produced by the VAR model traces out the response of current and future values of unemployment and inflation to one-unit increase in the error term of the ffr equation, holding error terms in inflation and unemployment equation at zero. We can study both the \textbf{impact effect} as well as the \textbf{cumulative effect} of such a shock to ffr on unemployment and inflation.

\hypertarget{forecase-error-variance-decomposition}{%
\subsection{Forecase Error Variance Decomposition}\label{forecase-error-variance-decomposition}}

Another useful output from an estimated VAR model is the \textbf{forecast error variance decomposition (fevd)}. Here, we can address the following question: following a shock to one of the variables in the VAR model what percentage of the variance in the forecast error of other variables in the model can be attributed to this particular shock? For example, what percentage of the forecast error variance of inflation can be attributed to the shock in inflation, shock in unemployment, and shock to ffr. In this sense the forecast error variance decomposition measures the amount of each variable in the VAR model contributes to the other variables in the autoregression.

\hypertarget{granger-causality}{%
\subsection{Granger causality}\label{granger-causality}}

Another important analysis that one can conduct using a VAR model is \textbf{Granger-causality}. As a bi-variate concept, Granger causality means that lags of one variable improves our capacity to predict another variable. For example, if unemployment \textbf{Granger-causes} inflation, then lags of unemployment have non-zero coefficients in the reduced-form inflation equation. Formally, consider the following regression:

\[\pi_t=\beta_0 + \sum_{i=1}^{P} \beta_i u_{t-i} \sum_{i=1}^{P} \delta_i \pi_{t-i} + \epsilon_t\]

To test whether unemployment Granger causes inflation we can conduct the following hypothesis test:

\[H_0: \beta_1=\beta_2=...=\beta_p=0\]
\[H_A: \text{Not} \ H_0\]

This is an F-test and rejection of the null hypothesis indicates unemployment Granger-causes inflation.

\hypertarget{application-effect-of-monetary-policy-on-inflation-and-unemployment}{%
\section{Application: Effect of monetary policy on inflation and unemployment}\label{application-effect-of-monetary-policy-on-inflation-and-unemployment}}

The Federal reserve bank (The Fed) in the U.S. has a dual mandate: to maintain low inflation and low unemployment. The monetary policy stance of the Fed is expressed in terms of the federal funds rate (ffr). In this section we learn how to estimate a VAR model with three variables: inflation (\(\pi_t\)), unemployment (\(u_t\)), and interest rate (\(i_t\)). For this purpose we will use quarterly data on these variables from 1960Q1 through 2019Q2. We will use the Fred Stat website and download data on these variables:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Seasonally quarterly GDP deflator (GDPDEF): \url{https://fred.stlouisfed.org/series/GDPDEF} ht
  We define inflation as follows:
\end{enumerate}

\[\pi_t = 400 \times ln\left(\frac{P_t}{P_{t-1}} \right)\]

Here \(P_t\) denotes GDP deflator for quarter \(t\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Seasonally adjusted Civilian unemployment rate (UNRATE): \url{https://fred.stlouisfed.org/series/UNRATE}
\item
  Effective federal funds rate (FEDFUNDS): \url{https://fred.stlouisfed.org/series/FEDFUNDS}
\end{enumerate}

The first step in any VAR analysis is to ensure that all variables in the model are stationary. Although not reported here, I find that inflation is stationary whereas unemployment and interest rate are first difference stationary. Accordingly, we will use inflation and first differences of unemployment and interest rate in our VAR analysis. The second step in the VAR analysis is to determine the optimal number of lags. Table \ref{tab:ch7table1} presents different information criteria for up to 8 lags. Using SC criterion we select optimal lags to be 2.

\begin{verbatim}
## Error in VARselect(yvector, lag.max = 8, type = c("const")): 
## NAs in y.
\end{verbatim}

\label{tab:ch7table1} Lag selection for VAR model

Hence, our reduced-form VAR model is given by:

\[ \pi_t = a_{10} + a_{11} \pi_{t-1} + a_{12} u_{t-1} + a_{13} i_{t-1} + b_{11} \pi_{t-2} + b_{12} u_{t-2} + b_{13} i_{t-2} + e_{1t}\]
\[ u_t = a_{20} + a_{21} \pi_{t-1} + a_{22} u_{t-1} + a_{23} i_{t-1}+ b_{21} \pi_{t-2} + b_{22} u_{t-2} + b_{23} i_{t-2} + e_{2t}\]
\[ i_t = a_{30} + a_{31} \pi_{t-1} + a_{32} u_{t-1} + a_{33} i_{t-1}+ b_{31} \pi_{t-2} + b_{32} u_{t-2} + b_{33} i_{t-2}+ e_{3t}\]

We can estimate the above model using OLS for each equation separately. It is a common practice to report the variance decomposition and impulse response from an estimated VAR model instead of reported the estimated coefficients for each equation. I will follow that convention and only report the analysis based on the estimated VAR model. The table below shows the results of Granger-causality test using optimal lag of 2. We find that inflation Granger-causes ffr and unemployment as indicated by a low p-value for the null hypothesis of no Granger-causality. Similarly, unemployment Granger-causes inflation and ffr. Finally, ffr also Granger-cause other two variables.

\begin{verbatim}
## Error in VAR(yvector, p = 2, type = c("const")): 
## NAs in y.
\end{verbatim}

\begin{verbatim}
## Error in causality(varmodel, cause = "inf"): object 'varmodel' not found
\end{verbatim}

\begin{verbatim}
## Error in causality(varmodel, cause = "u"): object 'varmodel' not found
\end{verbatim}

\begin{verbatim}
## Error in causality(varmodel, cause = "ffr"): object 'varmodel' not found
\end{verbatim}

Next we present the impulse responses for the recursive VAR structure where we use the following ordering: inflation, unemployment, and interest rate. For brevity, I only show the effect of an unexpected increase in ffr on all three variables as this shock propagates through the recursive VAR structure using the estimated coefficients from the reduced-form VAR model. From Figures 7.1 through 7.3 we observe a persistent positive effect of higher interest rate on both unemployment and inflation that fades over time.

\begin{verbatim}
## Error in irf(varmodel, impulse = "ffr", n.ahead = 24, response = c("inf")): object 'varmodel' not found
\end{verbatim}

\begin{verbatim}
## Error in irf(varmodel, impulse = "ffr", n.ahead = 24, response = c("u")): object 'varmodel' not found
\end{verbatim}

\begin{verbatim}
## Error in irf(varmodel, impulse = "ffr", n.ahead = 24, response = c("ffr")): object 'varmodel' not found
\end{verbatim}

\begin{verbatim}
## Error in plot(impulse_inf): object 'impulse_inf' not found
\end{verbatim}

\begin{verbatim}
## Error in plot(impulse_u): object 'impulse_u' not found
\end{verbatim}

\begin{verbatim}
## Error in plot(impulse_ffr): object 'impulse_ffr' not found
\end{verbatim}

Next we present the forecast error variance decomposition results that decomposes the variance in forecast error of a variable into own contribution and contribution of shocks to other variables. In Table below we show this analysis at 4 forecast horizons, namely, 1 period, 4 period, 8 period, and 12 periods.
They suggest considerable interaction among the variables. For example, at the 12-quarter
horizon, 63 percent of the forecast error variance in for ffr can be attributed to the inflation and unemployment shocks in the recursive VAR. In contrast, only 15 percent of the forecast error variance for inflation comes from shocks to ffr and unemployment.

\begin{verbatim}
## Error in fevd(varmodel, n.ahead = 12): object 'varmodel' not found
\end{verbatim}

\begin{verbatim}
## Error in rbind(fd$inf[1, ], fd$inf[4, ], fd$inf[8, ], fd$inf[12, ]): object 'fd' not found
\end{verbatim}

\begin{verbatim}
## Error in rbind(fd$u[1, ], fd$u[4, ], fd$u[8, ], fd$u[12, ]): object 'fd' not found
\end{verbatim}

\begin{verbatim}
## Error in rbind(fd$ffr[1, ], fd$ffr[4, ], fd$ffr[8, ], fd$ffr[12, ]): object 'fd' not found
\end{verbatim}

\begin{verbatim}
## Error in cbind(horizon, fd_inf): object 'fd_inf' not found
\end{verbatim}

\begin{verbatim}
## Error in cbind(horizon, fd_u): object 'fd_u' not found
\end{verbatim}

\begin{verbatim}
## Error in cbind(horizon, fd_ffr): object 'fd_ffr' not found
\end{verbatim}

\begin{verbatim}
## Error in kable(fd_inf, digits = 3, caption = " Panel A-FEVD for Inflation"): object 'fd_inf' not found
\end{verbatim}

\begin{verbatim}
## Error in kable(fd_u, digits = 3, caption = "Panel B- FEVD for Unemployment"): object 'fd_u' not found
\end{verbatim}

\begin{verbatim}
## Error in kable(fd_ffr, digits = 3, caption = "Panel C- FEVD for FFR"): object 'fd_ffr' not found
\end{verbatim}

Finally, we can use our reduced-form VAR estimation to produce forecasts for each variable in the model.
Figure \ref{fig:ch7fig1} below provides forecast for next 8 quarters from 2019Q3 through 2021Q2. These forecasts can be compared with univariate forecast for each variable using ARIMA or some other method to assess the improvement in forecast accuracy due to estimating a VAR model.

\begin{verbatim}
## Error in forecast(varmodel, h = 8): unused argument (h = 8)
\end{verbatim}

\begin{verbatim}
## Error in plot(fcast, include = 24): object 'fcast' not found
\end{verbatim}

\hypertarget{modeling-volatility}{%
\chapter{Modeling Volatility}\label{modeling-volatility}}

Consider a stock market analyst who is interested in finding out whether to invest in a particular stock or not. What kind of variables will govern her choice? If we focus on modern portfolio theory (MPT), a benchmark model in finance, a rational investor only cares about two variables:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  expected return from a financial asset such as a stock
\item
  risk or volatility underlying this asset
\end{enumerate}

Let \(P_t\) denotes the adjusted closing price for a stock traded in the market. The continually compounded return of this stock is given by:

\[y_t=[log(P_t)-log(P_{t-1})]\times 100\]

According to the MPT as an investor you should care about:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E(y_{t+h}|\Omega_t)\): this is the expected return on the asset, given information at time t
\item
  \(Var(y_{t+h}|\Omega_t)\): this is the expected volatility or risk underlying this asset, given the information at time t.
\end{enumerate}

Note that even though this example is using stock market as the motivation, the same is true for any other time series of interest. I use stock market as an example because here the variance has an intuitive appeal as risk underlying an asset.

Thus far our discussion has focused on modeling the conditional mean of a given time series. For example, consider the following \(ARMA(1,1)\) model for the stock market return:

\[y_t=\phi_0 + \phi_1 y_{t-1} + \theta_1 \epsilon_{t-1} + \epsilon_t ; \quad \epsilon_t\sim WN(0, \sigma^2_\epsilon )\]

From this model, we can obtain the forecast for \(y_{t+h}\) as the conditional mean of \(y_t\) given the information available at time \(t\):

\[f_{t,h}=E(y_{t+h}|\Omega_t)\]

In this sense ARMA models are inherently models for the conditional mean of a stationary time series. Going back to our stock market example, this ARMA model can give us information about how the expected return on a stock will evolve over time.

An important assumption we make in estimating this model is that the error term is homoscedastic, i.e, the error term has a constant variance across observations. However, often this assumption will not be satisfied in data. More importantly in some cases, such as our current example of stock market, making this assumption is conceptually incorrect. This is because in our example, assuming constant variance is equivalent to assuming constant risk underlying a stock. Such an assumption is clearly not desirable for an investor.

Hence, we now need a class of models where \(Var(y_t|\omega_t)\) are not constant over time. Formally, our object of interest in this topic is the conditional variance of a time series, \(y_t\):

\[\sigma_t^2=Var(y_t|\Omega_t)\]

\hypertarget{some-stylized-facts-about-stock-market-volatility}{%
\section{Some stylized facts about stock market volatility}\label{some-stylized-facts-about-stock-market-volatility}}

Before proceeding with formally modeling the conditional variance of a time series, let us establish some stylized facts about financial assets, such as a stock. Below I plot the daily return for SP500 along with its ACF (see Figure 1A and 1B). Using squared returns as a proxy for variance, I also plot squared returns for SP500 and its ACF (see Figure 2A and 2B).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{bookdown-demo_files/figure-latex/ch7-figure1-1} 

}

\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{bookdown-demo_files/figure-latex/ch7-figure1-2} 

}

\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{bookdown-demo_files/figure-latex/ch7-figure1-3} 

}

\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{bookdown-demo_files/figure-latex/ch7-figure1-4} 

}

\end{figure}

Focusing on the volatility, the plot of squared returns and its ACF establishes the following stylized facts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  From Figure 2A, we observe that large values of squared returns cluster together, and small values of squared returns cluster together. That is periods of high volatility are followed by periods of high volatility and periods of low volatility are followed by periods of low volatility. This phenomenon is known as \textbf{volatility persistence} or \textbf{volatility clustering} in the fields of economics and finance.
\item
  A more direct evidence for volatility persistence can be inferred from the ACF plot of squared returns. From Figure 2B, we observe a strong positive serial correlation in squared returns.
\end{enumerate}

Hence, it is reasonable to assume that the variance of a financial time series may not be constant over time. Next we learn two classes of models that have been suggested to model conditional variance of a time series.

\hypertarget{archq-autoregressive-conditional-heteroscedasticiy-of-order-q}{%
\section{\texorpdfstring{ARCH(q): Autoregressive Conditional Heteroscedasticiy of order \(q\)}{ARCH(q): Autoregressive Conditional Heteroscedasticiy of order q}}\label{archq-autoregressive-conditional-heteroscedasticiy-of-order-q}}

Engle (1982) proposed a non-linear model for the conditional variance of a stationary time series where past squared shocks affect current volatility. For simplicity, let us assume that we are not interested in modeling the mean of the time series. Hence, our model for the mean is a constant value, \(\mu\). Then, an \(ARCH(1)\) model can be specified as follows:

\[\text{Mean Model:} \quad y_t = \mu +\epsilon_t \quad \text{where} \ \epsilon_t=\nu_t  \sigma_t \ \text{and} \ \nu_t\sim N(0,1)\]

\[\text{Variance Model:} \quad  \sigma_t^2=\omega +\alpha_1 \epsilon_{t-1}^2 \text{where} \ \omega>0 \ \text{and} \ \alpha_1>0 \]

In this model the unconditional variance of the time series is constant, but the conditional variance depends on the past squared error term. The variance model can be easily generalized to include \(q\) past shocks which gives us \(ARCH(q)\)

\[\text{Variance Model:} \quad  \sigma_t^2=\omega +\alpha_1 \epsilon_{t-1}^2+\alpha_2 \epsilon_{t-2}^2 +\alpha_3 \epsilon_{t-3}^2 +...+ \alpha_q \epsilon_{t-q}^2 \]
\[\text{where} \ \omega>0 \ \text{and} \ \alpha_i>0 \ \forall \ i\]

\hypertarget{garchpq-generalized-autoregressive-conditional-heteroscedasicity-of-order-p-and-q}{%
\section{\texorpdfstring{GARCH(p,q): Generalized Autoregressive Conditional Heteroscedasicity of order \(p\) and \(q\)}{GARCH(p,q): Generalized Autoregressive Conditional Heteroscedasicity of order p and q}}\label{garchpq-generalized-autoregressive-conditional-heteroscedasicity-of-order-p-and-q}}

A generalization of the above ARCH model was proposed by Bolerslev (1986) where the conditional variance in the current period depend on past squared shocks as well as past observations of the conditional variance. This model is known as \(GARCH\) which stands for generalized ARCH model. Formally, the variance equation of a \(GARCH(1,1)\) is given by:

\[\quad  \sigma_t^2=\omega +\alpha_1 \epsilon_{t-1}^2 +\beta_1 \sigma^2_{t-1} \quad \text{where} \ \omega>0, \ \alpha_1>0, \ \text{and} \ \beta_1>0 \]

In practice most models of volatility now use some version of GARCH(1,1) as it provides a more parsimonious model of volatility when compared to an ARCH(q) model. Following are few important properties of a standard GARCH(1,1) model:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  Unconditional variance: The unconditional variance of \(y_t\) is still constant due to stationarity and is given by:
  \[\sigma^2_y = \frac{\omega}{1-\alpha_1-\beta_1}\]
\item
  Volatility persistence: the persistence is given by \(\alpha_1+\beta_1\).
\item
  Half-life measure: is the number of periods it takes for the estimated volatility to converge to half of the unconditional variance of the time series.
\end{enumerate}

\hypertarget{extensions-of-standard-garch-model}{%
\section{Extensions of standard GARCH model}\label{extensions-of-standard-garch-model}}

There are two issues with the standard GARCH model that merits more discussion:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the standard GARCH framework, the effect of past shocks on volatility is symmetric. That is whether the observed shock is \textbf{negative} or \textbf{positive}, the effect on the volatility is the same.
  However, in the financial market another stylized fact is the asymmetric response of volatility to news. For instance, it is quite common to find that negative news increases volatility more than the reduction in volatility caused by positive news. One analytical tool to illustrate this point is the \textbf{news impact curve} where we plot the shock on the x-axis and estimated/predicted volatility from our GARCH model on the y-axis. Below I plot three types of news impact curves. In the first case, the curve is symmetric: negative and positive shocks have identical effect on volatility. In the second case, negative news has a bigger effect on volatility, and finally in the third case, positive news has a bigger effect on volatility.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{bookdown-demo_files/figure-latex/ch7-figure2-1} 

}

\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{bookdown-demo_files/figure-latex/ch7-figure2-2} 

}

\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{bookdown-demo_files/figure-latex/ch7-figure2-3} 

}

\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Because these models are models of variance which cannot be negative, the estimation of these models imposes non-negativity condition on all estimated parameters. For instance, in \(GARCH(1,1)\) we assume that \(\omega, \alpha_1, \ \text{and} \ \beta_1\) are all positive.
\end{enumerate}

\hypertarget{gjr-garch11}{%
\subsection{GJR-GARCH(1,1)}\label{gjr-garch11}}

Glosten, Jagannathan, and Runkle (1993) proposed a variation of the standard GARCH model that incorporates the asymmetry of the impact of news on volatility. The resulting model is called GJR-GARCH and it addresses the first of the issues listed above. Formally, the variance equation of the GJR-GARCH(1,1) is given by:

\[ \sigma_t^2=\omega+\alpha_1 \epsilon^2_{t-1}+\beta_1 \sigma^2_{t-1}+\gamma_1 D_{t-1}\epsilon^2_{t-1}\]

where \(D_{t-1}=1\) if \(\epsilon_{t-1}<0\) and \(0\) otherwise. Hence, now the effect of \(\epsilon^2_{t-1}\) on volatility is \(\alpha_1+\gamma_1\) for negative shocks and \(\alpha_1\) for positive shocks. The news impact curve from this model will be asymmetric. The persistence from this model will also be affected by \(\gamma_1\). Specfically for the GJR-GARCH(1,1) model,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Persistence: \(\alpha_1+\beta_1 +\frac{\gamma_1}{2}\)
\item
  Unconditional variance: \[\sigma^2_y = \frac{\omega}{1-\alpha_1-\beta_1-\frac{\gamma_1}{2}}\]
\end{enumerate}

\hypertarget{exponential-garch-or-egarch11}{%
\subsection{Exponential GARCH or EGARCH(1,1)}\label{exponential-garch-or-egarch11}}

Exponential GARCH model by Nelson (1991) accounts for both the issues outlined above. It allows for asymmetric effects of shocks and also does not require the non-negativity constraints. More importantly, it also accounts for different effect of shocks of different magnitude, and hence provides an estimate of the \textbf{size effect}. Formally, the variance equation for EGARCH(1,1) is given by:

\[ln(\sigma^2_{t-1})=\omega+ \alpha_1 z_{t-1} +\gamma_1 (|z_{t-1}|-E|z_{t-1}|) +\beta_1\sigma^2_{t-1}\]

where \(z_t=\frac{\epsilon_t}{\sigma_t}\) is the standardized error term. Here, \(\alpha_1\) captures the sign effect and \(\gamma_1\) captures the size effect. A common finding is that negative news and larger shocks have bigger effect on volatility. Accordingly, we often find in empirical applications of EGARCH that \(\alpha_1<0\) and \(\gamma_1>0\). For this model, we have

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Persistence: \(\beta_1\)
\item
  Unconditional variance: \[\sigma^2_y = \frac{\omega}{1-\beta_1}\]
\end{enumerate}

\hypertarget{application-of-garch-model-estimating-volatility-of-sp500-return}{%
\section{Application of GARCH model: Estimating volatility of SP500 return}\label{application-of-garch-model-estimating-volatility-of-sp500-return}}

In this application, we will estimate the volatility underlying SP500 returns (Figure 1A and Figure 2A). The first step is to test whether squared returns have ARCH effects i.e, whether there is any evidence for time-varying volatility in our data. For this purpose we will use Engle's ARCH test. Consider our constant mean model:

\[y_t=\mu +\epsilon_t\]

We can estimate the above model by OLS and obtain residuals \(e_t=y_t-\hat{y_t}\). The test for ARCH effects is based on the idea that if there is conditional hetroscedasiticity in our data then the squared residuals will have serial correlation. The ARCH test involves estimating the following regression:

\[e^2_t= \beta_0 +\beta_1 e^2_{t-1} + \beta_2 e^2_{t-2}+...+ \beta_p e^2_{t-p} + u_t\]

Then, the test for ARCH effects is given by:

\[H_0: \beta_1=\beta_2=...=\beta_p=0 \ \Rightarrow \text{no ARCH effects}\]
\[ H_A: \text{Not} \ H_0\]

In R, we use a package called \textbf{aTSA} to implement this test.The function is called \textbf{arch.test()}. Figure 7.1 below shows the results of this test for our data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(aTSA)}
\KeywordTok{library}\NormalTok{(forecast)}

\NormalTok{fit=}\KeywordTok{arima}\NormalTok{(y, }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}

\KeywordTok{arch.test}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ARCH heteroscedasticity test for residuals 
## alternative: heteroscedastic 
## 
## Portmanteau-Q test: 
##      order   PQ p.value
## [1,]     4 1721       0
## [2,]     8 3130       0
## [3,]    12 4205       0
## [4,]    16 4706       0
## [5,]    20 5143       0
## [6,]    24 5442       0
## Lagrange-Multiplier test: 
##      order   LM p.value
## [1,]     4 1726       0
## [2,]     8  682       0
## [3,]    12  436       0
## [4,]    16  308       0
## [5,]    20  244       0
## [6,]    24  196       0
\end{verbatim}

\begin{figure}
\centering
\includegraphics{bookdown-demo_files/figure-latex/table1-1.pdf}
\caption{\label{fig:table1}ARCH LM test}
\end{figure}

We find strong evidence for ARCH effects in our data as the null hypothesis of no ARCH effects is rejected at different orders of serial correlation in squared residuals. Next we estimate three types of GARCH(1,1) models using the \textbf{rugarch} package. Tables 7.1A-7.1D below show the estimated parameters of these three models. The news impact curve for these 3 classes of GARCH model are plotted in Figure 2-4 below. Finally, the estimated conditional volatility from the three models are plotted along with the return in Figures 5-7. We find that there is strong evidence for the sign effect with negative news having a bigger effect on volatility as indicated by the positive value for \(\gamma_1\) in GJR-GARCH and negative value of \(\alpha_1\) in EGARCH. Further, in the EGARCH model we find evidence for the size effect as indicated by positive value for \(\gamma_1\). These findings are confirmed by the asymmetric news impact curves for the GJR-GARCH and EGARCH models.

\begin{table}

\caption{\label{tab:unnamed-chunk-14}(A) Estimated GARCH(1,1)}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  &  Estimate &  Std. Error &  t value & Pr(>|t|)\\
\hline
mu & 0.073 & 0.013 & 5.547 & 0\\
\hline
omega & 0.030 & 0.004 & 7.443 & 0\\
\hline
alpha1 & 0.153 & 0.014 & 11.286 & 0\\
\hline
beta1 & 0.828 & 0.013 & 63.929 & 0\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:unnamed-chunk-14}(B) Estimated GJR-GARCH(1,1)}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  &  Estimate &  Std. Error &  t value & Pr(>|t|)\\
\hline
mu & 0.031 & 0.013 & 2.349 & 0.019\\
\hline
omega & 0.030 & 0.003 & 9.032 & 0.000\\
\hline
alpha1 & 0.000 & 0.013 & 0.000 & 1.000\\
\hline
beta1 & 0.849 & 0.012 & 68.772 & 0.000\\
\hline
gamma1 & 0.251 & 0.024 & 10.455 & 0.000\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:unnamed-chunk-14}(C) Estimated EGARCH(1,1)}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
  &  Estimate &  Std. Error &  t value & Pr(>|t|)\\
\hline
mu & 0.032 & 0.010 & 3.330 & 0.001\\
\hline
omega & 0.000 & 0.003 & -0.037 & 0.971\\
\hline
alpha1 & -0.195 & 0.013 & -15.030 & 0.000\\
\hline
beta1 & 0.966 & 0.003 & 282.410 & 0.000\\
\hline
gamma1 & 0.174 & 0.015 & 11.829 & 0.000\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:unnamed-chunk-14}(D) Persistence, Unconditional Variance, and Half-life}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & GARCH(1,1) & GJR-GARCH(1,1) & EGARCH(1,1)\\
\hline
Persistence & 0.981 & 0.974 & 0.966\\
\hline
Unconditional Variance & 1.577 & 1.145 & 0.996\\
\hline
Half-life & 36.189 & 26.469 & 19.934\\
\hline
\end{tabular}
\end{table}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-14-1.pdf} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-14-2.pdf} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-14-3.pdf} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-14-4.pdf} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-14-5.pdf} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-14-6.pdf}

Finally, these models can also be used to forecast the volatility of SP500. Figure 8 below shows this forecast for the next 7 days from the EGARCH(1.1) model.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-15-1.pdf} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-15-2.pdf}

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{review-of-differential-calculus-and-optimization}{%
\chapter{Review of Differential Calculus and Optimization}\label{review-of-differential-calculus-and-optimization}}

Given that all students must have taken a course in calculus before enrolling for this class, it is assumed that everyone in the class is comfortable with concepts such as derivatives, partial derivatives, and optimization. In this chapter, I will provide a brief review of some concepts that are most pertinent for Econometrics. I strongly encourage that you read your lecture notes for Calculus if you find it difficult to follow the material presented in this chapter.

\hypertarget{derivative-of-a-single-variable-function}{%
\section{Derivative of a single variable function}\label{derivative-of-a-single-variable-function}}

\begin{definition}[Derivative of a function]
\protect\hypertarget{def:unnamed-chunk-16}{}{\label{def:unnamed-chunk-16} \iffalse (Derivative of a function) \fi{} }Consider the following function, \(y=f(x)\). The \emph{derivative} of this function measures the rate of change in \(y\) caused by a change in \(x\).
\end{definition}

There are two alternative notations for the derivative of \(y\) with respect to \(x\): \(f'(x)\) or \(\displaystyle{\frac{dy}{dx}}\).

The derivative of a function is very closely related to the concept of \emph{slope} of a function. Let \(\Delta\) denotes change in a variable. Then, by definition, the slope of \(y\) with respect to \(x\) is given by:

\[slope=\frac{\Delta y}{\Delta x}\]

The derivative of \(y\) with respect to \(x\) is the limit value of the slope as \(\Delta x \rightarrow 0\). Hence,
\[\frac{dy}{dx} \ or \ f'(x)=\lim_{\Delta x \to 0} \left( \frac{\Delta y}{\Delta x}\right) \]

\hypertarget{rules-of-differentiation}{%
\subsection{Rules of Differentiation}\label{rules-of-differentiation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Derivative of a constant is \(0\).
\item
  Derivative of a function multiplied by a constant is constant times the derivative of the function:
\end{enumerate}

\[\frac{d}{dx} [\ a\times f(x)]=a\times f'(x)\]

where it is assumed that \(a\) is an constant.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Addition rule:
\end{enumerate}

\[ \frac{d}{dx} [f(x)+ g(x)]=f'(x)+ g'(x) \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Subtraction rule:
\end{enumerate}

\[ \frac{d}{dx} [f(x)- g(x)]=f'(x)-g'(x) \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Product rule:
\end{enumerate}

\[ \frac{d}{dx} [f(x)\times g(x)]= f(x) \times g'(x) + g(x)\times f'(x)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Quotient rule:
\end{enumerate}

\[ \frac{d}{dx} \left[\frac{f(x)}{g(x)}\right]=\frac{f'(x) \times g(x) - g'(x) \times f(x)}{g(x)^2}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Chain rule:
\end{enumerate}

\[ \frac{d}{dx} [f(g(x))] = f'(g(x)) \times g'(x)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  Derivative of some common functions:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Power function: \(f(x)=x^a\). Then,
    \[f'(x)=a \times x^{a-1}\]
  \item
    Natural log function: \(f(x)=ln(x)\). Then,
    \[f'(x) = \frac{1}{x}\]
  \item
    Exponential function: \(f(x)=e^x\)
    \[f'(x)=e^x\]
  \end{enumerate}
\end{enumerate}

\hypertarget{second-derivative-and-non-linearity}{%
\section{Second derivative and non-linearity}\label{second-derivative-and-non-linearity}}

\begin{definition}[Second derivative of a function]
\protect\hypertarget{def:unnamed-chunk-17}{}{\label{def:unnamed-chunk-17} \iffalse (Second derivative of a function) \fi{} }Consider the following function, \(y=f(x)\). The \emph{second derivative} of this function measures the change in the rate of change of this function. Formally it is denoted by \(f''(x)\) or \(\displaystyle{\frac{d^2y}{dx^2}}\).
\end{definition}

The second derivative measures the \emph{curvature} of the function and hence can be used to distinguish a \emph{linear} function from a \emph{non-linear} function. By definition, a linear function has a constant slope implying the its second derivative must be zero.

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-18}{}{\label{exm:unnamed-chunk-18} }For example, consider the following linear function:

\[f(x) = mx +b\]

Here\(f'(x)=m\) and \(f''(x)=0\).
\end{example}

A non-linear function will have a non-zero second derivative. There are only two possibilities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(f''(x)<0\). In this case we have a concave relationship. An example from economics is the production function where the relationship between output and input is concave.
\end{enumerate}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-19-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \(f''(x)>\). In this case we have a convex relationship. An example from economics is the marginal cost function where the relationship between cost of production and level of output can be convex.
\end{enumerate}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-20-1.pdf}

\hypertarget{partial-derivatives-multi-variable-functions}{%
\section{Partial derivatives: Multi-variable functions}\label{partial-derivatives-multi-variable-functions}}

\emph{Ceteris paribus} aka \emph{holding other things equal} is one of the key concepts used in Economic analysis. A \emph{partial derivative} is a mathematical counterpart of this assumption.

\begin{definition}[Partial Derivative]
\protect\hypertarget{def:unnamed-chunk-21}{}{\label{def:unnamed-chunk-21} \iffalse (Partial Derivative) \fi{} }Consider a function of n-variables given by \(y=f(x_1,x_2, x_3,...,x_n)\). Then, there are n partial derivatives of this function that can be obtained by taking derivative with respect to one of the \(x\)-variables, holding all other constant. Formally, the partial derivative of \(y\) with respect to \(x_i\) is denoted by \(\displaystyle{f_{x_i} \text{or} \frac{\partial y}{\partial x_i}}\).
\end{definition}

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-22}{}{\label{exm:unnamed-chunk-22} }Consider the following 3-variable function:

\[y=ln(x_1)+x_1\times x_2+3x_2^2 + x_1 \times x_3 + ln(x_3)\]

Then we can compute three partial derivatives of this function:

\begin{itemize}
\item
  Partial derivative of \(y\) with respect to \(x_1\), treating \(x_2\) and \(x_3\) as constants:
  \[\frac{\partial y}{\partial x_1} = \frac{1}{x_1} + x_2+x_3\]
\item
  Partial derivative of \(y\) with respect to \(x_2\), treating \(x_1\) and \(x_3\) as constants:
  \[\frac{\partial y}{\partial x_2} = x_1+6x_2\]
\item
  Partial derivative of \(y\) with respect to \(x_3\), treating \(x_1\) and \(x_2\) as constants:
  \[\frac{\partial y}{\partial x_3} = x_1+ \frac{1}{x_3}\]
\end{itemize}
\end{example}

\begin{example}[Cobb-Douglas Production Function]
\protect\hypertarget{exm:unnamed-chunk-23}{}{\label{exm:unnamed-chunk-23} \iffalse (Cobb-Douglas Production Function) \fi{} }One of the most used functional form for the production function is the Cobb-Douglas production function. Suppose you have two inputs: labor (L) and capital (K). Let Y denotes output. Then, the Cobb-Douglas production function is given by:

\[Y=L^{\beta_1}K^{\beta_2}\]

Now, output can change because we change our labor input or our capital input. In each case, we are thinking about a change in output caused by change in one input, holding the other input constant. This is exactly what a partial derivative captures! In what follows next we will use two mathematical concepts to further our understanding of economics of production:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change in natural logs of a variable approximates percent change in that variable. Formally, \(\Delta ln(x) \times 100 \approx \text{\% change in x}\). Hence, it is often useful to express economic relationships in natural logs. The Cobb-Douglas production function in natural logs is given by:
\end{enumerate}

\[ln(Y)=\beta_1 \times ln(L) + \beta_2 \times ln(K) \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  The partial derivative of the above equation gives us \textbf{elasticity of output} with respect to each input.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Output elasticity of Labor:
    \[\frac{\% \ \text{change in Y}}{\% \ \text{change in L}} = \frac{\partial ln(Y) \times 100}{ \partial ln(L) \times 100}=\beta_1\]
  \item
    Output elasticity of Capital:
    \[\frac{\% \ \text{change in Y}}{\% \ \text{change in K}} = \frac{\partial ln(Y) \times 100}{ \partial ln(K) \times 100}=\beta_2\]
  \end{enumerate}
\end{enumerate}

Note that we can also infer whether production is subject to increasing, decreasing, or constant returns to scale from the numerical values assigned to \(\beta_1\) and \(\beta_2\). Returns to scale is simply the sum of output elasiticities with respect to labor and capital:

\[\text{Returns to scale}= \frac{\% \ \text{change in Y}}{\% \  \text{change in L}}+\frac{\% \ \text{change in Y}}{\% \ \text{change in K}}=\beta_1+\beta_2\]

Hence, we obtain constant returns to scale as long as \(\beta_1+\beta_2=1\). We get decreasing returns to scale if \(\beta_1+\beta_2<1\). Finally, increasing returns to scale require \(\beta_1+\beta_2>1\).
\end{example}

\hypertarget{optimization}{%
\section{Optimization}\label{optimization}}

In Economics it is often assumed that rational individuals \emph{optimize}. For instance, firms seek to maximize profits (or minimize costs) and households seek to maximize utility. Mathematically, this is equivalent to finding \textbf{extreme} values of an \textbf{objective function}.

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-24}{}{\label{exm:unnamed-chunk-24} }Consider a firm that is choosing a level of output \((q)\) to maximize its profits. By definition, profits are total revenue \(R(q)\) minus total cost \(C(q)\). The resulting profit function \(\pi(q)\) is the firm's objective function and \(q\) is the control variable:

\[\pi(q)=R(q)- C(q)\]

The firm will choose a value of \(q\) that will maximize its profits. Mathematically, this can be written as:

\[ \max_{q} \pi(q)\]
\end{example}

One way to solve this problem, is to assume a functional form for profits and evaluate this function for all possible values of \(q\). Then, select the value of \(q\) that yields highest value for profits. This approach is called \textbf{numerical optimization} and is often used for complicated objective functions. But in many cases, we can use calculus and obtain an \emph{analytical} solution for the optimization problem.

Formally, suppose the objective function is denoted by \(f(x)\) and assume that this function is continuous and twice differentiable. Then,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(x^*\) is a maximizer if \(f(x^*)\geq f(x)\) for all \(x\neq x^*\). Note that at this point the slope of the tangent to the function is \(0\), i.e., \(f'(x^*)=0\). This is the \textbf{first order condition (foc)} for obtaining a maximum. The graph below illustrates the maximum of a generic function. Note that the slope of the function changes sign from positive to negative around \(x^*\). This will give us the \textbf{second order condition} for obtaining a maximum.
\end{enumerate}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-25-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \(x^*\) is a minimizer if \(f(x^*)\leq f(x)\) for all \(x\neq x^*\). Note that at this point the slope of the tangent to the function is \(0\), i.e., \(f'(x^*)=0\). This is the \textbf{first order condition (foc)} for obtaining a minimum. The graph below illustrates the minimum of a generic function. Note that the slope of the function changes sign from negative to positive around \(x^*\). This will give us the \textbf{second order condition} for obtaining a minimum.
\end{enumerate}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-26-1.pdf}

Note for a maximum, Similarly, . We can now outline the steps for computing a maximum or minimum of a given function.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  First-order condition: Compute the first derivative of the function and equate it to 0. The solution to this equation gives us \(x^*\):
\end{enumerate}

\[ f'(x^*) = 0\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Second-order condition: Compute the second derivative of the function and evaluate it at \(x^*\).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    If \(f''(x^*) < 0\), then \(x^*\) is a maxmizer.
  \item
    If \(f''(x^*) > 0\), then \(x^*\) is a minimizer.
  \end{enumerate}
\end{enumerate}

\begin{example}[Single variable optimization example]
\protect\hypertarget{exm:unnamed-chunk-27}{}{\label{exm:unnamed-chunk-27} \iffalse (Single variable optimization example) \fi{} }
Consider a firm that produces a single good \(q\) and sells it at a price of \$10 per unit. The cost of production is given by:
\[C(q)=2q+ 5+0.1q^2\]
At what level of output would profits be maximized?
\end{example}

\begin{solution}
\iffalse{} {Solution. } \fi{}The profit of a firm is revenue minus cost:

\[\pi(q)= R(q) -C(q)= 5q-2q-5-0.1q^2=2q-5-0.1q^2\]

Hence, we want to solve the following problem:
\[\max_{q} \pi(q)\]

The first order condition is given by:

\[\pi'(q)=0 \Rightarrow 2-0.2q=0 \rightarrow q^*=10 \]

The second order condition is given by:
\[\pi''(q)=-0.2<0\]

Hence, \(q^*=10\) maximizes the profits. The maximum level of profits is given by \(\pi(q^*)=2\times 10-5=0.1\times 10=5\).
\end{solution}

Note that the above process can be easily applied to multivariable functions. In that case there will be one first order condition for every control variable.

\begin{example}[Multi-variable optimization example]
\protect\hypertarget{exm:unnamed-chunk-29}{}{\label{exm:unnamed-chunk-29} \iffalse (Multi-variable optimization example) \fi{} }
Consider a two-variable function:

\[f(x_1,x_2)=2x_1x_2 + \frac{100}{x_1} - 4x_2^2\]

Solve the following minimization problem:

\[\min_{x_1,x_2} f(x_1,x_2)\]
\end{example}

\begin{solution}
\iffalse{} {Solution. } \fi{}Now we have two first order conditions:

\[f_{x_1}(x_1,x_2) = 0 \Rightarrow 2x_2 -\frac{100}{x_1^2}=0\]\\
\[f_{x_2}(x_1,x_2) = 0 \Rightarrow 2x_1 -8x_2=0\]

So we have two equations in two unknowns. You can show that \(x_1*=5.84\) and \(x_2^*=1.46\). The minimum of this function is given by \(f(x_1^*,x_2^*)=2\times 5.84*1.46+\frac{100}{5.84} - 4\times 1.46^2=25.65\).
\end{solution}

\hypertarget{problems}{%
\section*{Problems}\label{problems}}
\addcontentsline{toc}{section}{Problems}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-31}{}{\label{exr:unnamed-chunk-31} }Compute the derivative of the following functions.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \(f(x)=2x^2\)
\item
  \(f(x)=2x^2+ ln(x)\)
\item
  \(f(x)=e^{ax}\)
\item
  \(f(x)= (2x+x^2)^3\)
\item
  \(f(x)= ln(5x+x^2)\)
\item
  \(f(x)= \displaystyle{\frac{x+ln(x)}{x^3}}\)
\end{enumerate}
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-32}{}{\label{exr:unnamed-chunk-32} }Compute the second derivative of each function given in Exercise 2.1.
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-33}{}{\label{exr:unnamed-chunk-33} }
Compute the partial derivative for each variable for the following functions:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \(f(x_1,x_2,x_3)=4x_1^3x_2-e^{x_3}x_1+3 x_2\)
\item
  \(\displaystyle{f(x_1,x_2)=\frac{2x_1 +3x_2}{4x_1^3-7x_1x_2}}\)
\item
  \(\displaystyle{f(x, y)= ln(y^2)-ln(x)+2 ln\left(\frac{x}{y}\right)}\)
\item
  \(f(x,y)=2x^{0.4} y^{0.8}+2x\)
\end{enumerate}
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-34}{}{\label{exr:unnamed-chunk-34} }Solve the following optimization problems. In each case compute the maximizer(s) (or minimizer(s)) for the function as well as the optimum value of the function.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  \(\displaystyle{\max_x f(x) = 3ln(x) - 0.5x+4}\)
\item
  \(\displaystyle{\min_{x,y} f(x,y) = 2xy+\frac{2000}{x}+\frac{2000}{y}}\)
\item
  \(\displaystyle{\max_x f(x) = ax^{0.5} - bx+4}\)
\end{enumerate}
\end{exercise}

\hypertarget{review-of-probability-and-statistics}{%
\chapter{Review of Probability and Statistics}\label{review-of-probability-and-statistics}}

Given that all students must have taken a course in statistics before enrolling for this class, it is assumed that everyone in the class is comfortable with concepts such probability, expected value, measures of central tendency, hypothesis testing etc. In this chapter, I will provide a brief review of some concepts that are most pertinent for Econometrics. I strongly encourage that you read your lecture notes for Statistics if you find it difficult to follow the material presented in this chapter.

\hypertarget{probability}{%
\section{Probability}\label{probability}}

We begin with a brief review of probability thoery. To define probability we first need to develop an understanding of what we mean by \emph{experiment}, \emph{sample space}, and \emph{event} in statistics.

\begin{definition}[Experiment]
\protect\hypertarget{def:unnamed-chunk-35}{}{\label{def:unnamed-chunk-35} \iffalse (Experiment) \fi{} }An experiment is a process with an uncertain observable outcome. e.g.~Toss of a coin can have two possible outcomes, heads or tails.
\end{definition}

\begin{definition}[Sample Space]
\protect\hypertarget{def:unnamed-chunk-36}{}{\label{def:unnamed-chunk-36} \iffalse (Sample Space) \fi{} }The sample space is the set of all possible outcomes of an experiment. I will denote it by \(S\). If we toss a coin then \(S=\{Heads,Tails\}\).
\end{definition}

\begin{definition}[Event]
\protect\hypertarget{def:unnamed-chunk-37}{}{\label{def:unnamed-chunk-37} \iffalse (Event) \fi{} }An event is a subset of the sample space. I will denote it by \(E\). If we toss a coin and Heads shows up then \(E={Heads}\).
\end{definition}

Now, we can define probability, which is a function that assigns a numerical value to the chance of an event occuring among all possible events in the sample space.

\begin{definition}[Probability]
\protect\hypertarget{def:unnamed-chunk-38}{}{\label{def:unnamed-chunk-38} \iffalse (Probability) \fi{} }
A function \(P\) is called a probability function if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For any given event, \(E\), \(0 \leq P(E)< \leq 1\).
\item
  Suppose there are N possible events in S, i.e., \(S=\{E_1, E_2, E_3,..,E_N\}\). Then,
  \[P(E_1)+P(E_2)+P(E_3)+...+P(E_N)=1\]
\item
  Consider an event E. Then,
\end{enumerate}

\[P(\lnot E) = 1 -P(E) \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  If we have two disjoint events \(A\) and \(B\), then:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \(P(A \cup B)= P(A) + P(B)\)
  \item
    \(P(A \cap B)=0\)
  \end{enumerate}
\item
  If we have two non-disjoint events \(A\) and \(B\), then:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \(P(A \cup B)= P(A) + P(B)-P(A \cap B)\)
  \item
    \(P(A \cap B)= P(A) \times P(B|A)\)
  \end{enumerate}
\item
  If we have two independent events \(A\) and \(B\), then:
  \[P(A \cap B) = P(A) \times P(B)\]
\item
  Bayes rule:
\end{enumerate}

\[P(A|B)=\frac{P(A) \times P(B|A)}{P(B)}\]

where \(\displaystyle{P(B)= P(B|A)\times P(A) + P(B|\lnot A) \times P(\lnot A)}\)
\end{definition}

One such probability function is:
\begin{align}
P(E) = \frac{\text{Number of  outcomes  in  E}}{\text{Number  of  outcomes  in  S}}
\end{align}

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-39}{}{\label{exm:unnamed-chunk-39} }Consider a fair six-sided dice. The probability of obtaining an odd number if this dice is rolled once is given by 0.5. To see this, note that the event here is obtaining an odd number when a dice is rolled. Hence, \(E=\{1,3,5\}\). Also, \(S=\{1,2,3,4,5,6\}\). Using this, we get:
\begin{equation}
P(E)=\frac{3}{6}=0.5
\end{equation}
\end{example}

\hypertarget{random-variable}{%
\section{Random Variable}\label{random-variable}}

One of the most important applications of statistics is to resolve the randomness that is inherent in most economic choices. For example, the outcome of your college major is a random variable with many possible values. Most economic variables can be thought of as \textbf{random variables} that have many possible values which are unknown until they are realized. We will begin by formally defining a random variable.

\begin{definition}[Random Variable]
\protect\hypertarget{def:unnamed-chunk-40}{}{\label{def:unnamed-chunk-40} \iffalse (Random Variable) \fi{} }A random variable is a numerical representation of outcomes of an experiment. For example, in the example of a toss of a coin, suppose you win \$10 if heads shows and you lose \$5 if tails shows. In this case, tossing the coin was the experiment, and winnings from this game is the random variable with two possible values: \$10 and -\$5.
\end{definition}

There are two types of random variables.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Discrete random variable: takes finite number of values. e.g.~GPA points earned in Econ 385.
\item
  Continuous random variable: can take any value on the number line. e.g.~GDP in the last quarter of 2019.
\end{enumerate}

\hypertarget{probability-distribution}{%
\section{Probability distribution}\label{probability-distribution}}

By definition a random variable can take many \emph{possible values}. In statistics a function that provides the probabilties of different realizations of a random varuable is called its \textbf{probability distribution}.

\hypertarget{probability-distribution-of-a-discrete-random-variable}{%
\subsection{Probability distribution of a discrete random variable}\label{probability-distribution-of-a-discrete-random-variable}}

For a discrete random variable the probability distribution is simply the list of all possible values this variable can and their corresponding probabilities. Let \(X\) be a discrete random variable with \(n\) possible values give by \(\{x_1,x_2,x_3,..,x_n\}\). Let \(p_i\) denotes that probability that \(X=x_i\). Then, the probability distribution function of this random variable is given by:

\begin{longtable}[]{@{}cc@{}}
\toprule
X & p(X)\tabularnewline
\midrule
\endhead
\(x_1\) & \(p_1\)\tabularnewline
\(x_2\) & \(p_2\)\tabularnewline
\(x_3\) & \(p_3\)\tabularnewline
\(\vdots\) & \(\vdots\)\tabularnewline
\(x_n\) & \(p_n\)\tabularnewline
\bottomrule
\end{longtable}

\begin{example}[Grade Distribution]
\protect\hypertarget{exm:unnamed-chunk-41}{}{\label{exm:unnamed-chunk-41} \iffalse (Grade Distribution) \fi{} }A typical grade distribution is an example of a discrete random variable. Consider the following grade distribution:

\begin{longtable}[]{@{}cc@{}}
\toprule
GPA & Percent of Students\tabularnewline
\midrule
\endhead
\(0\) & \(10\%\)\tabularnewline
\(1\) & \(20\%\)\tabularnewline
\(2\) & \(40\%\)\tabularnewline
\(3\) & \(20\%\)\tabularnewline
\(4\) & \(10\%\)\tabularnewline
\bottomrule
\end{longtable}
\end{example}

Note that every GPA point corresponds to a letter grade. From the perspective of the student, \(X\) is the random variable that is his letter grade, and the above distribution gives the probability of obtaining a particular letter grade. We can plot this simple probability distribution as follows:

\begin{figure}
\centering
\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-42-1.pdf}
\caption{\label{fig:unnamed-chunk-42}Probability Distribution of Letter Grades}
\end{figure}

We can use the probability distribution of a discrete random variable in two different ways.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We can compute the probability of the random variable taking an exact value. This is known as the \textbf{probability mass function (p.m.f)} and is denoted by \(f(x)\):
\end{enumerate}

\[f(x)=P(X=x)\]

For example, the probability of obtaining a letter grade of C or \(P(X=2)\) is 0.4 or 40\%.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We can also infer the probability that a discrete random variable will be less than or equal to a certain value by cumulatively adding the probabilities. Formally, we can compute the \textbf{cumulative probability distribution (c.d.f)} which is denoted by \(F(x)\):
\end{enumerate}

\[F(x)=P(X\leq X)\]

Going back to our grade distribution example, we can add the column of cumulative probabilities to obtain the \(c.d.f\):

\begin{longtable}[]{@{}ccc@{}}
\toprule
Grade & Percent of Students & \(F(x)\)\tabularnewline
\midrule
\endhead
\(0\) & \(10\%\) & \(10\%\)\tabularnewline
\(1\) & \(20\%\) & \(30\%\)\tabularnewline
\(2\) & \(40\%\) & \(70\%\)\tabularnewline
\(3\) & \(20\%\) & \(90\%\)\tabularnewline
\(4\) & \(10\%\) & \(100\%\)\tabularnewline
\bottomrule
\end{longtable}

So for example, we can infer that the probability of obtaining the letter grade of C or lower i.e, \(P(X\leq 2)\) is 0.7 or 70\% which is obtained by adding the probabilities of obtaining letter grades of C, D, and F, respectively.

\begin{example}[Bernoulli Random Variable]
\protect\hypertarget{exm:unnamed-chunk-43}{}{\label{exm:unnamed-chunk-43} \iffalse (Bernoulli Random Variable) \fi{} }When a random variable is binary then we call it a \textbf{Bernoulli} random variable and its probability distrubition is called \textbf{Bernoulli} distribution. Consider a random variable that can only take two values, say, \(0\) or \(1\). It is common to think of these two values as coding a set criterion with \(1\) typically assigned if the criterion is met and \(0\) is assigned for failing to meet the criterion. For example, \(X\) could be whether you will get a job right after graduation. If you do then \(X=1\) and if you do not then \(X=0\). Let \(p\) denotes the probability that you will get a job. Then, the \(p.m.f.\) of the Bernoulli distribution is given by:

\[f(x)=\begin{cases}
    p & if \ X=1\\
    1-p & if \ X=0
     \end{cases}\]

The \(c.d.f\) of the Bernoulli distribution is given by:

\[F(x)=\begin{cases}
    0 & if \quad X < 0\\
    1-p & if \quad 0\leq X<1\\
    p & if \quad  X\geq 1
     \end{cases}\]
\end{example}

\hypertarget{probability-distribution-of-a-continuous-random-variable}{%
\subsection{Probability distribution of a continuous random variable}\label{probability-distribution-of-a-continuous-random-variable}}

In economics a large majority of variables of interest in theory are continous random variables. For example, the change in the price of Apple stock between two time periods is the return on Apple stock. If you are a trader in the NYSE then the stock return on Apple is a continuous random variable that can take any value on an interval. In such a case we cannot obtain the probability of the random variable taking an exact value. But we can only compute the probability that this random variable will fall in a given interval. So at best we can determine the probability that GDP growth for the US next quarter will be between say 1/\% and 2\%. This probability is obtained by computing the area under the \textbf{probability density function (p.d.f)}. Let \(X\) denote a continuous random variable and \(f(x)\) denotes the p.d.f. Then,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The probability that \(X\) takes value over the interval \(\{a,b\}\) is given by:
\end{enumerate}

\[P(a\leq X \leq b)=\int_a^bf(x) \ dx\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The c.d.f (the probability that \(X\leq x\)) is given by:
\end{enumerate}

\[F(x)=P(X\leq x)=\int_{-\infty}^xf(x) \ dx\]

Below I plot the empirical c.d.f for Apple's stock return. Let \(X\) denotes this stock return. From Fig 3.2 we can infer that \(P(X\leq 0)=0.47\) and \(P(X\leq 3)=0.95\).

\begin{figure}
\centering
\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-44-1.pdf}
\caption{\label{fig:unnamed-chunk-44}Empirical c.d.f of daily Apple Stock Return (2007-2019)}
\end{figure}

Figure 3.3 below presents the \emph{p.d.f} of the daily stock return that corresponds to the \emph{c.d.f} plotted in Figure 3.2. Using this we can work the probability of stock returns falling in any given interval. For instance, the probability that Apple stock return will fall between 0 and 3\% is the area under the p.d.f. between these two values. Figure 3.3 higlights this area and we can see that this probability is equal to 0.47.

\begin{figure}
\centering
\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-45-1.pdf}
\caption{\label{fig:unnamed-chunk-45}Empirical p.d.f of daily Apple Stock Return (2007-2019)}
\end{figure}

\hypertarget{moments-of-a-probability-distribution-function}{%
\section{Moments of a probability distribution function}\label{moments-of-a-probability-distribution-function}}

The information contained in a probability distribution can be meaningfully summarized into measures that are called \textbf{moments} of that distribution. There are three moments we often use in economics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Center of the distribution: this is first moment of a given probability distribution and it gives us the most likely value of the random variable. It can be measured by mean, median or mode. We will use mean as a measure of the center of the distribution.
\item
  Width of the distribution: this is the second moment and it measures the average distance from mean under a given probability distribution. We will use standard deviation as a measure of the width of the distribution.
\item
  Shape of the distribution: this feature relates to role played by \textbf{tail events} , i.e., events that have very low probability of happening under a given probability distribution. Two relevant measures are Skewness and Kurtosis
\end{enumerate}

\hypertarget{first-moment-of-a-probability-distribution-expected-value}{%
\subsection{First moment of a probability distribution: Expected value}\label{first-moment-of-a-probability-distribution-expected-value}}

What is the most likely value of a random variable? To answer that we often compute \textbf{expected value} of the random variable which gives us the center (or peak) of the underlying probability distribution. We will use \textbf{E} to denote expected value. So \(E(X)\) is the expected value of a random variable and we will use \(\mu_X\) to denote the mean or average value of \(X\).

\begin{definition}[Expected Value]
\protect\hypertarget{def:unnamed-chunk-46}{}{\label{def:unnamed-chunk-46} \iffalse (Expected Value) \fi{} }Consider a discrete random variable \(X\) that can take \(n\) possible values and has the following probabilty distrubution:

\begin{longtable}[]{@{}cc@{}}
\toprule
X & p(X)\tabularnewline
\midrule
\endhead
\(x_1\) & \(p_1\)\tabularnewline
\(x_2\) & \(p_2\)\tabularnewline
\(x_3\) & \(p_3\)\tabularnewline
\(\vdots\) & \(\vdots\)\tabularnewline
\(x_n\) & \(p_n\)\tabularnewline
\bottomrule
\end{longtable}

Then, the expected value of \(X\) is given by:

\[E(X)= x_1 p_1 + x_2 p_2+...+ x_n p_n = \sum_{i=1}^n y_ip_i\]
\end{definition}

Hence, expected value is a probability-weighted average of all possible values of a random variable.
\begin{example}
\protect\hypertarget{exm:unnamed-chunk-47}{}{\label{exm:unnamed-chunk-47} }Suppose you toss a fair coin and receive \$10 if tails shows and receive 0 if heads shows. What is the expected value of the winnings from a single toss of this coin?
\end{example}

\begin{solution}
\iffalse{} {Solution. } \fi{}Let \(X\) denotes winnings from this game. It can take a value of \$10 with a probability of 0.5 and 0 with a probability of half. So the expected value of X is:

\[E(X) = x_1 p_1+x_2 p_2=10\times 0.5 + 0\times 0.5=\$5\]
\end{solution}

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-49}{}{\label{exm:unnamed-chunk-49} }Suppose you can invest \$10,000 in a mutual fund after 1 year can earn a return of 10\%
with a probability of 0.1 or a return of 2\% with a probability of 0.5 or a loss of 5\% with a probability of 0.4. What is the expected return of investing \$10,000 in this mutual fund?
\end{example}

\begin{solution}
\iffalse{} {Solution. } \fi{}Let \(X\) denotes expected return in dollars. It can take 3 possible values: \$1000 with a probability of 0.1, \$200 with a probability of 0.5, and -\$400 with a probability of 0.4 The expected value is given by:

\[E(X) = 1000\times 0.1 + 200 \times 0.5 - 400 \times 0.4=\$40 \]
\end{solution}

As mentioned earlier, the first moment of the probability distribution (i.e., the expected value) gives us the most likely value of the random variable. How useful is this knowledge will depend on how far any realiaztion of the random variable can be from its expected value. The average distance from the average measures the width of the distribution. Wider the distribution, less useful is the knowledge of the expected value.

\hypertarget{second-moment-of-the-distribution.}{%
\subsection{Second moment of the distribution.}\label{second-moment-of-the-distribution.}}

To determine the width or \emph{dispersion} of a probability distribution we use \textbf{variance} or \textbf{standard deviation}. The variance is the expected value of the squared deviation of each realization of the random variable from its average. We will denote the variance by \(Var(X)\) or \(\sigma^2_X\):

\[Var(x)= \sigma^2_X=E[(X-\mu_x)^2]= (x_1-\mu_X)^2 \times p_1+(x_2-\mu_X)^2 \times p_2+...+ (x_n-\mu_X)^2 \times p_n =\sum_{i=1}^n (x_i-\mu_x)^2p_i\]

The standard deviation is simply the square root of the variance and is in the same units as the random variable. This allows easy comparison of the width and the center of the distribution. We will denote standard deviation by \(\sigma_X\).

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-51}{}{\label{exm:unnamed-chunk-51} }Using the mutual fund example, the variance will measure \textbf{riskiness} of the investment. It is given by:

\[Var(X)=(1000-40)^2\times 0.1 + (200-40)^2 \times 0.5 - (400-40)^2 \times 0.4=53120\]

Because variance is in square units and hence hard to interpret, we can easily compute the standard deviation as the square root of the variance:

\[\sigma_X=\sqrt{53120}=\$230.47\]

Hence, even though the average return from this investment is \$40, you can be \$230 above or below this average.
\end{example}

How much can we say about a random variable if we only know its mean and the standard deviation? That depends on the type of distribution the random variable follows. One of the most commonly used distribution in statistic is the \textbf{Normal Distribution} or the \textbf{Gaussian Distribution}. A random variable that follows normal distribution has a bell-shaped probability distribution with a given mean and standard deviation. One of the most useful features of such a distribution is that knowledge of the first two moments alone is sufficient to characterize the entire probability distribution. Figure 3.4 below shows a normal distribution with a mean of 5 and a standard deviation of 2.

\begin{figure}

{\centering \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-52-1} 

}

\caption{Normal distribution with mean=5 and s.d.=2}\label{fig:unnamed-chunk-52}
\end{figure}

Key features of the normal distribution that are very useful for us:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  95\% of the values fall within 1.96 times the standard deviation of the mean:
\end{enumerate}

\[P(\mu_X -1.96\sigma_X \leq X \leq \mu_X + 1.96\sigma_X)=0.95\]

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\item
  Tail events (low probability events) on either side of the mean are equally unlikely.
\item
  Central limit theorem: The distribution of sample means calculated from repeated random sampling from a given population approaches a normal distribution as the sample size approaches \(\infty\).
\end{enumerate}

\hypertarget{third-and-fourth-moments-skewness-and-kurtosis}{%
\subsection{Third and Fourth Moments: Skewness and Kurtosis}\label{third-and-fourth-moments-skewness-and-kurtosis}}

In many cases, the distribution of a random variable is not normal and in such cases higher moments provide useful information about the shape of such probability distribution. The shape of the probability distribution plays an important role in many economic and financial applications. There are two measures of shape that are of interest:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Skewness}: this is the third moment of the distribution and it measures how skewed a distribution is. The formula for skewness is given by:
\end{enumerate}

\[Skewness=\frac{E[(X-\mu_X)^3]}{\sigma^2_X}\]

A normal distribution has a skweness of zero. There are two possible types of skewed distributions:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  A positively skewed distribution will have a long right tail implying lower probability of very large values relative to the mean.
\item
  A neagtively skewed distribution will have a long left tail implying lower probablity of very small values relative to the mean.
\end{enumerate}

Figure 3.5 shows three probability distributions. For the left-skewed distribution, a longer left tail indicates low probability of obtaining values below the mean. Similarly, for the right-skewed distribution, a longer right tail indicates low probability of obtaining a value above the mean. For a normal distribution, the probability of obtaining a value above the mean is the same as the probability of obtaining a value below the mean.

\begin{figure}

{\centering \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-53-1} 

}

\caption{Skewness of a Probablity distribution}\label{fig:unnamed-chunk-53}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Kurtosis: this is the fourth moment of the distribution that captures the peakedness of the distribution (or thickness of the tail), i.e., how many observations fall on the extreme ends of a given probability distribution. As a result it tells us the role played by extreme values in driving the variance of a random variable. The formula is given by:
\end{enumerate}

\[Kurtosis=\frac{E[(X-\mu_X)^4]}{\sigma^4_X}\]

A normal distribution has a Kurtosis of 3. A value that is above or below 3 will give us excess or deficient Kurtosis. Two possiblities are:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Leptokurtic distribution: has a Kurtosis value greater than three. Such a distribution will have fat tails compared to a normal distribution indicating greater area under the tails.
\item
  Platykurtic distribution: has a Kurtosis value less than 3. Such a distribution will have thin tails compared to a normal distribution.
\end{enumerate}

\begin{figure}

{\centering \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-54-1} 

}

\caption{Kurtosis of a Probablity distribution}\label{fig:unnamed-chunk-54}
\end{figure}

Fig 3.6 shows three types of distribution based on their Kurtosis. The leptokurtic distribution has a Kurtosis value of greater than 3 and is more \textbf{heavy-tailed} or \textbf{peaked} than a norma distribution.

\hypertarget{useful-probability-distributions}{%
\section{Useful probability distributions}\label{useful-probability-distributions}}

Using the normal distribution we can derive a few useful probability distributions that are utilized in hypothesis testing.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Standard Normal Distribution: A random variable that follows normal distribution with a mean of 0 and standard deviation of 1.
\item
  Chi-square distribution: is obtained by squaring and adding indpendent standard normal distribution. For example, is \(X\) and \(Y\) are two standard normal random variables, then \(Z=X^2 + Y^2\) follows a Chi-square distribution with two degrees of freedom.
\item
  F-distribution: is obtained by taking a ratio of two chi-square distribution. For example, if \(X\) is Chi-square with \(v_1\) degrees of freedom and \(Y\) is a Chi-quare with \(v_2\) degress of freedom, then \(\displaystyle{Z=\frac{X}{Y}}\) follows F-distribution with \(v_1\) and \(v_2\) degrees of freddom.
\item
  t-distribution: Student's t-distribution is obtained by taking a ratio of a standard normal and the square root of a Chi-square random variable. For example, if \(X\) is a standard normal and \(Y\) is a Chi-square with \(m\) degrees of freedom, then \(Z=\displaystyle\frac{X}{\sqrt{Y/m}}\) follows t-distribution with \(m\) degrees of freedom. t-distribution has fatter tails when compared to normal.
\end{enumerate}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-55-1} \end{center}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-56-1} \end{center}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-57-1} \end{center}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-57-2} \end{center}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-58-1} \end{center}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-58-2} \end{center}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-58-3} \end{center}

\hypertarget{joint-probability-distribution}{%
\section{Joint Probability Distribution}\label{joint-probability-distribution}}

In economics, often we are interested in the relationship between a pair of variables. For example, how does interest rate affects consumption spending? Or how does education affect wages? In order to statistically answer such questions, we need to understand the meaning of statistical relationship between two or more variables. One way to move forward is to assume that both variables jointly follow some given probability distribution which can be used to infer their relationship with one another.

For simplicity, I will use the discrete random variables case but the concepts covered can be easily extended for the continuous random variables case.

Let \(X\) and \(Y\) denote two random variables of interest, both from a common probablity distribution denoted by \(F(x,y)\). This function gives us the probability that \(X\) and \(Y\) simultaneously take on certain values:

\[F(x,y)=P(X=x, Y=y)\]

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-59}{}{\label{exm:unnamed-chunk-59} }
Suppose you are an investment banker and you are considering investment into two assets: a stock listed in NYSE (\(X\)) and a cotton futures (\(Y\)) listed in Chicago Mercantile Exchange. Suppose \(X\) can take three possible values: 2/\%, 3/\%, or 4/\%. Similarly \(Y\) can take three possible values given by 6/\%,4/\%, or 1/\%. The value will depend on the state of the economy. Suppose there are three possiblities for the economy next year: boom, expansion, and status quo. The joint probability distribution for \(X\) and \(Y\) is given by:

\begin{longtable}[]{@{}cccccc@{}}
\toprule
State of Economy & X/Y & 6 & 4 & 1 & Total\tabularnewline
\midrule
\endhead
Recession & 2 & 0.15 & 0.2 & 0.1 & 0.45\tabularnewline
Expansion & 3 & 0.1 & 0.1 & 0.2 & 0.4\tabularnewline
Status quo & 4 & 0.1 & 0.05 & 0 & 0.15\tabularnewline
& \textbf{Total} & 0.35 & 0.35 & 0.3 & 1\tabularnewline
\bottomrule
\end{longtable}
\end{example}

So in a recession, the probability of obtaining a return of 2/\% on the stock and 6/\% return on the commodity, i.e, \(P(X=2,Y=6)\), is 0.15.
Using the above joint probabity distribution of \(X\) and \(Y\) we can compute two related distributions for each random variable:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Marginal distribution: For each random variable, we can extract its own probability distribution from the joint probability distribution. This is done by simply adding probabilities of all possible outcomes for a particular value of a given random variable. For example, the marginal distribution for \(X\) is given by:
\end{enumerate}

\[P(X=x)=\sum_{i=1}^nP(X=x, Y=y_i)\]

Hence, in our example, the marginal distribution of \(X\) is given by the last column, called Total in the table. For \(Y\) it is the row called Total. We can use the marginal distribution to compute the unconditional expected value of each random variable. For example,

\[E(Y) = 6 \times P(Y=6) + 4 \times P(Y=4)+1 \times P(Y=1)=3.8\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Conditional distribution: For each random variable, we can also compute its probability distribution conditional on the other variable taking on a specific value. For exampl,e the conditional distribution of \(Y\) given that \(X=x\) is given by:
\end{enumerate}

\[ P(Y=y|X=x) =\frac{P(X=x,Y=y)}{P(X=x)}\]

From our example, what is the probability of obtaining 4\% return on commodity under status quo if the return on the stock is 4\%? So here we are interested in finding out:

\[ P(Y=4|X=4) =\frac{P(X=4,Y=4)}{P(X=4)}= \frac{0.05}{0.15}=0.33\]

To see this, note that from the table that P(X=4, Y=4) under status quo is given by 0.05. Also, using the definition of marginal distribution, we know that P(X=4)=0.15.

The conditional distribution of a random variable is a first step toward understanding the statistical relationship between two or more random variables. Just like the probability distribution of a random variable has a mean and a variance, the conditional distribution can similarly be characterized by conditional mean and conditional variance:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Conditional expected value (\(E(Y|X)\)): Using the conditional distribution we can now compute the expected value of a random variable, given the value of another random variable. This is denoted by \(E(Y|X)\) and can be computed as follows:
\end{enumerate}

\[E(Y|X)=y_1 \times P(Y=y_1|X=x) + y_2 \times P(Y=y_2|X=x)+...+ y_n \times P(Y=y_n|X=x)\]

As we can see, this expected value will be a function of \(X\). Depending on the realization of \(X\) our expectation of \(Y\) would change. In economics, we can imagine many such examples. For example, given our education level our expected wage will change. Similarly, given expenditure on advertising, expected sales will change. Hence, conditional expected value goes a long way in establishing statistical relationship between economic variables.

Going back to our example, let us compute the expected return on the commodity \(Y\) conditional on the information that the return on \(X\) is 3\%:

\[E(Y|X) = 6 \times P(Y=6|X=3) + 4 \times P(Y=4|X=3) + 1 \times P(Y=1|X=3)\]

Here, \(P(Y=6|X=3)= \displaystyle\frac{0.1}{0.4}=0.25\), \(P(Y=4|X=3)= \displaystyle\frac{0.1}{0.4}=0.25\) and \(P(Y=1|X=3)= \displaystyle\frac{0.1}{0.2}=0.5\). Hence, \(E(Y|X=3)=3\%\). Contrast this to the uncondtional expected value of \(Y\) of 3.8\% we computed earlier.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Conditional variance (\(Var(Y|X)\)): Now even the variance of a random variable can be affected by another random variable. Here, we are interested in deviations of the random variable from its conditional mean:
\end{enumerate}

\begin{align}
Var(Y|X) = (y_1 -E(Y|X))^2 \times P(Y=y_1|X=x) + (y_2 -E(Y|X))^2\times P(Y=y_2|X=x)+...\\ \nonumber
+ (y_n -E(Y|X))^2 \times P(Y=y_n|X=x)
\end{align}

\hypertarget{measures-of-statistical-association}{%
\section{Measures of statistical association}\label{measures-of-statistical-association}}

We can now define two measures of statistical relationship. The first one is called \textbf{Covariance} and the second is \textbf{Correlation}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Covariance is a measure of association that captures how deviations from mean of one random variable are related to deviations of another random variable to its respective mean. For example, if your hours of study are above average, then what is your test score relative to average? Formally,
\end{enumerate}

\[Cov(X,Y) = E(Y-\mu_Y)(Y-\mu_X)\]

If the above number is positive, then there is a positive relationship between \(X\) and \(Y\). That is, when \(X\) is above its mean then \(Y\) is also above its mean. If the number is negative then there is a negative relationship between \(X\) and \(Y\).

Note that because \(X\) and \(Y\) are often in different units of measurement, the number we obtain for covariance has no meaning or implication for the strength of the relationship between two variables.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Correlation: is the value of covariance that is standardized by dividing this number by standard deviations of each random variable:
\end{enumerate}

\[Cor(X,Y) = \frac{Cov(X,Y)}{\sigma_X \times \sigma_Y }\]

This number is unit free and falls between \(-1\) and \(1\). The sign of the correlation tell us about the direction of the relationship whereas the value of the correlation gives information about the strength of the relationship. A higher absolute value indicates stronger statistical relationship between two variables.

\hypertarget{rules-of-expectation-and-variances}{%
\subsection{Rules of expectation and variances}\label{rules-of-expectation-and-variances}}

Here are some useful rules that are useful for our purpose:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(E(\beta)=\beta\) and \(Var(\beta)=0\) where \(\beta\) denotes a constant.
\item
  \(E(\beta X)= \beta E(X)\) and \(Var(\beta X)= \beta^2 Var(X)\) where \(\beta\) denotes a constant.
\item
  Consider two random variables \(X\) and \(Y\), and let \(a\) and \(b\) denotes two constants. Then,

  3.1. \(E(aX+bY)=aE(X)+bE(Y)\)

  3.2. \(E(aX-bY)=aE(X)-bE(Y)\)

  3.3. \(Var(aX+bY)=a^2 Var(X)+b^2 Var(Y)+2abCor(X,Y)\sqrt{Var(X)}\sqrt{Var(Y)}\)

  3.4. \(Var(aX-bY)=a^ 2Var(X)+b^2 Var(Y)-2abCor(X,Y)\sqrt{Var(X)}\sqrt{Var(Y)}\)
\end{enumerate}

\hypertarget{sampling-and-estimation}{%
\section{Sampling and Estimation}\label{sampling-and-estimation}}

An important distinction in statistics is between the population of interest and a sample of this population that we usually work with. Due to feasibility of data collection and cost both in terms of time and money, most real world analysis is based on a sample that is a subset of the population of interes. For example, to study how business major affects starting salary, the relevant population is all business majors from a graduating class in the U.S. in a given year. In practice however, we will most likely use a sample of this population, for example all business majors from JMU. How useful an analysis based on a sample is depends on how representative the chosen sample is of the entire population.

For our purpose, lack of data on population means that the true probablity distribution of a random variable is unknown and hence the true values of mean, variance, covarinace etc are also unknown to us. Statistics provides a way of using samples to \textbf{estimate} relevant moments of the probability distribution. The approach we take is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Consider the unknown moments of the true probability distribution as ** population parameters** that we would like to estimate.
\item
  Draw a representative sample from the population. In simple random sampling we draw \(n\) obeservations at random so that each member of the population is equally likely to be included in the sample. We can also use other complex sampling schemes where certain groups of population are more likely to be selected in the sample than others. Two examples:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Suppose we are interested in finding out starting salary of CoB majors at JMU. The population will be every graduating student for a given year. However, we may work with a sample of students, where we draw randomly from every major ensuring that all graduating students have equal probability of selection.
  \item
    Suppose we are interested in finding out usage of food stamps in Harrisonburg area. The population of interest will be all residents of Harrisonburg who use food stample. However, we may work with a sample where a certain demographic group is more likely to be part of the sample (and hence is \emph{oversampled}).
  \end{enumerate}
\item
  Use the sample to compute sample estimates for each population parameter of interest. For example for expected value we can use sample mean as an estimator, for variance we can use sample variance as an estimator and so on. There are following key differences between population parameters and their sample estimates:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Population parameters are true but unknown values that we are interested in measuring. In contrast, sample estimates can be computed using our sample data.
  \item
    Population parameters are fixed whereas sample estimates change as we change our sample. For example, if we compute mean starting salary of business majors from JMU we get one number. If use data from UVA we get another number for mean starting salary.
  \item
    Because different samples give us different sample estimates for the same population parameter, we need to ensure that our sample estimator from one sample data is reliable.
  \end{enumerate}
\item
  Sampling distribution: Hypothetically, we can draw many samples from the same population and compute sample estimate for each sample. This will give us a distribution of for the sample estimate which will have its own mean and variance. We can use this sampling distribution to:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Establish reliablity of the sample estimator. Specifically any sample estimator should be unbiased and efficient. More on this in the next section.
  \item
    Statsitically test hypotheses about the true population parameter
    \#\#\# Unbiasedness and efficiency
  \end{enumerate}
\end{enumerate}

Let \(\theta\) denote a population parameter of interest. For example, it can be the mean of the random variable of interest. Let \(\widehat{\theta}\) denotes a sample estimator of \(\theta\) that can be computed using sample data. Then,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\widehat{\theta}\) is an \textbf{unbiased} estimator of \(\theta\) if:
\end{enumerate}

\[E(\widehat{\theta})=\theta\]

The idea here is that if we repeatedly draw a sample from the same population and compute \(\widehat{\theta}\) for each such sample, the average of these estimators must be equal to the true population parameter for unbiasedness. In otherwords, the center of the sampling distribution is at the true population parameter value.

We can now define \textbf{bias} of an estimator as follows:

\[Bias(\widehat{\theta}) = E(\widehat{\theta})-\theta\]

For an unbiased estimator, \(Bias(\widehat{\theta})=0\). If \(Bias(\widehat{\theta})>0\) then we have an over-estimate and if \(Bias(\widehat{\theta})<0\) then we have an under-estimate.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Efficiency: Unbiasedness ensure that the average of sample estimator is equal to the true population parameter. But if the standard deviation of the sample estimator is too high, then knowing that the average is close to the true value is not very useful. In statistics, we call such an estimator unbiased but \textbf{imprecise or inefficient}. To be efficient the standard deviation (or variance) of the sample estimator should be as small as possible. Between two unbiased estimators, a more efficient estimator will have a lower variance.
\end{enumerate}

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-60}{}{\label{exm:unnamed-chunk-60} }Suppose we have a random sample with \(n\) observations: \(\{x_1,x_2,...,x_n\}\) drawn from a population with a mean of \(\mu_x\). Sample mean is defined as:

\[\overline{X}=\frac{\sum_{i=1}^N x_i}{N}\]

The expected value of the sample mean is given by:

\[E(\overline{X})=E\left(\frac{\sum_{i=1}^N x_i}{N}\right)\]

Using properties of the expected value, we get:

\[E(\overline{X})=\frac{E(x_1)+E(x_2)+...+ E(x_N)}{N}\]

Note that because this is a random sample from the same population with a mean of \(mu_x\), we get \(E(x_1)=E(x_2)=..=E(x_n)=\mu_x\). Hence,

\[E(\overline{X})=\frac{\overbrace{\mu_x+\mu_x+...+\mu_x}^{\text{N terms}} }{N}=\mu_x\]

As a result the sample mean is an unbiased estimator of the population mean. However, there are many other possible unbiased estimators of the population mean. We can show that among all other unbiased estimator of the population mean, sample mean has the lowest variance and hence is most efficient estimator as well.
\end{example}

\begin{definition}[Best Unbiased Estimator (BUE)]
\protect\hypertarget{def:unnamed-chunk-61}{}{\label{def:unnamed-chunk-61} \iffalse (Best Unbiased Estimator (BUE)) \fi{} }Let \(\theta\) denote a population parameter of interest. Then, an sample estimator denoted by \(\hat{\theta}\) is the \underline{best unbiased estimator} of \(\theta\) if the following two conditions are satisfied:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\hat{theta}\) is an unbiased estimator, i.e., \(E(\hat{\theta})=\theta\). In this case the sampling distribution is centered at the true value of the parameter.
\item
  \(\hat{\theta}\) is an efficient estimator, i.e., \(Var(\hat{\theta})< Var(\hat{\theta_A})\) for any other unbiased estimator denoted by \(\hat{\theta_A}\). In this case the width of the sampling distribution around the mean is smallest possible.
\end{enumerate}
\end{definition}

\hypertarget{hypothesis-testing}{%
\section{Hypothesis testing}\label{hypothesis-testing}}

An important part of any statistical analysis is testing various hypotheses about population parameters of interest. This is known as \emph{statistical inference} and here we use the sampling distribution of the estimator to formally test whether the corresponding population of interest takes a certain value or not. This is important because even with an best unbiased estimator we do not know the true value of the population parameter of interest. In this section we will look at two types of hypotheses testing procedures that are most relevant for Econometrics. The procedure for any statistical test more or less consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Formulate a hypothesis of interest. This typically manifest as a restriction on the value of a population parameter (or a combination of multiple parameters). The goal is to test whether there is support for this restriction in our sample or not. There are two types of hypotheses that we must formulate:

  1.1. Null Hypothesis (\(H_0\)): A null hypothesis is the statement about the population parameter we assume to be true until we find evidence otherwise. For example, we can test whether the population mean of starting salary for CoB majors is \$60,000. Formally,

  \[H_0: \mu_X = 60,000\]

  Note that the null hypothesis statement is an equality condition.

  1.2. Alternative Hypothesis (\(H_A\)): This is the logical counterpart of the null hypothesis and here we specify. There are two types of alternative hypothesis we can specify:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Two-sided alternative: Here, the alternative hypothesis statement allows for both sides of the inequality. Going back to our example of starting salary, a two-sided alternative will be:

    \[H_A: \mu_X \neq 60,000\]
  \item
    One-sided alternative: Here, we either use a greater or less than sign for the alternative hypothesis. So for example, we can speficy the following one-sided alternative:

    \[H_A: \mu_X > 60,000\]
  \end{enumerate}
\item
  Compute the relevant test statistic that is a function of the sample data. The formula for the test statistic is a function of the sample estimator and the value of the population parameter(s) we assumed in the null hypothesis.
\item
  The test statistic is assumed to follow a certain probability distribution under the assumption that the null hypothesis is correct. The tails of this distribution summarizes values of the test statistic that are less likely to realize. Such a value of the test statistic provides us a threshold level, called the \textbf{critical value}, beyond which the test statistic values are less likely to realize if our hypothesis is true. The decision rule for rejecting or not rejecting the null hypothesis is based on the comparison between the computed test statistic and the associated critical value.
\end{enumerate}

Note that there is always a measure of uncertainty in any hypothesis testing: we may end up making a wrong decision. There are two types of errors we can make here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Type I} error: here we reject \(H_0\) when it is true. The probability of this type of error is denoted by \(\alpha\) and is called the \textbf{level of significance} of a test.
\item
  \textbf{Type II} error: here we do reject \(H_0\) when it is false. The probability of this type of error is related to the \textbf{power} of a test.
\end{enumerate}

Ideally we would like to minimize the probability of both types of errors but we cannot do that because reducing one error comes at the cost of increasing the other. As a result, we first specificy an \textbf{acceptable} level of significance (type one error probability) and then try to minimize the probability of type two error (or maximize the power of the test). It is common to assume a level of signficance of 5\% or \(\alpha=0.05\). So here we are willing to tolerate a 5\% chance of falsely rejecting the null hypothesis.

Once we have fixed the level of significance, we can use the distribution table of the test-statistic to obtain the corresponding critical value(s).

\hypertarget{testing-a-restriction-on-a-single-population-parameter}{%
\subsection{Testing a restriction on a single population parameter}\label{testing-a-restriction-on-a-single-population-parameter}}

Here our goal is to develop tests for testing statements about a single population parameter of interest. So for example, we can either test a statement about a population mean or a population variance.

\begin{example}[t-test for population mean]
\protect\hypertarget{exm:unnamed-chunk-62}{}{\label{exm:unnamed-chunk-62} \iffalse (t-test for population mean) \fi{} }
Suppose you are interested in measuring mean hourly wage of males aged 25-35. Accordingly, we collect a sample of 100 workers from the population of male in this age group with a mean of \(\mu_X\) and a standard deviation of \(\sigma_X\). The sample mean is \(\hat{\mu}_X=\$25\) and the sample standard deviation is \(\hat{\sigma}_X=\$7\). Now, suppose we want to test the following hypothesis:

\[H_0: \mu_X=27\]

\[H_0: \mu_X \neq 27\]

The test statistic is given by the \emph{t-statistic} where:

\[t=\frac{\hat{\mu}_X-\mu_X}{s.e.(\hat{\mu}_X)} \]

where \(s.e.(\hat{\mu}_X)=\displaystyle \frac{\hat{\sigma}_X}{\sqrt{N}}\) is the standard error of sample mean and N denotes sample size.

If the null hypothesis is true, this test statistic follows \textbf{t-distribution} with N-1 degrees of freedom. Using the t-distribution table we can then compute the critical value which is used in formulating the decision rule. Let \(t_c\) denote this critical value from the distribution table. Then,

\[|t|>t_c \quad \Rightarrow  \text{reject $H_0$} \]

\[|t|<t_c \quad \Rightarrow  \text{do not reject $H_0$} \]

In our example, \(N=100\), and

\[t=\frac{25-27}{\frac{7}{\sqrt{100}}}=-2.86\]

The degrees of freedom is \(N-1=99\) and at 5\% level of significance the critical value from the t-distribution table is \(t_c=1.98\). Because \textbar{}t\textbar{} is larger than the critical value, we reject the null hypothesis. Hence, we find evidence against the statement that the mean hourly wage of male workers is \$25.

Note that an alternative way of testing hypothesis like this is to use the \textbf{p-value} rule. The underlying idea is to find out the largest significance level at which we will fail to reject the null hypothesis. This value is called the p-value and most statistical softwares report this value. The decision-rule is then greatly simplified:

\[\text{If p-value is less than the chosen level of significance (value of $\alpha$) then reject $H_0$.}\]

In our case, the p-value is 0.0053. Because we chose \(\alpha=0.05\), according to the p-value rule we will reject the null hypothesis.
\end{example}

\begin{example}[Chi-square test for population variance]
\protect\hypertarget{exm:unnamed-chunk-63}{}{\label{exm:unnamed-chunk-63} \iffalse (Chi-square test for population variance) \fi{} }Using the same example, we also test a statement about the population variance. Suppose we want to test whether the variance of the hourly wage is 52.

\[H_0: \sigma^2_X=52\]

\[H_0: \sigma^2_X >52 \]

The test statistic is given by the \emph{V-statistic} where:
\[V=\frac{(N-1)\times \hat{\sigma^2_X}}{\sigma^2_X}\]

If the null hypothesis is true, this test statistic follows \textbf{Chi-square distribution} with N-1 degrees of freedom. Using the distribution table we can then compute the critical value which is used in formulating the decision rule. Let \(V_c\) denote this critical value from the distribution table. Then,

\[V>V_c \quad \Rightarrow  \text{reject $H_0$} \]

\[V<V_c \quad \Rightarrow  \text{do not reject $H_0$} \]

In our example,

\[V=\frac{(100-1)\times 7^2}{52}=93.29\]

The degrees of freedom is \(N-1=99\) and at 5\% level of significance the critical value from the Chi-square distribution table is \(V_c=43.77\). Because \(V\) is larger than the critical value, we reject the null hypothesis. Hence, we find evidence against the statement that the variance of the hourly wage of male workers is 52.
\end{example}

\hypertarget{testing-a-restriction-on-multiple-population-parameter}{%
\subsection{Testing a restriction on multiple population parameter}\label{testing-a-restriction-on-multiple-population-parameter}}

Often we are interested in testing a restriction that is a linear combination of two or more population means. Similarly, we maybe interested in comparing the variance of two different populations. In such cases we need to develop statistical tests that allow for comparison between parameters of different populations with given means and variances.

\begin{example}[t-test for comparing population mean of two populations]
\protect\hypertarget{exm:unnamed-chunk-64}{}{\label{exm:unnamed-chunk-64} \iffalse (t-test for comparing population mean of two populations) \fi{} }Suppose you are interested in comparing mean weekly hours studied by Econ majors (X) and non-Econ majors in the college of business. For this purpose, you collect a sample of 25 econ majors and a sample of 30 non-econ majors. The sample mean of weekly hours studied by econ majors is 10 hours with a standard deviation of 4 hours. The sample mean of weekly hours studied by non-econ majors is 8 hours with a standard deviation of 2 hours. Also suppose that the covariance between weekly hours studied by econ and non-econ majors is 0.12. Test whether mean weekly hours studied by econ majors is more than the mean weekly hours studied by non-Econ majors.

Let \(X\) denote hours studied, \(N_X\) denotes sample size, \(\hat{\mu}_X\), and \(\hat{\sigma}_X\) denote sample mean and standard deviation, respectively for econ majors. Similarly, let \(Y\) denote hours studied, \(N_Y\) denotes sample size, \(\hat{\mu}_Y\), and \(\hat{\sigma}_Y\) denote sample mean and standard deviation, respectively for non-econ majors.

The first step, as usual, is to formulate the null and the alternative hypotheses:

\[H_0= \mu_X - \mu_Y = 0\]
\[H_A= \mu_X - \mu_Y > 0\]

The next step is to compute the relevant test statistic, which in this case is the t-ratio given by:

\[t= \frac{(\hat{\mu_X}-\hat{\mu_Y})-0}{s.e.(\hat{\mu_X}-\hat{\mu_Y})}\]

Using the properties of variance, we get:
\[s.e.(\hat{\mu_X}-\hat{\mu_Y})=\sqrt{Var(\hat{\mu_X})+Var(\hat{\mu_Y})-2 \times Cor(X,Y)\times s.e.(\hat{\mu_X}) \times s.e.(\hat{\mu_Y})}=0.84\]

So, \(t=\displaystyle \frac{10-8}{0.84}=2.38\)

The sample size here is \(N_X+N_Y=55\). Using 5\% level of significance and degrees of freedom of 53, the critical value from the t-distribution table for the one-sided alternative is 1.67. Because the \textbar{}t\textbar{} is more than 1.67, we reject the null hypothesis. We find evidence for econ majors studying more on average than non-econ majors in our sample.
\end{example}

\begin{example}[F-test for comparing population variance of two populations]
\protect\hypertarget{exm:unnamed-chunk-65}{}{\label{exm:unnamed-chunk-65} \iffalse (F-test for comparing population variance of two populations) \fi{} }Often we may be interested in comparing the variability between two populations. Using our previous example, we may want to test whether variability in hours studied is bigger for econ majors versus non-econ majors. This can be tested by comparing the ratio of two variances against the value of 1. As before, we start by formulating the null and the alternative hypotheses:

\[H_0: \sigma^2_X/sigma^2_Y = 1\]
\[H_0: \sigma^2_X/sigma^2_Y > 1\]

The corresponding test statistic is the F-ratio:

\[F = \frac{\hat{\sigma^2_X}}{\hat{\sigma^2_Y}}=\frac{4^2}{2^2}=4\]

If the null hypothesis is true, the above test statistic follows F-distribution with \(N_x-1\) degrees of freedom for the numerator and \(N_y-1\) degrees of freedom for the denominator. At 5\% level of significance, the critical value for \(\nu_1=24\) and \(\nu_2=29\) from the F-distribution table is 3. Because the computed F-ratio exceeds the critical value we reject the null hypothesis.
\end{example}

\hypertarget{confidence-interval-and-hypothesis-testing}{%
\subsection{Confidence interval and Hypothesis testing}\label{confidence-interval-and-hypothesis-testing}}

One issue with using a sample to estimate population parameters is that by definition a sample estimator will be different for different samples. Thus, sample mean provides no information about how close this estimator is to the true population mean. This uncertainty in estimation can be summarized by computing the standard deviation, with higher value of standar deviation indicating greater uncertainty about the true population parameter. A better measure of this uncertainty is the \textbf{confidence interval}.

\begin{definition}[Confidence Interval]
\protect\hypertarget{def:unnamed-chunk-66}{}{\label{def:unnamed-chunk-66} \iffalse (Confidence Interval) \fi{} }Suppose we draw a random sample \(\{x_1, x_2,...,x_N\}\) from a normally distributed population with mean of \(\mu_X\) and a standard deviation of \(\sigma_X\). Let \(\hat{\mu_X}\) denotes the sample mean and \(\hat{\sigma_X}\) denotes sample standard deviation. Then, the 95\% confidence interval for \(\hat{\mu_X}\) is given by:

\[\left[\hat{\mu_X}-t_{c,2-sided} \times \frac{\hat{\sigma_X}}{\sqrt{N}},\hat{\mu_X}+t_{c,2-sided} \times \frac{\hat{\sigma_X}}{\sqrt{N}} \right]\]

where \(t_{c,2-sided}\) is the critical value that can be obtained from the t-distribution table for a given level of signicance and degrees of freedom. For example, for a 95\% confidence interval we will use 5\% level of significance.
\end{definition}

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-67}{}{\label{exm:unnamed-chunk-67} }Suppose N=20, \(\hat{\mu_X}=5\), and \(\hat{\sigma_X}=2\). Then, the 95\% confidence interval for \(\hat{\mu_X}\) is given by:

\[\left[5-2.093 \times \frac{2}{\sqrt{20}}, 5+2.093 \times \frac{2}{\sqrt{20}} \right]=[4.06,5.94]\]

Hence, before we drew our sample from the population, there is a 95\% chance that the true population parameter (\(\mu_X\)) will fall between 4.12 and 5.94. Note that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Wider the confidence interval, greater is the uncertainty about the true value of the population mean.
\item
  We can use the confidence interval to conduct hypothesis testing for a \textbf{two-sided} alternative hypothesis. If the null hypothesis value does not fall in the confidence interval, then with 95\% confidence (or at 5\% level of significance) we can reject the null hypothesis. For example, consider the following test:
\end{enumerate}

\[H_0: \mu_X=3.8\]
\[H_A: \mu_X\neq 3.8\]

Because 3.8 is not in the confidence interval we will reject the null hypothesis at 5\% level of significance. Note that we will obtain the same conclusion if we were to compute the t-ratio and compare it with the corresponding critical value from the t-distribution table.
\end{example}

\hypertarget{problems-1}{%
\section*{Problems}\label{problems-1}}
\addcontentsline{toc}{section}{Problems}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-68}{}{\label{exr:unnamed-chunk-68} }Suppose you roll a 6-sided fair dice. If an odd number shows you win \$10. If either 2 or 4 shows you lose \$5. If 6 shows, you neither gain nor lose anything.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Denote the winnings from this game as \(X\). Tabulate the probability distribiution of the random variable \(X\).
\item
  Compute the expected value and the standard deviation for \(X\).
\end{enumerate}
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-69}{}{\label{exr:unnamed-chunk-69} }
Consider a population with a mean of \(\mu\) and variance of \(\sigma^2\). Suppose you draw a random sample \(X_1, X_2,..,X_N\).

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Show that \(\hat{\mu_A}=0.25\times X_1 +0.25\times X_3+ 0.25 \times X_8 + 0.25 X_20\) is an unbiased estimator of \(\mu\).
\item
  Show that \(\hat{\mu_B}=0.1\times X_1 +0.1\times X_3+ 0.5 \times X_8+0.3 \times X_11\) is an unbiased estimator of \(\mu\).
\item
  Now compute variance of \(\hat{\mu_A}\) and \(\hat{\mu_B}\). Which one is more efficient estimator of \(\mu\).
\end{enumerate}
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-70}{}{\label{exr:unnamed-chunk-70} }Suppose you collect a random sample of 100 observations and find that sample mean is -25 and sample variance is 350.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Test whether the population mean is -22.
\item
  Test whether the population variance is 400.
\end{enumerate}
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-71}{}{\label{exr:unnamed-chunk-71} }Suppose you are interested in comparing performance of two different mutual funds, \(X\) and \(Y\). Let \(\mu_X\) and \(mu_Y\) denote unknown population mean returns on investment in \(X\) and \(Y\), respectively. Suppose you collect past 20 months data for both mutual funds and find that sample mean for fund \(X\) is 2\% with a standard deviation of 0.5\%. In contrast, the sample mean for fund \(Y\) is 5\% with a standard deviation of 2\%. Suppose that the correlation between returns on these two funds is 0.2.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Test whether mean return on \(Y\) is greater than that on \(X\).
\item
  Test whether variance of \(Y\) is greater than that of \(X\).
\item
  Compute the 95\% confidence interval for \(\hat{\mu_X}\). Using the confidence interval, what can you say about the population mean return for fund \(X\)?
\end{enumerate}
\end{exercise}

\bibliography{book.bib,packages.bib}

\end{document}
