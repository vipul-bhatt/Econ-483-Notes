# Modeling Cycle

In this chapter we will focus on the cyclical component of a time series and hence focus on data that either has no trend and seasonal components, or data that is filtered to eliminate any trend and seasonality. One of the most commonly used method to model cyclicality is the  *Autogressive Moving Average (ARMA)*.  This model has two distinct components:

1. *Autoregressive (AR) component*: the current period value of a time series variable depends on its past (lagged)  observations. We use $p$ to denote the **order** of the AR component and is the number of lags of a variable that directly affect the current period value. For example, a firm's production in the current period maybe impacted by past levels of production. If last year's production exceeded demand, the stock of unsold goods may be used to meet this period demand first, hence lowering the current period production.

2. *Moving average (MA) component*: the current period value of a time series variable depends on current period **shock** as well as past shocks to this variable. We use $q$ to denote the **order** of the MA component and is the number of past period shocks that affect the current period value of the variable of interest. For example, if the Federal Reserve Bank raises the interest in 2016, the effects of that policy shock may impact investment and consumption spending in 2017.

Before we consider these time series model in details it is useful to discuss certain properties of time series that allow us a better understanding of these models.


## Stationarity and Autocorrelation

### Covariance Stationary Time Series
```{definition, d7, name='Covariance Stationary Time Series'}
```
A time series $\{y_t\}$ is said to be a *covariance stationary process* if:
 
1. $E(y_t)=\mu_y \quad \forall \quad t$
2. $Var(y_t)=\sigma_y^2 \quad \forall \quad t$
3. $Cov(y_t,y_{t-s})=\gamma(s) \quad \forall \quad s\neq t$


One way to think about stationarity is *mean-reversion*, i.e, the tendency of a time series to return to its *long-run* unconditional mean following a shock (or a series of shock). Figure @\ref(fig:ch6-figure1) below shows this property graphically.


```{r ch6-figure1, echo = FALSE, fig.cap = 'Reversion to mean', out.width='80%', fig.asp=.75, fig.align='center'}
### simulate data and plot

phi1 =-0.8
phi0 = 0.5
timeserieslength = 20
y1 =0.5 #initial Value
y2a =0
t = 0
ybara=phi0/(1-phi1)
for (i in 1:timeserieslength) {
  y2a[i] = phi0+(phi1*y1[i])
  t[i] = i
  if (i < timeserieslength){y1[i+1]=y2a[i]}}



phi1 =0.8
phi0 = 0.5
timeserieslength = 20
y1 =0.5 #initial Value
y2b =0
t = 0
ybarb=phi0/(1-phi1)
for (i in 1:timeserieslength) {
  y2b[i] = phi0+(phi1*y1[i])
  t[i] = i
  if (i < timeserieslength){y1[i+1]=y2b[i]}}


phi1 =1.2
phi0 = 0.5
timeserieslength = 20
y1 =0.5 #initial Value
y2c =0
t = 0
ybarc=phi0/(1-phi1)
for (i in 1:timeserieslength) {
  y2c[i] = phi0+(phi1*y1[i])
  t[i] = i
  if (i < timeserieslength){y1[i+1]=y2c[i]}}



phi1 =-1.5
phi0 = 0.5
timeserieslength = 20
y1 =0.5 #initial Value
y2d =0
t = 0
ybard=phi0/(1-phi1)
for (i in 1:timeserieslength) {
  y2d[i] = phi0+(phi1*y1[i])
  t[i] = i
  if (i < timeserieslength){y1[i+1]=y2d[i]}}

par(mar=c(3,3,3,3))
par(mfrow=c(2,2))
plot(t, y2a, type="o", main="Non-monotonic Stationary", ylab="Y", ylim=c(min(y2a)-0.2,max(y2a)+0.2),xlab="time", lwd=2)
abline(h = ybara, v=0, col = "red", lwd=2,lty=2)

plot(t, y2b, type="o", main="Monotonic Stationary", ylab="Y", ylim=c(min(y2b)-0.2,max(y2b)+0.2),xlab="time", lwd=2)
abline(h = ybarb, v=0, col = "red", lwd=2,lty=2)


plot(t, y2d, type="o", main="Non-monotonic Non-Stationary", ylab="Y",xlab="time", lwd=2)
abline(h = ybard, v=0, col = "red", lwd=2,lty=2)

plot(t, y2c, type="o", main="Monotonic Non-Stationary", ylim=c(min(y2c)-20, max(y2c)+10)+10,ylab="Y",,xlab="time", lwd=2)
abline(h = ybarc, v=0, col = "red", lwd=2,lty=2)

```

```{r ch6-figure2, echo = FALSE, fig.cap = 'Reversion to mean in practice', out.width='80%', fig.asp=.75, fig.align='center'}


suppressMessages(library(xts))
suppressMessages(library(quantmod))
suppressWarnings(library(quantmod))
# get data from fred stat
options("getSymbols.warning4.0"=FALSE)


suppressMessages(library(xts))
suppressMessages(library(quantmod))
suppressWarnings(library(quantmod))
# get data from fred stat
options("getSymbols.warning4.0"=FALSE)
invisible(getSymbols('GDPC1',src='FRED'))

###
### gdp growth
y=diff(log(GDPC1))*100

## vector of mean growth over the sample

ymean=rep(mean(coredata(y), na.rm=T),length(y))

### get date from xts

date=index(y)

### create xts for mean
ymean=xts(ymean,order.by=date)

###
par(mar=c(8,8,1,1))
p1=plot.xts(as.xts(y),yaxis.right=F, main="US Real GDP Growth")
p1=lines(ymean, col="red", lwd=2, lty=2)
p1
#mtext(text="Long Run Mean",
     #  side=4, las=1,
     #  at=c(0.77:0.77)
     #  )

```

In practice however, you will not be able to visualize a mean-reverting stationary process this clearly. For example, in Figure \@ref(fig:ch6-figure2) we plot real GDP growth for the U.S. which is a stationary process with a mean of 0.7%. In this chapter we will only consider stationary time series data. Later on we will learn how to work with non-stationary data.


<!-- Note: -->

<!-- 1. By definition a time series with trend is non-stationary. For example, consider the following linear trend model: -->

<!-- \[ y_t = \beta_0 + \beta_1 t + \epsilon_t \quad where \ \epsilon_t \sim WN(0,\sigma^2_\epsilon) \] -->

<!-- Here it is easy to show that the unconditional mean of this model changes over time and hence is not constant. If after removing the trend (i.e., the residual from the above model) we get stationarity, then such data is called *trend stationary*. -->

<!-- 2. Similarly, a model with seasonality will be non-stationary. -->


<!--  -->

### Correlation vs Autocorrelation

In statistics, correlation is a measure of relationship between two variables. In the time series setting, we can think of the current period value and the past period value of a variable as two **separate** variables, and compute correlation between them. Such a correlation, between current and lagged observation of a time series is called **serial correlation** or **autocorrelation**. In general, for a time series, $\{y_t\}$, the autocorrelation is given by:


 \begin{align}
 	Cor(y_t,y_{t-s})=\frac{	Cov(y_t,y_{t-s})}{\sqrt{\sigma^2_{y_t} \times \sigma^2_{y_{t-s}}}}
 	 	\end{align}
 	where $Cov(y_t,y_{t-s})= E(y_t-\mu_{y_t})(y_{t-s}-\mu_{y_{t-s}})$ and $\sigma^2_{y_t}=E(y_t-\mu_{y_t})^2$

For a stationary time series, using the three conditions the **Autocorrelation Function (ACF)** denoted by $\rho(s)$ is given by:

 \begin{align}
 	ACF(s) \ or \ \rho(s)=\frac{\gamma(s)}{\gamma(0)}
 	\end{align}
 	
Non-zero values of the ACF indicates presences of serial correlation in the data. Figure \@ref(fig:ch6-figure3)	shows the ACF for a stationary time series with positive serial correlation. If your data is stationary then the ACF should eventually converge to 0. For a non-stationary data, the ACF function will not decay over time.
 	
 	
```{r ch6-figure3, echo = FALSE, fig.cap = 'ACF for a Stationary Time Series', out.width='80%', fig.asp=.75, fig.align='center'}
suppressMessages(library(forecast))
ar1.model = list(ar=0.5)
mu = 1
set.seed(123)
ar1.sim = mu + arima.sim(model=ar1.model,n=1000)
acf(ar1.sim, lag.max=10, main="")

```

### Partial Autocorrelation
```{definition, d9, name='Partial Auto Correlation Function (PACF)'}
```
The ACF captures the relationship between the current period value of a time series and all of its past observations. It includes both direct as well as indirect effects of the past observations on the current period value. Often times it is of interest to measure the direct relationship between the current and past observations, **partialing** out all indirect effects. The *partial autocorrelation function  (PACF)* for a stationary time series $y_t$ at lag $s$ is the direct correlation between $y_t$ and $y_{t-s}$, after filtering out the linear influence of  $y_{t-1},\ldots,y_{t-s-1}$ on $y_t$. Figure \@ref(fig:ch6-figure4) below shows the PACF for a stationary time series where only one lag directly affects the time series in the current period.
 
```{r ch6-figure4, echo = FALSE, fig.cap = 'PACF for a Stationary Time Series', out.width='80%', fig.asp=.75, fig.align='center'}
 
suppressMessages(library(forecast))
ar1.model = list(ar=0.5)
mu = 1
set.seed(123)
ar1.sim = mu + arima.sim(model=ar1.model,n=1000)
pacf(ar1.sim, lag.max=10, main="")

```
 
 
### Lag operator

A **lag operator** denoted by $L$ allows us to write ARMA models in a more concise way. Applying lag operator once moves the time index by one period; applying it twice moves the time index back by two period; applying it $s$ times moves the index back by $s$ periods.
\[ Ly_t=y_{t-1} \]
\[ L^2y_t=y_{t-2} \]
\[ L^3y_t=y_{t-3} \]
\[\vdots\]
\[ L^sy_t=y_{t-s} \]
 
 
## Autoregressive (AR) Model

A *stationary*time series $\{x_t\}$ can be modeled as an AR process. In general, an AR(p) model is given by:

 \begin{equation}
 y_t = \phi_0 +\phi_1 y_{t-1} + \phi_2 y_{t-2} + ...... + \phi_p y_{t-p}+\epsilon_t
 \end{equation}

Here $\phi_i$ captures the effect of $y_{t-i}$ on $y_t$. The order of the AR process is not known apriori. It is common to use either AIC or BIC to determine the optimal lag length for an AR process.

Using the Lag operator, we can rewrite the above AR(p) model as follows:
\[ \Phi(L)y_t=\phi_0+\epsilon_t \]

where $\displaystyle \Phi(L)$ is a polynomial of degree $p$ in L:

\[ \Phi(L) = 1-\phi_1 L - \phi_2 L^2- \ldots\ldots\ldots\ldots -\phi_p L^p\]

For example, an AR(1) model can be written as:
\[y_t=\phi_0+\phi_1 y_{t-1} + \epsilon_t \Rightarrow  \Phi(L)y_t=\phi_0+\epsilon_t\]
where,
\[ \Phi(L) = 1-\phi_1 L \]

**Characteristic equation**: A characteristic equation is given by:

\[\Phi(L)=0\]

The roots of this equation play an important role in determining the dynamic behavior of a time series.


### Unit root and Stationarity

For a time series to be stationary there should be no **unit root**  in its *characteristic equation*. In other words, all roots of the characteristic equation must fall outside the unit circle. Consider the following AR(1) model:
\[\Phi(L)y_t = \phi_0 + \epsilon_t\]

The characteristic equation is given by:
\[\Phi(L)=1-\phi_1L=0 \]

The root that satisfies the above equation is:
\[ L^*=\frac{1}{\phi_1}\]

For no unit root to be present, $L^*>|1|$ which implies that $|\phi_1|<1$.

Typically, for any AR process to be stationary, some restrictions will be imposed on the values of $\phi_i's$, the coefficients of the lagged variables in the model.


### Properties of an AR(1) model

A stationary AR(1) model is given by:
\[ y_t=\phi_0 +\phi_1 y_{t-1}+ \epsilon_t \quad ; \ \epsilon_t\sim WN(0, \sigma_\epsilon^2) \ and \  |\phi_1|<1\]
 
 1. $\displaystyle \phi_1$  measures the persistence in data. A larger value indicates shocks to $y_t$ dissipate slowly over time.
 
 2. Stationarity of $y_t$ implies certain restrictions on the AR(1) model.

    i. Constant long run mean: is the unconditional expectation of $y_t$:
    \[ E(y_t) = \mu_y= \frac{\phi_0}{1-\phi_1}  \]
    ii. Constant long run variance: is the unconditional variance of $y_t$:
 \[ Var(y_t)=\sigma^2_y= \frac{\sigma^2_\epsilon}{1-\phi_1^2}\]
    iii. ACF function:
    \[ \rho(s) = \phi_1^s\]
    iv. PACF function:
  \begin{equation*}
  PACF(s) =
  \begin{cases}
    \phi_1 & \text{if  s=1}\\
    0 & \text{if s>1}
  \end{cases}
 \end{equation*}
     
## Estimating an AR model

When estimating the AR model we have two alternatives:

1. OLS: biased (but consistent) estimates. Also, later on when we add MA components we cannot use OLS.

2. Maximum Likelihood Estimation (MLE): can be used to estimate AR as well as MA components

### Maximum Likelihood Estimation (MLE)
+ MLE approach is based on the following idea:

*what set of values of our parameters maximize the likelihood of observing our data if the model we have was used to generate this data.* 

**Likelihood function**: is a function that gives us the probability of observing our data given a model with some parameters.

#### Likelihood vs Probability

Consider a simple example of tossing a coin. Let $X$ denotes the random variable that is the outcome of this experiment being either heads or tails. Let $\theta$ denote the probability of heads which implies $1-\theta$ is the probability of obtaining tails. Here, $\theta$ is our parameter of interest. Suppose we toss the coin 10 times and obtain the following data on $X$:
\[X=\{H,H,H,H,H,H,T,T,T,T\}\]

Then, the probability of obtaining this sequence of X is given by:
\[Prob (X|\theta)=\theta^6 (1-\theta)^4\]

This is the probability distribution function the variable $X$. As we change $X$, we get a different probability for a given value of $\theta$.

Now let us ask a different question. Once we have observed the sequence of heads and tails, lets call it our data which is fixed. Then, what is probability of observing this data, if our probability distribution function is given by the equation above? That gives us the likelihood function:

\[ L(\theta)=Prob(X|\theta)=\theta^6(1-\theta)^4\]

Note that with fixed $X$, as we change $\theta$ the likelihood of observing this data will change. 

**This is an important point that distinguishes likelihood function from the probability distribution function. Although both have the same equation, the probability function is a function of the data with the value of the parameter fixed, while the likelihood function is a function of the parameter with the data fixed.**

#### Maximum Likelhood Estimation

Now we are in a position to formally define the likelihood function. 

``` {definition}
Let $X$ denotes a random variable with a given probability distribution function denoted by $f(x_i|\theta)$. Let $D=\{x_1, x_2,\dots,x_n\}$ denote a sample realization of $X$. Then, the likelhood function, denoted by $L(\theta)$ is given by:
\[L(\theta)=f(x_1,x_2,\dots,x_n|\theta)\]
```

If we further assume that each realization of $X$ is independent of the others, we get:
\[L(\theta)=f(x_1,x_2,\dots,x_n|\theta)=f(x_1|\theta)\times f(x_2|\theta) \times \dots \times f(x_n|\theta)\]

A mathematical simplification is to work with natural logs of the likelihood function, which assuming independently distributed random sample, gives us:

\[ lnL(\theta)=ln(f(x_1|\theta)\times f(x_2|\theta) \times \dots \times f(x_n|\theta))=\sum_{i=1}^{N}ln(f(x_i|\theta))\]

```{definition}
The maximum likelihood estimator, denoted by $\hat{\theta}_{MLE}$, maximizes the log likelihood function:
  \[ \hat{\theta}_{MLE} \equiv arg \max_{\theta} lnL(\theta) \]
```

```{example}
Compute maximum likelihood estimator of $\mu$ of an indpendently distributed random variable that is normally distributed with a mean of $\mu$ and a variance of $1$:
  
  \[ f(y_t|\mu)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (y_t-\mu)^2}\]

Solution: The log likelihood function is given by:
  
  \[lnL= -Tln2\pi-\frac{1}{2}\sum_{t=1}^T(y_t-\mu)^2 \]

From the first order condition, we get
\[ \frac{\partial LnL}{\partial \mu}=\sum_{t=1}^T(y_t-\mu)=0\Rightarrow \hat{\mu}_{MLE}=\frac{\sum_{t=1}^T y_t}{T}\]
```
### MLE of an AR(p) model

One complication we face in estimating an AR(p) model is that by definition the realizations of the variable are not independent of each other. As a result we cannot simplify the likelihood function by multiplying individual probability density functions to obtain the joint probability density function, i.e.,
\[ f(y_1,y_2,\dots,y_T|\theta) \neq f(y_1|\theta)\times f(y_2|\theta)\times \dots \times f(y_T|\theta)\]

Furthermore, as the order of AR increases, the joint density function we need to estimate becomes even more complicated. In this class we will focus on the method that divides the joint density into the product of conditional densities and density of a set of initial values. The idea comes from the conditional probability formula for two related events $A$ and $B$:

\[ P(A|B) =\frac{P(\text{A and B})}{P(B)} \Rightarrow P(\text{A and B}) = P(A|B)\times P(B) \]

In the time series context, I will explain this for a stationary AR(1) model. We know that in this model only last period observation directly affects the current period value. Hence, consider the first two observations of a stationary time series: $y_1$ and $y_2$. Then the joint density of these adjacent observations is given by,

\[ f(y_1,y_2;\theta)= f(y_2|y_1; \theta)\times f(y_1;\theta)\]

Similarly, for the first three observations we get:

\[ f(y_1,y_2,y_3;\theta)= f(y_3|y_2; \theta)\times f(y_2|y_1; \theta) \times f(y_1; \theta)\]

Hence, for $T$ observations we get:


\[ f(y_1,y_2,y_3, ...,y_T; \theta)= f(y_T|y_{T-1};\theta)\times f(y_{T-1}|y_{T-2}; \theta)\times.... \times f(y_1; \theta)\]

The log-likelihood function is given by:

\[ ln \ L(\theta) = ln \ f(y_1;\theta) + \sum_{t=2}^{T} ln \ f(y_t|y_{t-1}; \theta)  \]

We can then maximize the above likelihood function to obtain an MLE estimator for the AR(1) model.

### Selection of optimal order of the AR model

Note that apriori we do not know the order of the AR model for any given time series. We can determine the optimal lag order by using either AIC or BIC. The process is as follows:

1. Set $p=p_{max}$ where $p_{max}$ is an integer. A rule of thumb is to set
\[p_{max}=integer\left[12\times \left(\frac{T}{100}\right)^{0.25}\right]\]

2. Estimate all AR models from $p=1$ to $p=p_{max}$.

3. Select the  final model as the one with lowest AIC or lowest BIC.


### Forecasting using AR(p) model

Having estimated our AR(p) model with the optimal lag length, we can use the conditional mean to compute the forecast and conditional variance to compute the forecast errors. Consider an AR(1) model:

\[y_t=\phi_0+\phi_1 y_{t-1} +\epsilon_t\]

Then, the 1-period ahead forecast is given by:
\[f_{t,1}=E(y_{t+1}|\Omega_t)=\phi_0+\phi_1 y_t\]
Similarly, the 2-period ahead forecast is given by:
 \[f_{t,2}=E(y_{t+2}|\Omega_t)=\phi_0+\phi_1 E(y_{t+1}|\Omega_t) =\phi_0+\phi_1f_{t,1}\]

In general, we can get the following recursive forecast equation for h-period's ahead:
 \[f_{t,h}=\phi_0+\phi_1 f_{t,h-1}\]

Correspondingly, the h-period ahead forecast error is given by:
 \[e_{t,h}=y_{t+h}- f_{t,h}=\epsilon_{t+h}+\phi_1 e_{t,h-1}\]



```{theorem}
The h-period ahead forecast converges to the unconditional mean of $y_t$, i.e., $$\lim_{h\to\infty} f_{t,h}=\mu_y=\frac{\phi_0}{1-\phi_1}$$
```
```{theorem}
The variance of the h-period ahead forecast error converges to the unconditional variance of $y_t$, i.e., $$\lim_{h\to\infty} Var(e_{t,h})=\sigma^2_y=\frac{\sigma^2_\epsilon}{1-\phi_1^2}$$
```

## Moving Average (MA) Model
Another commonly used method for capturing the cyclical component of the time series is the **moving average (MA)** model where the current value of a time series linearly depends on current and past shocks. Formally, a *stationary* time series $\{y_t\}$ can be modeled as an MA(q) process:
  \begin{equation}
  y_t = \theta_0 + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ...... + \theta_q \epsilon_{t-q}
    \end{equation}

Using lag operator, we can write this in more compact form as:

\[y_t = \theta_0 +\Theta(L) \epsilon_t\]

where $\Theta(L)=1+\theta_1 L+ \theta_2 L^2+...+\theta_q L^q$ is lag polynomial of order $q$.

Note that because each one of the current and past shocks are white noise processes, an MA(q) model is always stationary.


### Invertibility of an MA process

Consider the following MA(1) process with$\theta_0=0$ for simplicity:
\[y_t=\epsilon_t +\theta_1 \epsilon_{t-1}\]

Using the lag operator we can rewrite this equation as follows:

\[y_t= (1+\theta_1L)\epsilon_t \Rightarrow y_t(1+\theta_1 L) ^{-1}=\epsilon_t\]

Note that if $|\theta_1|<1$, then we can use the Taylor series expansion centered at 0 and get:

\[(1+\theta_1 L)^{-1}=1-\theta_1 L+(\theta_1L)^2-(\theta_1L)^3+ (\theta_1L)^4-...... \]

Hence, an MA(1) can be rewritten as follows:

\[y_t (1-\theta_1 L+(\theta_1L)^2-(\theta_1L)^3+ (\theta_1L)^4-......)=\epsilon_t\]
\[\Rightarrow y_t -\theta_1 y_{t-1} +\theta_1^2y_{t-2}-\theta_1^3 y_{t-3}....=\epsilon_t\]

Rearranging terms, we get the $AR(\infty)$ representation for an invertible MA(1) model:
\[y_t=-\sum_{i=1}^{\infty}(-\theta_1)^i \ y_{t-i}+\epsilon_t\]

```{definition}
An MA process is invertible if it can be represented as a stationary $AR(\infty)$.
```

### Properties of an invetible MA(1)

An invertible MA(1) model is given by:

\[ y_t = \theta_0 + \epsilon_t + \theta_1 \epsilon_{t-1} \quad ; \ \epsilon_t\sim WN(0, \sigma_\epsilon^2) \ and \  |\theta_1|<1\]


1. Constant unconditional mean of $y_t$:
\[E(y_t)=\mu_y =\theta_0 \]

2. Constant unconditional variance of $y_t$:

\[Var(y_t)=\sigma^2_y=\sigma^2_\epsilon(1+\theta_1^2)\]

3. ACF function:
  \begin{equation*}
  ACF(s) =
  \begin{cases}
    \frac{\theta_1}{1+\theta_1^2} & \text{if  s=1}\\
    0 & \text{if s>1}
  \end{cases}
 \end{equation*}

4. PACF function: using the invertibility it is evident that PACF of an MA(1) decays with $s$.

### Forecast based on MA(q)

Like before, the h-period ahead forecast is the conditional expected value of the time series. Consider an MA(1) model:

\[y_t=\theta_0 +\epsilon_t + \theta_1 \epsilon_{t-1}\]

Then, the 1-period ahead forecast is given by:


The h-period ahead forecast for $h>1$ is given by:
 \[f_{t,h}=E(y_{t+h}|\Omega_t)=\theta_0\]
 
In general, for an MA(q) model, the forecast for $h>q$ is the long run mean $\theta_0$.  This is why we say that an MA(q) process has a memory of *q* periods.


## ARMA(p, q)
  
An ARMA model simply combines both AR and MA components to model the dynamics of a time series. Formula,
  
   \begin{equation}
   y_t = \phi_0 +\phi_1 y_{t-1} + \phi_2 y_{t-2} + ...... + \phi_p y_{t-p}+\epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ...... + \theta_q \epsilon_{t-q}
   \end{equation}

Note that:

1. Estimation is done by maximum likelihood method. 

2. Optimal order for AR and MA components is selected using AIC and/or BIC.

3. The forecast of $y_t$ from an ARMA(p,q)  model will be dominated by the AR component for $h>q$. To see this consider the following ARMA(1,1) model:

\[y_t = \phi_0 +\phi_1 y_{t-1}+ \epsilon_t + \theta_1 \epsilon_{t-1}\]

Then, the 1-period ahead forecast is:
\[f_{t,1} = E(y_{t+1}|\Omega_t) = \phi_0 + \phi_1 y_t + \theta_1 \epsilon_{t-1}\]
 
Here both MA and AR component affect the forecast. But now consider the 2-period ahead forecast:

\[f_{t,2} = E(y_{t+2}|\Omega_t) = \phi_0 + \phi_1 f_{t,1}\]

Hence, no role is played by the MA component in determining the 2-period ahead forecast. For any $h>1$ only the AR component affects the forecast from this model.


## Integrated ARMA or ARIMA(p,d,q)

Thus far we have assumed that our data is stationary. However, often we may find that this assumption is not supported in practice. In such a case we need to tranform our data appropriately before estimating an ARMA model. The procedure can be summarized as follows:

1. Determine whether there is a unit root in data or not. Presence of unit root indicates non-stationarity. We will use Augmented Dickey-Fuller (ADF) test for this purpose.

2. If data is non-stationary, then we need to appropriately transform our data to make it stationary. 

3. Once we have obtained a stationary transformation of our original data, we can proceed and estimate the ARMA model as before.

## Trend Stationary vs Difference Stationary Time Series

There are two types of time series we often encounter in real world:

1. Trend-stationary: a time-series variable is non-stationary becuase it has a deterministic trend. Once we detrend our data then it will become stationary. In this case the appropriate transformation is to estimate a trend model and then use the residual as the detrended stationary data. For example, suppose our data has a linear trend given by:

\[y_t = \beta_0 +\beta_1 t +\epsilon_t\]

Then the OLS residual from this model, $e_t=y_t-\hat{y_t}$ is the detrended $y_t$ which will be stationary. Hence, we will estimate an ARMA(p,q) model using this detrended variable.

2. Difference-stationary: a time-series variable is non-stationary because it contains a stochastic trend. Here, the transformation requires us to difference the orignial data until we obtain a stationary time series. Let $d$ denote the minimum number of differences needed to obtain a stationary time series:

\[\Delta_d \ y_t=(1-L)^d \ y_t\]

In this case, we say that $y_t$ is intergrated of order $d$ or more formally, $y_t$ is an $I(d)$ process. Hence, for $d=1$ we obtain an $I(1)$ process implying that:

\[\Delta_1 \ y_t=(1-L)^1 \ y_t=y_t-y_{t-1} \quad  \text{is stationary}\]

In otherwords, the first difference of an I(1) process is stationary. Similarly for $d=2$, we obtain an $I(2)$ process where second difference will be stationary and so forth. 

## Testing for a unit root

Consider the following AR(1) model with no trend and intercept:

\[y_t=\phi_1 y_{t-1} +\epsilon_t  \ quad ; \epsilon_t\sim WN(0,\sigma^2_\epsilon)\]

We know that if $\phi_1=1$ we have a unit root in this data. Lets subtract $y_{t-1}$ from both sides and rewrite this model as:

\[y_t-y_{t-1}= (\phi_1-1)y_{t-1}+\epsilon_t \]

Define $\rho=phi_1-1$. Then, we get:
\[\Delta y_t= \rho \ y_{t-1}+\epsilon_t \]

We can now estimate the above model and carry out the following test known as the Dickey-Fuller (DF) test:

\[H_0: \rho=0 \]
\[H_A: \rho<0\]

If the null hypothesis is not rejected, then we do not have sample evidence against the statement that $\rho=0 \Rightarrow \phi_1=1$. Hence, we conclude that there is no evidence against the statement that there is unit root in the data. In contrast, if we reject the null hypothesis, then we can conclude that there is no unit root and hence the data is stationary.

The t- statistic is for the above test is denoted by $\tau_1$ and is given by:
\[\tau_1 =\frac{\hat{\rho}}{se(\hat{\rho})}\]

Under the null hypothesis this test statistic follows the DF-distribution and the critical values are provided in most statistical softwares. Given that this is a left-tail test, the decision rule is that if the test statistic is less than the critical value then we reject the null hypothesis.

There are two issues we face when implementing this test in practice:

1. First, the above procedure assumes that there is no intercept and trend in the data. In real world, we cannot make that assumption and must extend the test procedure to accomodate a non-zero intercept and trend. Hence, we have the following two additional versions of the DF test:
    i. Constant and no trend model: Here our AR(1) model is
      
      \[\Delta y_t= \phi_0 + \rho \ y_{t-1}+\epsilon_t \]
      
      Now we can do two possible tests. The first test is that of the unit root:
      
      \[H_0: \rho=0 \]
      \[H_A: \rho<0\]

     The t statistic for this test is denoted by $\tau_2$ and is given by:
    \[\tau_2 =\frac{\hat{\rho}}{se(\hat{\rho})}\]

     If the test statisitic is less than the critical value, we reject the null. 
       
     The second test we can do is:
    \[H_0: \rho=\phi_0=0 \]
    \[H_A: Not \ H_0\]
    The test statistic for this test is denoted by  $\phi_1$. If the test statistic      exceeds the critical value then we reject the null.

    ii. Constant and linear trend model: Here our AR(1) model is
    
    \[\Delta y_t= \phi_0 + \beta \ t+ \rho \ y_{t-1}+\epsilon_t \]
    
    Now we can do three possible tests. The first test is that of the unit root;
          \[H_0: \rho=0 \]
           \[H_A: \rho<0\]
    The t statistic for this test is denoted by $\tau_3$ and is given by:

    \[\tau_3 =\frac{\hat{\rho}}{se(\hat{\rho})}\]

    If the test statisitic is less than the critical value, we reject the null. 

    The second test is:
    \[H_0: \rho=\phi_0=\beta=0 \]
           \[H_A: Not \ H_0\]
    The test statistic for this test is denoted by  $\phi_2$. If the test statistic exceeds the critical value then we reject the null.

    Finally the third test is: 
           \[H_0: \rho=\beta=0 \]
           \[H_A: Not \ H_0\]
    The test statistic for this test is denoted by  $\phi_3$. If the test statistic      exceeds the critical value then we reject the null.

2. Second, we only have allowed for AR(1). We need to extend the above testing procedure for higher order AR models. The Augmented DF (ADF) test allows for higher order lags in testing for a unit root. For example, the model with an intercept, trend, and $p$ lags is given by:
 \[\Delta y_t= \phi_0 + \beta \ t+ \rho \ y_{t-1}+\sum_{i=2}^p\delta_i  y_{t-i}+\epsilon_t  \quad where \ \rho=\sum_{i=1}^p \phi_i-1\]
   
### Testing for unit root in USD/CAD exchange rate

In this application we will test for unit root in US-Canada exchange rate. For this purpose we work with monthly data from Jan 1971 through Oct 2018. Below I show the results for 3 models using the **urca** package in R.

```{r, echo=F}

library(urca)
library(quantmod)
library(knitr)

options("getSymbols.warning4.0"=FALSE)
invisible(getSymbols('EXCAUS',src='FRED'))

y=EXCAUS


###3 adf test
one=summary(ur.df(y,type="none",selectlags="AIC"))
two=summary(ur.df(y,type="drift",selectlags="AIC"))
three=summary(ur.df(y,type="trend",selectlags="AIC"))

one; two; three
```

For the first model with no constant and trend, the test statistic $\tau_1= 0.2469$ and the 5\% critical value is -1.95. For the second model with a constant, the test statistics is $\tau_2= -1.9315$ and the 5\% critical value is -2.86. Finally, for the third model with a trend, the test statistic $\tau_3= -1.8839$ and the 5\% critical value is -3.41. In each, because the test statistic is greater than the critical value, we do not reject the null hypothesis and conclude there is unit root.

## Box-Jenkins Method for estimating ARIMA(p,d,q)

Box-Jenkins is a three-step procedure for finding the best fitting ARIMA(p,d,q) for a non-statinary time series.

1. Model identification: here we determine the order of integration $d$, and the optimal number of AR and MA components, $p$ and $q$ respectively. 

    i. To determine $d$, we conduct ADF test on successive differences of the original time series. The order of integration is the number times we difference our data to obtain stationarity.
    ii. This is followed by estimating ARMA model for different combinations of $p$ and $q$. The optimal structure is chosen using eithr AIC or BIC.

2. Parameter estimation: we estimate the identified model from the previous step using ML estimation.

3. Model Evaluation: mostly showing that the residuals from the optimal model is a white noise process. We can do this by using the Breusch-Godfrey LM test of serial correlation for the residuals. If residuals  from the final model are white noise then there should be no serial correlation.






